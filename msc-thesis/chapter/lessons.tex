\chapter{Key Findings and Lessons}
\label{sec:lessons}

The main findings of this study are as follows.

\begin{itemize}
  \item Both techniques we investigated were successful in revealing
    bugs (several of them);
  %% \item We have found more bugs with test transplantation than we have
  %%   with cross-engine differential testing. In addition, test
  %%   transplantation demanded less effort per bug;
  \item Even simple black-box fuzzers can create surprisingly
    interesting inputs. We conjecture that there is room to find more
    bugs using other fuzzers;
  \item Mozilla's \smonkey\ appears to be the most reliable
    \js\ engine we analyzed, with Google's \veight\ after it.
\end{itemize}

The main lessons we learned from this study are as follows: 1)~Even
for software projects with fairly clear specifications, as the case of
\javascript{}~\cite{ecmas262-spec}, there is likely to be (a lot of)
variation between different implementations and, therefore,
opportunities for bugs. 2)~Not only multiple different implementations
can be leveraged in differential testing, but differences in test
suites can also be important. 3)~Finding functional/non-crash bugs
with differential testing is feasible on real, complex, widely used
pieces of software. 4)~Further reducing cost of inspection is an
important problem. Although the inspection activity was not
uninterrupted, it is safe to say that each warning required a
substantial amount of time to analyze for potential false alarms. In
fact, many \hi\ warnings reported with differential testing were not
analyzed. In determining cost, we observed, from experience, that the
complexity of the \js\ specifications that the original test covers
increases cost of diagnosing (as developers need to read and
understand those) and the availability of alternative implementations
(for the cases warnings are revealed through differential testing)
reduces the cost of diagnosis. We prefer to see such problem as an
opportunity for future research. For example, applying learning
techniques to prioritize the warnings more likely to be faulty (in the
spirit of the work of Chen and
colleagues~\cite{Chen:2017:LPT:3097368.3097451}) may be a promising
avenue to explore. Recall that the rate of TP of the
techniques we studied is rather small.  5)~We learned that reporting
real bugs is a great way to train (and encourage) students in software
testing. Students praised the experience of diagnosing failures,
understanding part of the specs (as needed), writing bug reports,
participating in discussions on issue trackers, and observing the
change of status. That was a relatively self-contained hands-on
activity that enabled students to engage in a real-life serious
industrial project.