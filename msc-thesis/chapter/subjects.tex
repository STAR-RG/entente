\chapter{Objects of Analysis}

\section{Cross-Engine Differential Testing}
\label{sec:design}

This section describes the infrastructure we used for cross-engine
differential testing. Figure~\ref{fig:workflow} illustrates the
workflow of the approach. It takes on input a list of JS files and
generates warnings on output. Numbered boxes in the figure denote the
data processors and arrowed lines denote data flows. The cycle icons
indicate repetition--the leftmost icon indicates that each file in the
input list will be analyzed in separate whereas the rightmost icon
shows that a single file will be fuzzed multiple times.

The bug-finding process works as follows. For a given test input, the
toolchain produces new inputs using some off-the-shelf input fuzzer
(step 1).  (Section~\ref{sec:objects:fuzzers} describes the fuzzers we
selected.)  Then, the oracle checks whether or not the output produced
for the fuzzed file is consistent across all engines (step 2). In case
the test passes in all engines or fails in all engines (\ie{}, the
output is consistent), the infrastructure ignores the
input. Otherwise, it considers the input as potentially
fault-revealing; hence, interesting for human inspection. Finally, to
facilitate the human inspection process, the infrastructure
prioritizes warnings and clusters them in groups (step 3). We describe
these features in Sections~\ref{sec:prioritization} and
\ref{sec:clusterization}. Note that a number of reasons exist, other
than a bug, to explain discrepancy (see
Tables~\ref{fig:piecharts-transplantation} and
~\ref{tab:false-positives}) and there is no clear automatic approach
to precisely distinguish false and \gls{tp}. As such, a human
needs to inspect the warning to classify the issue. As mentioned
earlier, this justifies why differential testing is challenging to
automate at the functional level.

For step 2, we considered using the open-source tool
eshost-cli~\cite{eshost-cli}, also used at Microsoft, for checking
output discrepancy. \Igor{However, we noticed that eshost-cli does not
handle discrepancies involving crashes, but our oracle checks if
there are a crash report on the running.}
% \Igor{nos nao fizemos esse fork e implementamos essa funcionalidade no eshost}
%As such, we forked the
%eshost-cli GitHub project and implemented that functionality on it.
It is also important to note that our checker does not support the case
where the test fails in all engines but the kind of failure (\eg{},
exception thrown) is different. Currently, our infrastructure does not
report discrepancy for that case. For that, it would be necessary to
properly parse the error message to retrieve the error types. We left
that as future work as we already found several discrepancies even
without that.

%% \Mar{any difference in
%%   output is considered for reporting? are there any
%%   filtering?}\Igor{acho que nos equivocamos ao deixar o "eshost-cli", 
%%   nao estamos mais utilizando isso devido a essa ferramenta nao 
%%   capturar certos tipo de issues (e.g. crash), retirar tambem da Figura 2.
%%   A respeito de filtros nos reports, nao. Nos so capturamos o output,
%%   o que modificamos eh apenas a formatacao do output para coletar apenas a
%%   mensagem e nao o stack trace.
%%   }


\subsection{Prioritization}
\label{sec:prioritization}

We prioritized warnings based on their types, reflecting likelihood of
manifesting a real bug. We defined two types---``\hi{}'' and
``\lo{}''.

Warnings of the kind ``\hi{}'' are associated with the cases where the
test code executes without violating any internal checks, but it
violates an assertion declared in the test itself or its harness. The
rationale is that the test data is more likely to be valid in this
case as execution does not raise exceptions in application
code. Warnings of kind ``lo'' cover the remaining cases. These
warnings are more likely to be associated with invalid inputs. They
reflect the cases where the anomaly is observed during the execution
of application functions as opposed to assertions. We observed that
different engines often check pre-conditions of functions
differently. It can happen, for example, that one engine enforces a
weaker pre-condition, compared to another engine, on the inputs of a
function and that is acceptable.
% \Comment{ For instance, it is acceptable to pass values
% greater than \Fix{x} to function \CodeIn{\Fix{WWW}} in \Fix{y} but not in \Fix{z}.}
In those cases, the infrastructure would report a warning that is more likely to be
associated with an invalid input produced by the fuzzer, \ie{}, it is
likely to be a ``bug'' in the test code as opposed to a bug in the
engine. Recall that, for differential testing, we only use seed tests
that pass in all engines.

Despite the problem mentioned above, ``\lo'' warnings can reveal bugs.
Figure~\ref{fig:lo-truepositive} shows one of these cases. In this
example, the test instantiates an \CodeIn{ArrayBuffer} object and
stores an 8-bit integer at the 0 position. According to the
specification~\cite{ecmas262-getviewvalue}, a \CodeIn{RangeError}
exception should be thrown if a negative value is passed to the
function \CodeIn{ToIndex}, indirectly called by the test case from the
function call \CodeIn{getInt8()}. In this case, however, the \chakra{}
engine did not throw any exception, as can be confirmed from the
report that our infrastructure produces starting with text ``Engine
Messages'' at the bottom of Figure~\ref{fig:lo-truepositive}. This is
a case of undocumented precondition. It was fixed by
\chakra\ developers and is no longer present in the most recent
release of \chakra.

\subsection{Clusterization}
\label{sec:clusterization}

%% Prioritization helps to steer our focus to the warnings more likely to
%% reveal bugs, but there could be several tests producing similar
%% warnings.

Clusterization is complementary to prioritization; it helps to group
similar warnings reported by our infrastructure. We only clustered
``\lo'' warnings as ``\hi'' warnings produce messages that arise from
the test case, which are typically distinct.

Figure~\ref{fig:lo-truepositive} shows, at the bottom, a sequence of
three elements that we use to characterize a warning--1) the
identifier of an engine, 2) the exception it raises, and 3) the
message it produces on a ``\lo'' warning.  This sequence of triples
defines a warning signature that we use for clustering. It is worth
mentioning that we filter references to code in messages as to
increase ability to aggregate warnings. Any warnings, including this
one, that has this same signature will be included in the same
``bucket''. Considering the example from
Figure~\ref{fig:lo-truepositive}, the signature for that cluster will
be [(JavaScriptCore, ``RangeError'', ``byteOffset cannot be
  negative''), (SpiderMonkey, ``RangeError'', ``invalid or
  out-of-range index''), (V8, ``RangeError'', ``Offset is outside the
  bounds of the DataView'')].

\begin{figure}[t!]
  \centering
  \begin{lstlisting}
var buffer = new ArrayBuffer(64);
var view = new DataView(buffer);
view.setInt8(0,0x80);
assert(view.getInt8(-1770523502845470856) === -0x80);

Engines Messages (1:V8, 2:JavaScriptCore, 3:SpiderMonkey):
1. RangeError: Offset is outside the bounds of the DataView
2. RangeError: byteOffset cannot be negative
3. RangeError: invalid or out-of-range index
  \end{lstlisting}
  \caption{\label{fig:lo-truepositive}Example of a ``\lo'' warning.}
    % that led to a confirmed bug report in \chakra.}
    % The bug is caused by a required precondition check in the implementation of function
    % \CodeIn{ToIndex}~\cite{ecmas262-getviewvalue}, which is indirectly called by the test.
\end{figure}

\subsection{Fuzzers}
\label{sec:objects:fuzzers}

Fuzzers are typically categorized in two main groups--those that build
inputs anew (generational) and those that modify existing inputs
(mutational). We used two black-box mutational
fuzzers %\Comment{Radamsa~\cite{radamsa} and QuickFuzz~\cite{quickfuzz}}
in this study. In the following, we provide rationale for this
selection.

Generational fuzzers are typically grammar-based. These fuzzers
generate a new file using the grammar of the language whose inputs
should be fuzzed. Intuitively, those fuzzers implement a traversal of
the production rules of the input grammar to create syntax trees,
which are then pretty-printed. Consequently, this approach produces
inputs that are syntactically valid by construction. We analyzed four
grammar-based fuzzers--Grammarinator~\cite{grammarinator},
jsfunfuzz~\cite{jsfunfuzz},
LangFuzz~\cite{Holler:2012:FCF:2362793.2362831}, and
Megadeth~\cite{grieco2016quickfuzz}.  Unfortunately, none of those
were effective out of the box. For example, we produced 100K inputs
with Grammarinator and only few inputs were valid. With Megadeth, we
were able to produce more%\Comment{ \Fix{Y, Y$>$Y?}}
valid inputs as it contains some heuristics to circumvent 
violations of certain typing rules. %\Comment{ such as \Fix{variable used must be defined?}.}
Nonetheless, running those inputs in our infrastructure we were unable
to find discrepancies. Inspecting those inputs, we realized that they
reflected very simple scenarios. To sum up, a high percentage of inputs
that Grammarinator and Megadeth generated were semantically-invalid
that we needed to discard whereas the valid inputs manifested no
discrepancies. Considering jsfunfuzz~\cite{jsfunfuzz}, we noticed
that, in addition to the issues mentioned above, it produces inputs
that use functions that are only available in the \smonkey{}
engine. We would need either to mock those functions in other engines
or to discard those tests. Considering
LangFuzz~\cite{Holler:2012:FCF:2362793.2362831}, the tool is not
publicly available. Another fundamental issue associated with
generational fuzzers in our context is that the tests they produce do
not contain assertions; to enable the integration of this kind of
fuzzers in our infrastructure---we would need to look for
discrepancies across compiler error messages as opposed to assertion
violations.  All in all, although grammar-based fuzzers have been
shown effective to find real
bugs~\cite{Holler:2012:FCF:2362793.2362831}, we did not consider those
fuzzers in this study for the reasons above.

%% Our
%% infrastructure supports any grammar fuzzer with a few
%% adjusts. However, we try to integrate several grammar-based fuzzers,
%% for example
%% and \Fix{others fuzzers} to
%% generate new JavaScript files based on grammar, but after several runs
%% it was observed that this approach was ineffective due the amount of
%% invalid files and/or files without discrepancies.
%% For example, if we
%% ran Grammarinator to generate 1K JS files ten times with a random seed
%% generation, we obtained \Fix{XX\%} of valid files. Checking in our
%% environment almost \Fix{XX\%} are js files that shows undefined
%% variables and due the differential testing in our environment all
%% engines will raise a SyntaxError and this approach was not relevant to
%% our experiment.

%% We initially considered used representatives of popular fuzzing approaches. For
%% random-based fuzzing we used Radamsa~\cite{radamsa}; for
%% coverage-based fuzzing we used
%% \Fix{AFL~\cite{afl}/libfuzzer~\cite{libfuzzer}?}, and for
%% generative-based fuzzing we used
%% \Fix{grammarinator,jsfunfuzz?}. Details on how these fuzzers work can
%% be found elsewhere~\cite{fuzz-bart}.

Mutational fuzzers can be either white-box or black-box. White-box
mutational fuzzers are typically coverage-based. American Fuzz Lop
(AFL)~\footnote{American Fuzz Loop. Available at \url{http://lcamtuf.coredump.cx/afl/}} 
and LibFuzzer~\footnote{LibFuzzer. Available at \url{https://llvm.org/docs/LibFuzzer.html}}
are examples of this kind of fuzzers.
These fuzzers run tests inputs against instrumented
versions of the program under testing with the typical goal of finding
universal errors like crashes and buffer overflows. The instrumentation
adds code to collect branch coverage and to monitor specific
properties\footnote{There are options in the clang toolchain to build
  programs with fuzzing instrumentation~\cite{libfuzzer}. clang
  provides several sanitizers for property
  checking~\cite{clang-documentation}.}. AFL uses coverage to determine
inputs that uncover a new branch and hence should be fuzzed more
whereas libFuzzer uses evolutionary generation--it tries to minimize
the distances to still-uncovered branches of the program. AFL takes
the instrumented program binary (say, a JS engine) and one seed input
to that program (say, a JS program) and produces on output
fault-revealing inputs, if found. Considering our context of
application, we needed to instrument one runtime engine for fuzzing.
We chose \veight\ for that. Unfortunately, we found that most of the inputs produced
by AFL violate the JS grammar. Furthermore, the fuzzing task can take
days for a single seed input and there is no simple way to guide the
exploration\footnote{Exchanged emails with the tool author.}. That
happens because the fuzzer aims to explore the entire decision tree
induced from the engine's main function, including the branches
associated with the higher layers of the compiler (\eg{}, lexer and
parser). It is worth mentioning that Google mitigates that problem
with libFuzzer by asking developers to create fuzz targets for
specific program
functions~\cite{libFuzzer-tutorial-google,libFuzzer-chromium-google}. Although
that approach has shown to be effective, it requires domain knowledge
to create the calling context to invoke the fuzz target. For that, we
decide not to consider coverage-based in this study.

We used two black-box mutational fuzzers in this
study--\radamsa~\cite{radamsa} and \quickfuzz~\cite{grieco2016quickfuzz}. These
fuzzers require no instrumentation and domain-knowledge. They mutate
existing inputs randomly. The strength of the approach is
limited by the quality of the test suite and the supported mutation
operators, which are typically simple. We chose these specific fuzzers
because, conceptually, one complements the other. \quickfuzz\ creates
mutations like \radamsa. However, in contrast to \radamsa, \quickfuzz\ is aware
of the \js\ syntax; it is able to replace sub-trees of the syntax
tree~\cite{grieco2016quickfuzz} with trees created anew.
