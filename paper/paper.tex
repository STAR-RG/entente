\documentclass[sigconf,review, anonymous]{acmart}

\hyphenation{on-tol-o-gy ad-di-tion-al in-fra-struc-ture an-a-lyze a-nal-y-sis} 

%\input{packages}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{matrix,fit,shapes,calc,positioning,shadows,arrows,shapes,backgrounds,decorations.markings,fadings}
\usepackage{listings}
\usepackage[caption=false, font=footnotesize]{subfig}
%%%%%%%%%%%%% code listing
\renewcommand{\ttdefault}{pcr}
\lstset{
  basicstyle=\scriptsize\ttfamily,
  keywordstyle=\scriptsize\ttfamily\bfseries,
  language=Java,             % choose the language of the code
  frame=single,              % adds a frame around the code
  aboveskip=0pt,
  belowskip=0pt,
  breaklines=true,           % sets automatic line breaking
  breakatwhitespace=false,   % sets if automatic breaks should only happen at
  showspaces=false,
  %numbersep=5pt,              % Abstand der Nummern zum Text
  %tabsize=2,                  % Groesse von Tabs
  %extendedchars=true,         %
  %breaklines=true,            % Zeilen werden Umgebrochen
  keywords=[2]{class, incorporateUserFeedback, testPushPop, testPopPush},
}
\usepackage{balance}
\usepackage{wrapfig}
\usepackage{enumitem}

\input{macros}

% Copyright
%\setcopyright{none}
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[ESEC/FSE 2019]{The 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}{26--30 August, 2019}{Tallinn, Estonia}
\acmYear{1997}
\copyrightyear{2016}

\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
\editor{Jennifer B. Sartor}
\editor{Theo D'Hondt}
\editor{Wolfgang De Meuter}


\begin{document}


\title{Leveraging Diversity to Find Bugs in JavaScript Engines}
%% \titlenote{Produces the permission block, and
%%   copyright information}
%% \subtitle{Extended Abstract}
%% \subtitlenote{The full version of the author's guide is available as
%%   \texttt{acmart.pdf} document}


%mf: the following follow acm format and prevents to have superscripts in the header of every page
\settopmatter{authorsperrow=4}

\author{Igor Lima}
%\authornote{Dr.~Trovato insisted his name be first.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Federal University of Pernambuco}
  \streetaddress{P.O. Box 1212}
  \city{Recife}
  \state{PE}
  \country{Brazil}
%  \postcode{43017-6221}
}
\email{isol2@cin.ufpe.br}


\author{Marcelo d'Amorim}
\orcid{/0000-0002-1323-8769}
\affiliation{%
  \institution{Federal University of Pernambuco}
  \streetaddress{P.O. Box 1212}
  \city{Recife}
  \state{PE}
  \country{Brazil}
%  \postcode{43017-6221}
}
\email{damorim@cin.ufpe.br}


% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{Lima and d'Amorim}


\begin{abstract}
  \input{abstract}
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
%% \begin{CCSXML}
%% <ccs2012>
%% <concept>
%% <concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
%% <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
%% <concept_significance>500</concept_significance>
%% </concept>
%% </ccs2012>
%% \end{CCSXML}

%% \ccsdesc[500]{Software and its engineering~Software testing and debugging}

%% \keywords{JavaScript, Diversity, Software Testing}

\maketitle

\section{Introduction}

JavaScript (\js{}) is one of the most popular programming languages
today~\cite{stackify,redmonk-javascript}, with penetration in various software development segments including, web, mobile,
and, more recently, the Internet of
Things~(IoT)~\cite{simply-technologies}. The interest of the community
for the language encourages constant improvements in its specification~\cite{ecmas262-spec}. It is natural to expect that such improvements
lead to sensible changes in engine implementations~\cite{kangax}. Even small
changes can have high practical impact. For example, in October 2014 a
new attribute added to Array objects resulted in the MS Outlook
Calendar web app, which is built in \js{}, to fail under 
Chrome~\cite{array-bug-chromium-issue4247,array-bug-discussion}.

Finding bugs in \js\ engines is an important problem given the range
of applications that could be affected with those bugs. It is also
challenging.  Specifications are intentionally incomplete as to enable
development flexibility. In addition, they evolve frequently to
accommodate the pressing demands from
developers~\cite{ecmas262-spec-repo}. An official conformance test
suite exists for \js~\cite{tc39-github}, but, naturally, many test
scenarios are not covered in the suite. In addition, we noticed that a
significant fraction (5 to 15\%) of the tests fail regularly in the
most popular engines, reflecting the struggle of developers in keeping
up with the pace of spec evolution (see Table~\ref{tab:test262}).

%% WHAT WE DID
This paper reports on a study to evaluate the importance of diversity
to find bugs in JavaScript engines; it covers two complementary
diversity-aware testing techniques.

%\vspace{-1ex}
%noitemsep,
\begin{itemize}[topsep=0pt,parsep=0pt,partopsep=2pt,labelwidth=0cm,align=left,itemindent=-0.25cm]
\item \emph{Test transplantation} leverages diversity of test
  cases. It evaluates the effects that running test files from a given
  engine have in detecting bugs in other engines. The intuition is
  that developers design test cases with different objectives in
  mind. As such, replaying these tests in different engines could
  reveal unanticipated problems.

\item \emph{Cross-engine differential testing} leverages diversity of
  engine implementations. More precisely, it fuzzes existing test
  inputs~\cite{fuzz-testing-history} and then compares the output
  produced by different engines using a differential oracle. The
  intuition is that similar inputs to those already tested may reveal
  problems.
\end{itemize}

Our study measures the ability of these techniques in finding bugs and
the impact of false positives, which can arise in both testing
scenarios, on the practicality of the approach.

%One of
%various causes of output discrepancy is a bug.

%% It automates test generation in scenarios where multiple
%% implementations of a system exist. DT leverages diversity across
%% system's implementations to detect anomalous behavior.

The idea of test set diversity dates back to the
eighties~\cite{white-cohen-tse1980, ostrand-balcer-1988}. The
assumption behind test set diversity is that faults often spread
contiguously in the input domain. Consequently, it should be
beneficial to partition the input domain and explore it more evenly as
to find bugs faster. Later in the nineties, Chen and
Yu~\cite{chen-yu-tse1996} analyzed numerical software and confirmed
the continuity assumption. They found that faults, most open, manifest
themselves in regular patterns across the input
domain~\cite{Chen:2010:ART:1663656.1663914,7515474}.  Such
observations led researchers to investigate generalizations of the
approach to non-numerical data types and strategies to explore the
input (or output) space to solve different problems in Software
Engineering (\eg{}, test input generation and test
selection)~\cite{mayer-ase2005,bueno-etal-ase2007,ciupa-etal-icse08,alshahwan-harman-icse2012,alshahwan-harman-issta2014,7515474}. This
paper explores diversity of sources of test cases and diversity of
engines to test and find bugs. We remain to investigate the diversity
of test cases themselves. In principle, that could help reduce the
number of alarms to inspect, for example.

\emph{Related Ideas.}~The idea of test set diversity dates back to the
eighties~\cite{white-cohen-tse1980, ostrand-balcer-1988}. In constrast
to prior work on test set diversity, this paper explores diversity of
sources of test cases and diversity of implementations with the goal
of finding bugs. Section~\ref{sec:related-work} elaborates related
work on this topic. Differential Testing~\cite{Brumley-etal-ss07} (DT)
has been applied in a variety of contexts to find
bugs~\cite{Yang-etal-pldi11,Chen-etal-fse2015,Argyros-etla-ccs16,Chen-etal-pldi16,petsios-etal-sp2017,SivakornAPKJ17,Zhang:2017:ATD:3097368.3097448}.
It has shown to be specially practical in scenarios where the
observation of difference gives a strong signal of a real problem. For
example, Mozilla runs JS files against different configurations of
a given build of their \smonkey\ engine (\eg{}, trying to enable or
not eager JIT compilation\footnote{These files are created with
  the grammar-based fuzzer jsfunfuzz~\cite{jsfunfuzz}. Look for option
  ``compare\_jit'' from funfuzz.}). A positive aspect of the approach is that
it can be fully automated---as only one engine is used, the outcomes
of the test in both configurations are expected to be identical. The
Mozilla team uses this approach since 2002; they have been able to
find over 270 bugs since then~\cite{jsfunfuzz-at-mozilla}, including
security bugs. Cross-engine differential testing, in contrast, has not
been widely popularized. One possible reason is that it is still not
practical to fully automate the technique. In contrast to other
applications of differential testing, a number of legitimate reasons
exist, other than a bug, for a test execution to manifest discrepancy
(see Tables~\ref{fig:piecharts-transplantation} and
~\ref{tab:false-positives}). To sum, variations of these ideas have
been explored before in different contexts.  The goal of the paper is
to assess the ability of these techniques aforementioned in finding
bugs on \javascript\ engines.

%% This paper evaluates the benefits of
%% test case diversity and engine diversity in finding bugs on
%% \javascript\ engines.

%% Recently, Patra and
%% Pradel~\cite{patra2016learning} proposed a language-agnostic fuzzer to
%% find cross-browser HTML+JS discrepancies. The main difference of our
%% work to theirs is in goal; our goal is to evaluate ability and cost of
%% simple diversity-aware approaches to find bugs in \js\ engines.

%% Their main contribution was
%% analytical whereas our main contribution is empirical--


\emph{Results.}~We considered the following engines--Apple, Google,
Microsoft, and Mozilla. Figure~\ref{fig:summary} shows the breakdown
of bug reports per engine (\ref{fig:stacked-engine}) and per technique
(\ref{fig:stacked-technique}). Additionally, each stacked bar breaks
down the bugs per status (\eg{}, ``1-New''). The prefix number
indicates the ordering that status labels are assigned. Note that
several of these reports have the label ``3-Fixed'', indicating that
bug fixes have been incorporated into the code already. Note that most
of these bugs affected two engines--Microsoft's \chakra{} and Apple's
\jsc{}.  We reported \noBugsBugsReportedGoogle{} bugs in Google
\veight{} (\noBugsBugsConfirmedGoogle{} confirmed) and none in
Mozilla's \smonkey{}.  Furthermore, our results show that both
techniques revealed several bugs, most of which confirmed by
developers. Test transplantation revealed \noBugsTransplantation{}
bugs (of which, \noBugsTransplantationConfirmed{} were confirmed and
\noBugsTransplantationFixed{} were fixed) whereas differential testing
revealed \noBugsDifferentialTesting{} bugs (of which,
\noBugsDifferentialTestingConfirmed{} were confirmed and
\noBugsDifferentialTestingFixed{} were fixed).  Overall, results
indicate that both techniques were successful at finding bugs. The
number of bug reports were similar, if we consider only those
confirmed or fixed. Considering cost per bug found, we found that test
transplantation, compared to differential testing, demanded less
effort from the developers who inspected the alarms. Most of the bugs
we found are of moderate severity.

%% All in all, these simple techniques
%% seem promising to improve reliability of engines that continuously
%% evolve with their specifications.

\emph{Key Findings.}~1) Not only multiple different implementations
can be leveraged in differential testing, but differences in test
suites can also be important. 2) Even for problems with fairly clear
specifications, as in \javascript{}, there is likely to be (a lot of)
variation between different implementations. 3) Differential testing
is feasible on real, complex, widely used pieces of software.


%% LIST OF CONTRIBUTIONS
\emph{Contributions.}~This paper contributes the following: 1)~A
comprehensive study to analyze the effectiveness of diversity-aware
techniques to find functional bugs in popular \javascript\ engines.
2)~A number of bugs found and fixed. We reported a total of
\totalBugsReported{} bugs. Of these, \totalBugsConfirmed{} bugs were
confirmed and \totalBugsFixed{} bugs were fixed. 3)~An infrastructure
for diversity-aware testing. We made our testing infrastructure,
results, and experimental scripts available to the public. (upon
request for anonymity.)

\vspace{1ex} To sum, although more research needs be done to automate
parts of the inspection process (\eg{}, reduction of false alarms),
the paper provides initial, yet strong evidence that exploring
diversity should be encouraged to find functional bugs in JavaScript
engines.

\begin{figure}[t]
%  \vspace{-2ex}  
  \centering
  \subfloat[\label{fig:stacked-engine}Bug reports per engine.]{
    \includegraphics[trim=0 190 0 180,clip,width=0.45\textwidth,scale=0.4]{R/stackedbar/stacked-engine}
    \vspace{-5ex}
  }\\
  \subfloat[\label{fig:stacked-technique}Bugs reports per technique.]{
    \includegraphics[trim=0 180 0 180,clip,width=0.45\textwidth,scale=0.4]{R/stackedbar/stacked-technique}
    \vspace{-5ex}        
  }
  \caption{\label{fig:summary}Summary of bug reports.}
  \vspace{-3ex}
\end{figure}

\section{JavaScript}
\label{sec:es6-design}
\label{sec:imp-dep-behavior}

JavaScript engines are virtual machines that parse source code,
compile it in bytecodes, and run these bytecodes. These engines
implement some version of the ECMAScript (\es{}), which emerged with
the goal to standardize variants of the language, such as Netscape's
JavaScript and Microsoft's JScript\footnote{The name JavaScript still
  prevails today, certainly for historical reasons.}. The \es{}
specification is regulated by Ecma International~\cite{es6-website}
under the TC39~\cite{tc39-github} technical committee.  Every year, a
new version of the \es{} specification is produced with new features
and minor fixes. The canonical spec today is
ES6~\cite{ecmas262-spec-repo,ecmas262-spec}.

%% In the following, we briefly describe different sources of
%% discrepancies that can emerge across different engine implementations
%% as results of the active changes in the \es{} specification.
%% .........
%% A compatibility table relating features and supporting engines is
%% publicly available~\cite{kangax}.

The specification of JavaScript is incomplete for different
reasons. Certain parts of the specification are undefined; it is
responsibility of engineers to decide how to implement these
functionalities. The JavaScript spec uses the label
``implementation-dependent'' to indicate these cases, where behavior
may differ from engine to engine. One reason this flexibility in the
spec exists is to enable compiler optimizations. For example, the
\js\ \mcodeid{for-in} loop construct does not clearly specify the
iteration order of
elements~\cite{so-forin-undefined,javascript-in-chrome} and different
engines capitalize on that for loop
optimizations~\cite{for-in-undefined}.  As another example, the
specification states that if the \mcodeid{Number.toPrecision()}
function is called with multiple arguments then the floating-point
approximation is
implementation-dependent~\cite{es6-toPrecision}. Various other cases
like these exist in the specification. Given the speed the
specification changes and the complexity of the language some features
are not fully implemented as can be observed by the Kangax
compatibility table~\cite{kangax}.  It is also worth noting that, as
in other languages, some elements in JS have non-deterministic
behavior (\eg{}, \mcodeid{Math.random} and \mcodeid{Date}). A test
that make decisions based on these elements could, in principle,
produce different outcomes on different runs. Carefully-written test
cases should not manifest this kind of flaky behavior.  As mentioned
previously, all those aspects make testing \js\ engines more
challenging.


%\subsection{Randomness}


%\subsection{\Fix{Other}}

%% 1) The stack depth in JS is often close to the real (native) C stack
%% depth, nevertheless observable by JS through recursion. This causes
%% differential behavior even on the same engine when JITs are enabled or
%% disabled, as the stack frame sizes vary. LangFuzz in particular
%% produces a lot of recursions and hits these problems even more often
%% than jsfunfuzz.\Mar{ask C. Holler. it is unclear to me how the output
%%   could be different.}

%% \section{Objects of Study}
%% This section discusses the objects we used in our study.

\section{Engines Studied}
\label{sec:methodology}
\label{sec:methodology:engines}~We selected 
JS engines according to the following criteria: 1) Released latest
version after Jan 1, 2018, 2) Contains more than 1K stars on GitHub,
and 3) Uses a public issue tracker. We looked for highly-maintained
(as per the first criterion) and popular (as per the second criterion)
engines. As we wanted to report bugs, we also looked for project with
public issue trackers. Table~\ref{tab:engines} lists the engines we
analyzed. It is worth noting that we used Google Chrome Lab's JSVU
tool~\cite{jsvu} to automatically install and configure versions of
different JS engines in our host environment. This is important as we
aim to use the most recent stable versions of each engine as to avoid
reporting old and already-fixed bugs to developers.

%% Table~\ref{tab:engines} shows the engines we selected in this
%% study. We made an exception to the second criterion with
%% XS~\cite{xs2018repo}. As the project is young, created in Oct 2017, we
%% thought there was insufficient time to obtain 1K stars for XS. We
%% still considered this project as it seems to be attracting interest
%% from the community\Fix{is this true?}

\begin{table}[t]
  \small
  \centering
  \caption{\label{tab:engines}Engines selected.}
  \begin{tabular}{cccrr}
    \toprule
    Team & Name & URL & \# Stars  & DOB \\
    \midrule
    Apple & \jsc{} (WebKit) & \cite{jsc2018repo} &
    \multicolumn{1}{c}{3300+} & Jun 2001\\
    Google & \veight{} & \cite{v82018repo} & 9800+ & Jun 2008\\
    Microsoft & \chakra{} & \cite{chakra2018repo} & 7200+ & Nov 2009\\
    Mozilla & \smonkey{} & \cite{spidermonkey2018repo} &
    \multicolumn{1}{c}{1100+} & Mar 1996\\
   \bottomrule     
  \end{tabular}
\end{table}

\section{JS Files Mined}
\label{sec:seeds}
A good test set is critical for finding bugs with the techniques used
in this paper. For that reason, we looked for \js\ files from various
sources: 1) test files from the Test262~\cite{tc39-github} conformance
suite of the ECMA262 specification~\cite{ecmas262-spec}, 2) test files
from the test suite of the selected engines (see
Section~\ref{sec:methodology:engines}); these files are accessible
from the engine's official repositories, 3) test files from the suites
of public engines, and 4) test files mined from issue trackers of
these engines.

%% accessible from GitHub using the GitHub REST
%% API~\cite{github-rest-api}.

Table~\ref{tab:test-suites} shows the breakdown of tests per
engine. Column ``full'' shows the number of test cases associated with
each engine. Column ``pass-in-par.''  shows the number of test cases
that pass in their parent engine. We discarded tests that fail in
their parent engine as we could not reliably indicate the reason for
the failure, so we assumed the test could be broken. We removed \testsThatFail{}
test cases that fail for that reason--\testsThatFailJSC{} tests from \jsc{} and 
\testsThatFailSM{} tests from \smonkey. Column ``type-in-all'' 
shows the number of test cases whose executions do not throw 
dynamic type errors in any of the engines because of an undefined variable or property. 
These cases were captured by looking for the presence of \CodeIn{ReferenceError} and
\CodeIn{TypeError} in the output. A \CodeIn{ReferenceError}
(respectively, \CodeIn{TypeError}) is raised when test execution
attempts to access an undefined variable (respectively, property of an
object). We discarded those tests to avoid noise in the experiments as
they clearly indicate some missing feature as opposed to bugs. For
example, some tests use non-portable names (\eg{}, \jsc{}'s
\CodeIn{drainMicrotasks()} and \smonkey{}'s \CodeIn{Error.lineNumber})
or use functions that, albeit part of the spec, not all engines
currently support. For the evaluation of test transplantation, we used
the \totalTestFilesForTestTransplantation{} tests included in the
dashed rectangle under column ``type-in-all'', \ie{}, all tests under
that column but the Test262 tests. We did not consider tests from the
conformance suite as they are more likely to indicate missing features
as opposed to bugs. In addition, engine developers have access to
these test and are encouraged to run them. Finally, column
``no-fail-in-all'' shows the tests for which all engines pass. Note
that the set of tests in this column is a subset of the
``type-in-all'' test set. This test set is used in the evaluation of
differential testing as fuzzing seeds. The guarantee that they pass in
all engines assures that, if discrepancies occur, they are related to
the changes in the input as opposed to the original cause of failure.

\newcommand\marktopleft[1]{%
    \tikz[overlay,remember picture] 
        \node (marker-#1-a) at (0,2ex) {};%
}
\newcommand\markbottomright[1]{%
    \tikz[overlay,remember picture] 
        \node (marker-#1-b) at (0,0) {};%
    \tikz[overlay,remember picture,thick,dashed,inner sep=2pt]
        \node[draw,rectangle,fit=(marker-#1-a.center) (marker-#1-b.center)] {};%
}

\begin{table}[t]
  \small
  \centering
  \caption{\label{tab:test-suites}Number of test files. Dashed
    rectangle under column ``type-in-all'' (resp., ``no-fail-in-all'')
    shows the tests used for test transplantation (resp., cross-engine
    differential testing).}  \setlength{\tabcolsep}{2pt}
  \begin{tabular}{ccrrrr}
    \toprule
    \multirow{2}{*}{Name}      &  \multirow{2}{*}{Source} &
    \multicolumn{4}{c}{\# JS files} \\
    \cline{3-6}
                               &         & total & pass-in-par. & type-in-all &  no-fail-in-all \\
    \midrule
    Test262 & \cite{ecma262-conformance-suite} & 31,276 & - &  29,846 & \marktopleft{c2}17,639 \\
    \midrule
    \jsc{} & \cite{webkit} & 1,031 & 1,025 &\marktopleft{c1}1,021 & 955\\
    \smonkey\ & \cite{mozilla} & 2,634 & 1,977 & 2,116 & 1,686\\
    \veight{} & \cite{v8} & 75 & 75 & 57 & 57\\
    \midrule
    Duktape & \cite{duktape} & 1,195 & 1,195 & 921 & 915\\ 
    \jerry{} & \cite{jerryscript} & 1,951 & 1,951 & 1,878 & 1,837\\
    JSI & \cite{jsi} & 99 & 99 & 63 & 63\\
    Tiny-js & \cite{tinyjs} & 49 & 49 & 37\markbottomright{c1} & 37\markbottomright{c2}\\
    \midrule
     &  & \totalTestFiles{} & \totalTestFilesPassInPar{} & \totalTestFilesCompileInAll{} & \totalTestFilesPassInAll{}\\
   \bottomrule 
  \end{tabular}
\end{table}


%% The main source of test
%% cases is the official Test262~\cite{tc39-github} conformance test
%% suite for the ECMA262 specification~\cite{ecmas262-spec}.\Comment{
%%   This test suite contains 87\% of all the tests we used.} We also
%% considered test suites of three of the four engines mentioned in
%% Section~\ref{sec:methodology:engines} and four other engines. Overall,
%% we selected a total of \totfiles{} JS
%% files.

\lbrack{}Cleansing\rbrack{}~We noticed that some of the tests we found
depend on external libraries, which not all selected engines
support. We decided to discard those. For example, many tests we found
required a Node.js runtime~\cite{node} for execution. Also, we did not
consider tests from the \chakra{} repository because they depend on
non-portable objects. \Fix{fix... the number will increase}
Finally, note that the number of tests in
\veight\ is low; that happens because \veight{} uses many tests from
Mozilla and \jsc{}; we discarded those to avoid repetition and to give
credit where it is due. \lbrack{}Test Harness\rbrack{}~It is also
worth mentioning that some engines use a custom shell to run tests,
including a harness with specific assertions.  For that, we needed to
make minor changes in the testing infrastructure to be able to run the
tests uniformly across all engines. More precisely, we needed to mock
non-portable harness functions, which are only available in certain
engines.

\subsection{Mining Tests from Issue Trackers}

\sloppy{}We observed that issue trackers are an important source of
test data and should not be ignored. Analyzing a sample of issues, we
observed that developers either 1) add test cases as attachments of
issues or 2) embed test cases within the textual description of an
issue. The test cases in attachments are longer compared to the test
cases embedded in issue descriptions whereas the latter are more
common. Consequently, we thought we should handle both
cases.

% (\jsc{}=147, \veight{}=370, and \smonkey{}=110).
% (\jsc{}=\Fix{XX}, \veight{}=\Fix{XX}, \smonkey{}=\Fix{XX})
To obtain test files included as attachments, we wrote a crawler to
visit the issue trackers of all engines listed in
Table~\ref{tab:engines} and we were able to retrieve a total of
\filesAttached{} files.  To mine tests from the textual descriptions we
proceeded as follows. First, we broke the text describing the issue in
paragraphs and used a binary classifier to label each paragraph as
``code'' or ``not code'' (\ie{}, natural language). Then, based on
that information, we merged consecutive paragraphs labeled as ``code''
and used a \js\ parser to check well-formedness of the retrieved code
fragment. Using that method we were able to retrieve a total of
\Fix{XX} files. All those files were included in
Table~\ref{tab:test-suites}.

For the classification of ``code'' vs. ``not code'', we used a
pipeline of two Neural Networks (NN), a popular design for solving NLP
classification problems~\Fix{cite}. The first net in the pipeline
takes as input an arbitrary sentence and produces on output a
characterization vector for that sentence. The second net in the
pipeline takes that vector on input and the class for that attribute
(\ie{}, ``(not) code'') and produces a yes/no answer, determining
whether or not the input sentence was code. More in detail, for the
first net, we used word2vec~\cite{kusner2015word}, a popular NLP
technique to produce word embeddings. A word embedding is a mapping of
words to vectors of real numbers. For the second net, we used a
multi-layer perceptron~\Fix{cite} to infer the probability of the
input belonging or not to the class. We classify as code an input for
which the pipeline produces a probability of 0.7 or higher. We used a
corpus with 25K samples of English paragraphs and 25K snippets of
\js{} code and obtained a classifier with accuracy of 98\%. This
classifier is publicly available from our website as a separate
component.

%% With that mapping it is
%% possible to measure distance between two sentences as the distance
%% between the vectors characterizing them. Consequently, one can
%% measure, for example, the minimum distance between a sentence in
%% English and a sentence in a \js\ embedding, built with word2vec using
%% a corpus of \js\ sentences. \Mar{Please confirm if this is the idea!
%%   If yes, shouldn't just one corpus (e.g., the JS corpus) be enough to
%%   solve the problem? Are there two separate embeddings (one for
%%   English and one for JS)? If not, how they are combined? What about
%%   the classification scheme?  It needs to answer yes/no (not a
%%   distance).}  

%% For space, we summarize the solution that worked the best in
%% the following. The classifier that performed best for this case uses a
%% standard Embeddings NLP technique~\Fix{cite}\Mar{needs to explain what
%%   this ``Embeddings'' thing does at a high-level. Also, if this stuff
%%   exists why we did not find a solution off-the-shelf?}  that builds
%% on a neural net~\Fix{cite} to keep a dictionary of words and the
%% similarity distance among them\Mar{but we are talking about
%%   sentences. how does it go from words to sentences? Note that many
%%   words from the ``code'' vocabulary are from English!}. In more
%% detail, the classification model uses a Convolutional Neural
%% Network~\Fix{cite} classifier with two layers\Mar{why two? is there a
%%   meaning for each layer?} and has been trained and evaluated using a
%% dataset with 25K samples of English paragraphs and 25K snippets of
%% \js{}. We ran a standard \Fix{X}-repeated 10-fold cross validation to
%% evaluate the model and obtained a predicted accuracy of
%% \Fix{98\%}. This classifier is publicly available from our website as
%% a separate component.

%% N\'os mineramos as issues em bug trackers dos engines javascript mais conhecidos. 
%% Foi necessario capturar tanto os arquivos javascript anexados quanto os codigos escritos nos reports.
%% Para obter o codigo extraido dos reports, foi criado um modelo de aprendizagem para reconhecer
%% se um bloco de texto eh um bloco de codigo.

%% Motivacao:
%% Utilizamos a tecnica de Embeddings que eh reconhecida pela comunidade pelo seu uso em PLN (Processing Language Natural),
%% onde atraves de uma rede neural, consegue-se montar um dicionario de palavras com a distancia 
%% de similaridade entre elas.
%% Para treinamenteo, foi utilizado uma rede neural simples CNN (Convolutional Neural network) com apenas 2 camadas, uma de entrada e outra de saida.
%% Utilizando as variaveis \Fix{XYZ} e as funcoes \Fix{XYZ}. Obtivemos 98\% de acuracia em nosso treinamento.
%% (melhorar definicao do modelo)

%% Inputs:
%% Para nosso modelo de dados, foi necessario gerar um arquivo CSV com cerca de 50K de dados extraidos
%% de arquivos javascript (nossas seeds) e de linguagem natural, utilizamos 25K para cada classe.
%% Os rotulos das classes sao definidas em 0 (is not code) ou 1 (is code).

%% Com isso, extraimos cerca de \Fix{X} codigo e apos realizar o parser para validar o arquivo, obtivemos um total de \Fix{X} novas seeds.

%% \Fix{...elaborate...}


%% \Fix{revise this}
%% For example to run the \jerry{} tests it was necessary 
%% use the unit-test package to run it, but with our changes we added the assertion
%% does not have an assertion in the test file
%% \Fix{add code to explain}


\section{Cross-Engine Differential Testing}
\label{sec:design}

Figure~\ref{fig:workflow} illustrates the 
differential testing infrastructure used in part of this study. It takes on
input a list of JS files and generates warnings on output. Boxes in
the figure denote encapsulation, arrowed lines denote data flow, and
the cycle icons denote repetition--the leftmost icon indicates that
each file in the input list will be analyzed in separate whereas the
rightmost icon shows that a single file will be fuzzed multiple times.
The bug-finding process works as follows. New inputs are obtained for
a given test input using some off-the-shelf input
fuzzer; Section~\ref{sec:objects:fuzzers} describes the fuzzers we selected.
The oracle checks whether or not the output produced for the fuzzed
file is consistent across all engines. In case the test passes in all
engines or fails in all engines (\ie{}, the output is consistent), the
infrastructure ignores the input. Otherwise, it considers the input as
potentially fault-revealing; hence, interesting for human
inspection. The infrastructure uses the
open-source tool eshost-cli~\cite{eshost-cli}, also used at
Microsoft, for checking output discrepancy.\Mar{any difference in
  output is considered for reporting? are there any
  filtering?}\Igor{acho que nos equivocamos ao deixar o "eshost-cli", 
  nao estamos mais utilizando isso devido a essa ferramenta nao 
  capturar certos tipo de issues (e.g. crash), retirar tambem da Figura 2.
  A respeito de filtros nos reports, nao. Nos so capturamos o output,
  o que modificamos eh apenas a formatacao do output para coletar apenas a
  mensagem e nao o stack trace.
  Mas eu lembro que se houver um crash ou algo do tipo, nos reportamos de qualquer maneira,
  mas nao tenho certeza (vou olhar no nosso codigo)
  }

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 350 0 0,clip,width=0.65\textwidth]{diff-testing-runtimes}
  \caption{\label{fig:workflow}Differential Testing infrastructure overview.}
\end{figure}


It is worth noting that a number of reasons exist, other than a bug,
to explain discrepancy (see Tables~\ref{fig:piecharts-transplantation}
and ~\ref{tab:false-positives}) and there is no clear automatic
approach to precisely distinguish false and true positives. As such, a
human needs to inspect the warning to classify the issue. As mentioned
earlier, this justifies why differential testing is challenging to
automate at the functional level. To facilitate inspection, we
prioritized warnings and clustered them in groups. We describe these
strategies in the following.

\subsection{Prioritization}

We prioritized warnings based on their types, reflecting likelihood of
manifesting a real bug. We defined two types---``\hi{}'' and
``\lo{}''.

Warnings of the kind ``\hi{}'' are associated with the cases where the
test code executes without violating any internal checks, but it
violates an assertion declared in the test itself or its harness. The
rationale is that the test data is more likely to be valid in this
case as execution does not raise exceptions in application
code. Warnings of kind ``lo'' cover the remaining cases. These
warnings are more likely to be associated with invalid inputs. They
reflect the cases where the anomaly is observed during the execution
of application functions as opposed to assertions. We observed that
different engines often check pre-conditions of functions
differently. It can happen, for example, that one engine enforces a
weaker pre-condition, compared to another engine, on the inputs of a
function and that is acceptable.\Comment{ For instance, it is
  acceptable to pass values greater than \Fix{x} to function
  \CodeIn{\Fix{WWW}} in \Fix{y} but not in \Fix{z}.} In those cases,
the infrastructure would report a warning that is more likely to be
associated with an invalid input produced by the fuzzer, \ie{}, it is
likely to be a ``bug'' in the test code as opposed to a bug in the
engine. Recall that, for differential testing, we only use seed tests
that pass in all engines.

Despite the problem mentioned above, ``\lo'' warnings can reveal bugs.
Figure~\ref{fig:lo-truepositive} shows one of these cases. In this
example, the test instantiates an \CodeIn{ArrayBuffer} object and
stores an 8-bit integer at the 0 position. According to the
specification~\cite{ecmas262-getviewvalue}, a \CodeIn{RangeError}
exception should be thrown if a negative value is passed to the
function \CodeIn{ToIndex}, indirectly called by the test case from the
function call \CodeIn{getInt8()}. In this case, however, the \chakra{}
engine did not throw any exception, as can be confirmed from the
report that our infrastructure produces starting with text ``Engine
Messages'' at the bottom of Figure~\ref{fig:lo-truepositive}. This is
a case of undocumented precondition. It was fixed by
\chakra\ developers and is no longer present in the most recent
release of \chakra.

\subsection{Clusterization}
\label{sec:clusterization}

%% Prioritization helps to steer our focus to the warnings more likely to
%% reveal bugs, but there could be several tests producing similar
%% warnings.

Clusterization is complementary to prioritization; it helps to group
similar warnings reported by our infrastructure. We only clustered
``\lo'' warnings as ``\hi'' warnings produce messages that arise from
the test case, which are typically distinct.

Figure~\ref{fig:lo-truepositive} shows, at the bottom, the identifier
of an engine, the exception it raises, and the message it produces on
a ``\lo'' warning.  This sequence of engine id, exception, and error
message triples defines a warning signature that we use for
clustering. It is worth mentioning that we filter references to code
in messages as to increase ability to aggregate warnings from
different test sources. Any warnings, including this one, that has
this same signature will be included in the same
``bucket''. Considering the example from
Figure~\ref{fig:lo-truepositive}, the signature for that cluster will
be [(JavaScriptCore, ``RangeError'', ``byteOffset cannot be
  negative''), (SpiderMonkey, ``RangeError'', ``invalid or
  out-of-range index''), (V8, ``RangeError'', ``Offset is outside the
  bounds of the DataView'')]. 

\begin{figure}[t!]
  \centering
  \begin{lstlisting}
var buffer = new ArrayBuffer(64);
var view = new DataView(buffer);
view.setInt8(0,0x80);
assert(view.getInt8(-1770523502845470856) === -0x80);

Engines Messages (1:V8, 2:JavaScriptCore, 3:SpiderMonkey):
1. RangeError: Offset is outside the bounds of the DataView
2. RangeError: byteOffset cannot be negative
3. RangeError: invalid or out-of-range index
  \end{lstlisting}
  \caption{\label{fig:lo-truepositive}Example of a ``\lo'' warning
    that led to a confirmed bug report in \chakra. The bug is caused
    by a required precondition check in the implementation of function
    \CodeIn{ToIndex}~\cite{ecmas262-getviewvalue}, which is indirectly called by the test. }
\end{figure}

\subsection{Fuzzers}
\label{sec:objects:fuzzers}

Fuzzers are typically categorized in two main groups--those that build
inputs anew (generational) and those that modify existing inputs
(mutational). We used two black-box mutational
fuzzers\Comment{Radamsa~\cite{radamsa} and QuickFuzz~\cite{quickfuzz}}
in this study. In the following, we provide rationale for this
selection.

Generational fuzzers are typically grammar-based. These fuzzers
generate a new file using the grammar of the language whose inputs
should be fuzzed. Intuitively, those fuzzers implement a traversal of
the production rules of the input grammar to create syntax trees,
which are then pretty-printed. Consequently, this approach produces
inputs that are syntactically valid by construction. We analyzed four
grammar-based fuzzers--Grammarinator~\cite{grammarinator},
jsfunfuzz~\cite{jsfunfuzz},
LangFuzz~\cite{Holler:2012:FCF:2362793.2362831}, and
Megadeth~\cite{grieco2016quickfuzz}.  Unfortunately, none of those
were effective out of the box. For example, we produced 100K inputs
with Grammarinator and only few inputs were valid. With Megadeth, we
were able to produce more\Comment{ \Fix{Y, Y$>$Y?}} valid inputs as it
contains some heuristics to circumvent violations of certain typing
rules.\Comment{ such as \Fix{variable used must be defined?}.}
Nonetheless, running those inputs in our infrastructure we were unable
to find discrepancies. Inspecting those inputs, we realized that they
reflected very simple scenarios. To sum, a high percentage of inputs
that Grammarinator and Megadeth generated were semantically-invalid
that we needed to discard whereas the valid inputs manifested no
discrepancies. Considering jsfunfuzz~\cite{jsfunfuzz}, we noticed
that, in addition to the issues mentioned above, it produces inputs
that use functions that are only available in the \smonkey{}
engine. We would need either to mock those functions in other engines
or to discard those tests. Considering
LangFuzz~\cite{Holler:2012:FCF:2362793.2362831}, the tool is not
publicly available. Another fundamental issue associated with
generational fuzzers in our context is that the tests they produce do
not contain assertions; to enable the integration of this kind of
fuzzers in our infrastructure---we would need to look for
discrepancies across compiler error messages as opposed to assertion
violations.  All in all, although grammar-based fuzzers have been
shown effective to find real
bugs~\cite{Holler:2012:FCF:2362793.2362831}, we did not consider those
fuzzers in this study for the reasons above.

%% Our
%% infrastructure supports any grammar fuzzer with a few
%% adjusts. However, we try to integrate several grammar-based fuzzers,
%% for example
%% and \Fix{others fuzzers} to
%% generate new JavaScript files based on grammar, but after several runs
%% it was observed that this approach was ineffective due the amount of
%% invalid files and/or files without discrepancies.
%% For example, if we
%% ran Grammarinator to generate 1K JS files ten times with a random seed
%% generation, we obtained \Fix{XX\%} of valid files. Checking in our
%% environment almost \Fix{XX\%} are js files that shows undefined
%% variables and due the differential testing in our environment all
%% engines will raise a SyntaxError and this approach was not relevant to
%% our experiment.

%% We initially considered used representatives of popular fuzzing approaches. For
%% random-based fuzzing we used Radamsa~\cite{radamsa}; for
%% coverage-based fuzzing we used
%% \Fix{AFL~\cite{afl}/libfuzzer~\cite{libfuzzer}?}, and for
%% generative-based fuzzing we used
%% \Fix{grammarinator,jsfunfuzz?}. Details on how these fuzzers work can
%% be found elsewhere~\cite{fuzz-bart}.

Mutational fuzzers can be either white-box or black-box. White-box
mutational fuzzers are typically coverage-based. American Fuzz Loop
(AFL)~\cite{afl} and libFuzzer~\cite{libfuzzer} are examples of this
kind of fuzzers. These fuzzers run tests inputs against instrumented
versions of the program under testing with the typical goal of finding
universal errors like crashes and buffer overflows. The instrumentation
adds code to collect branch coverage and to monitor specific
properties\footnote{There are options in the clang toolchain to build
  programs with fuzzing instrumentation~\cite{libfuzzer}. clang
  provides several sanitizers for property
  checking~\cite{clang-documentation}.}. AFL uses coverage to determine
inputs that uncover a new branch and hence should be fuzzed more
whereas libFuzzer uses evolutionary generation--it tries to minimize
the distances to still-uncovered branches of the program. AFL takes
the instrumented program binary (say, a JS engine) and one seed input
to that program (say, a JS program) and produces on output
fault-revealing inputs, if found. Considering our context of
application, we needed to instrument one runtime engine for fuzzing.
We chose \veight\ for that. Unfortunately, we found that most of the inputs produced
by AFL violate the JS grammar. Furthermore, the fuzzing task can take
days for a single seed input and there is no simple way to guide the
exploration\footnote{Exchanged emails with the tool author.}. That
happens because the fuzzer aims to explore the entire decision tree
induced from the engine's main function, including the branches
associated with the higher layers of the compiler (\eg{}, lexer and
parser). It is worth mentioning that Google mitigates that problem
with libFuzzer by asking developers to create fuzz targets for
specific program
functions~\cite{libFuzzer-tutorial-google,libFuzzer-chromium-google}. Although
that approach has shown to be effective, it requires domain knowledge
to create the calling context to invoke the fuzz target. For that, we
decide not to consider coverage-based in this study.

We used two black-box mutational fuzzers in this
study--\radamsa~\cite{radamsa} and \quickfuzz~\cite{quickfuzz}. These
fuzzers require no instrumentation and domain-knowledge. They mutate
existing inputs randomly. The strength of the approach is
limited by the quality of the test suite and the supported mutation
operators, which are typically simple. We chose these specific fuzzers
because, conceptually, one complements the other. \quickfuzz\ creates
mutations like \radamsa. However, in contrast to \radamsa, \quickfuzz\ is aware
of the \js\ syntax; it is able to replace subtrees of the syntax
tree~\cite{grieco2016quickfuzz} with trees created anew.

\section{Results}
\label{sec:results}

The goal of this paper is to assess ability of techniques that
leverage data and implementation diversity on to find functional bugs
in \javascript\ engines. Based on that, we pose three questions:
\begin{description}[leftmargin=.5in]
\item[RQ1.] How conformant are the engines to the Test262 suite?
\item[RQ2.] How effective is test transplantation to find bugs?
\item[RQ3.] How effective is cross-engine differential testing to find bugs?  
\end{description}

The first question focuses on the conformance of our selected engines
to the official Test262 suite~\cite{ecma262-conformance-suite}
(Section~\ref{sec:stability}). In the limit, bugs would have low
relevance if the engines are too unreliable. The second question
focuses on the effectiveness of test transplantation
(Section~\ref{sec:transplantation}). The rationale for using inputs
from different engines is that developers consider different goals
when writing tests---suites written for a given engine may cover
scenarios not covered by a different engine. The third question
evaluates the effectiveness of cross-engine differential testing to
find bugs (Section~\ref{sec:cross-engine-diff-testing-results}). The
rationale for this question is that fuzzing inputs may explore
scenarios not well tested by at least one of the engines.

\subsection{Answering RQ1 (Conformance)}
\label{sec:stability}

The ECMA Test262~\cite{ecma262-conformance-suite} test suite serves to
check conformance of engines to the \js\ standard. It is
acceptable to release engines fulfilling the specification only
partially~\cite{kangax}. We expect that the pass rate on this suite
provide some indication of the engine's maturity. In the limit, it is
not desirable to flood bug reports on engines at early stages of
development. 

\begin{wraptable}[10]{r}{0.22\textwidth}
%  \vspace{-2ex}      
  \centering
  \caption{\label{tab:test262}Percentage of passing tests on
    the Test262 conformance suite.}
  \vspace{-2ex}        
  \small
  \begin{tabular}{crr}
    \toprule
    engine & \% passing \\
    \midrule
    \jsc{} & \percentSuiteTestJSC{}\\
    \veight{} & \percentSuiteTestVeight{} \\
    \chakra{} & \percentSuiteTestChakra{} \\    
    \smonkey{} & \percentSuiteTestSM{} \\
    \bottomrule
  \end{tabular}
  \normalsize
  \vspace{-4ex}
\end{wraptable}
For this experiment, we ran the suite once a day for seven consecutive
days and averaged the passing ratios. Table~\ref{tab:test262} shows
the average number of passing tests over this period. The variance of
results was negligible; for that reason, we omitted standard
deviations. We noticed that all four engines but \chakra{} used some
variant of the Test262 suite as part of their regression process, but
we used the same version in this
experiment~\cite{ecma262-conformance-suite}. Results show that there
are still many unsupported scenarios as can be observed from the
percentages in the table. The number of passing tests is high and
similar for \jsc{}, \veight{}, and \smonkey\ whereas \chakra\ performs
worse compared to the other engines. Note also that \chakra\ is both
the engine that has the lowest passing ratio in this test suite and
the one we were able to find more bugs (as per
Figure~\ref{fig:summary}). Although it is plausible to find
correlation between the passing ratios and reliability as measured by
the number of bugs found, we do not imply causality. It is important
to note that failures in this conformance test suite indicates missing
features as opposed to bugs.

\begin{center}
  \fbox{
    \begin{minipage}{8cm}
      \textit{Summary:}~All engines seem to adhere well to the
      \js\ standard. Except for \chakra, the passing ratio of all
      engines is above 90\%.
      \end{minipage}
    }
\end{center}


\subsection{Answering RQ2 (Test Transplantation)}
\label{sec:transplantation}

This section reports results of test transplantation. More
specifically, we analyzed the failures observed when running a test
suite original from a given engine in another engine. Intuitively, we
want to assess how effective is the idea of cross-fertilization of
testing knowledge among \js\ developers.

%\vspace{0.5ex}
\subsubsection{Methodology}
\label{sec:methodology}
In this experiment, a developer with experience in \js\ analyzed each
test failure, affecting a particular engine, and classified that
failure as potentially fault-revealing or not. The authors supervised
the classification process to validate correctness. For the
potentially fault-revealing cases, one of the authors carefully
inspected the scenario and, if agreed on the classification, reported
the bug on the issue tracker of the affected engine.

%If not, re-classified the failure as a false alarm.

%\vspace{0.5ex}
\subsubsection{Results}
\label{sec:results}

\Fix{Igor, please check these numbers! The status of most reports should
  have changed.}

Table~\ref{tab:cross-testing} shows the number of failures observed
for each pair of test suite and engine. The first column shows the
test suites and the first row shows the engines that run those
tests. We use a dash (``-'') to indicate that we did not consider the
combinations that associate a test suite with its parent engine;
failures in those cases would either indicate regressions or flaky
tests as opposed to unknown bugs for that engine. As explained in
Section~\ref{sec:seeds}, we used the
\totalTestFilesForTestTransplantation{} tests included in the \Igor{rectangle area}
from Table~\ref{tab:test-suites} in this experiment. Running those
tests we observed a total of \failuresTestTrans{} failures manifested across 
\failuresTestTransDistictFiles{} distinct files (\failuresTestTransPercent{} of total). 
Table~\ref{tab:cross-testing} shows
that \smonkey\ was the engine that failed the least whereas
\chakra\ was the engine that failed the most. The \smonkey\ test suite
also revealed more failures than any other, perhaps as expected,
considering that it is the suite with more tests (see
Table~\ref{tab:test-suites}).

%% This suggests that the investiment in
%% testing paid off in this case.
\begin{table}[t]
  \small
  \centering
  \caption{\label{tab:cross-testing}Number of failures observed exchanging
    test suites.}
  \renewcommand*{\arraystretch}{0.9}
  \begin{tabular}{crrrr}
    \toprule
    test suite\textbackslash{}engine & \jsc{} & \veight{} & \smonkey{} & \chakra{}\\
    \midrule
    % \Fix{Versions (03.08)} & 234555 & 7.0.158 & 62.0b14 & 1.10.1 \\
    \Comment{
      Lembrar dos testes que os testes da propria engine falham:
      V8 0 
      JSC 2 
      Spidermonkey 58
    }
    \jsc{} & - & 9 & 10 & 58   \\
    \veight{} & 29 & - & 0 & 0  \\
%    \chakra{} & - & - & - & -  \\
    \smonkey{} & 218 & 107 & - & 279 \\
    Duktape & 0 & 4 & 4 & 1   \\
    \jerry{} & 23 & 25 & 22 & 23   \\
    JSI & 0 & 0 & 0 & 0   \\ 
   Tiny-js & 0 & 0 & 0 & 0  \\
    \midrule
   \textbf{total} & 270 & 145 & 36 & 361 \\
    \bottomrule 
  \end{tabular}
  \vspace{-3ex}
\end{table}

%%  As another example, some tests from \Fix{Y} use the
%% implementation-dependent description message\Mar{what is the name of
%%   the property?} encapsulated in an \CodeIn{Error} object in
%% assertions. As such, those tests fail in all engines but
%% \Fix{Y}. 

\sloppy The sources of false positives found in this experiment are as
follows: \textbf{Undefined Behavior.} False positives of this kind are
manifested when tests cover implementation-dependent behavior, as
defined in the ECMA262 specification~\cite{ecmas262-spec}. For
example, one of the tests from \jerry\ uses the function
\CodeIn{Number.toPrecision([precision])}, which translates a number to
a string, considering a given number of significant digits. The
floating-point approximation of the real value is
implementation-dependent, making that test to pass only in
\chakra. \textbf{Timeout/OME\footnote{ome is for out of memory
    error.}.} False positives of this kind typically manifest when the
engine that runs the test does not optimize the code as the original
engine of the test. As result, the test fails to finish at the
specified time budget or it exceeds the memory budget. For example, a
test case from \jsc{} defines a function with a tail-call
recursion. The test fails in all engines but \jsc{}, which implements
tail-call optimization. \textbf{Not implemented.} False positives of
this kind manifest when a test fails because it covers a function that
is part of the official spec, but is not implemented in the target
engine yet. For example, at the time of writing, \chakra{} did not
implement by default various properties from the \mcodeid{Symbol}
object. These properties are only available activating the ES6
experimental mode with the flag \CodeIn{-ES6Experimental}.
\textbf{Non-Standard Element.} These cases manifest when a function or
an object property is undefined in the execution engine but we were
unable to capture that by looking for error types like
\CodeIn{ReferenceError} and \CodeIn{TypeError}.\Comment{ For example, we found
cases where the test (or its harness) fails in an assertion because of
an undefined property.}
\textbf{Other.} This category includes other sources. For example, it
includes the cases where the test was valid for some previous version
of the spec but is no longer valid for the current spec.

\begin{wraptable}[12]{r}{0.23\textwidth}
  \vspace{-3ex}
  \small
  \centering
  \caption{\label{fig:falsepositives}\label{fig:truepositives}\label{fig:piecharts-transplantation}Distribution
    of False (FP) and True Positives (TP).}
  \setlength{\tabcolsep}{3pt}      
  \renewcommand*{\arraystretch}{0.9}
  \begin{tabular}{ccr}
    \toprule
    & source &  \#\\
    \midrule
    \multirow{5}{*}{FP} & Undefined Behavior & 252 \\
    & Timeout/OME & 15 \\
    & Not Implemented & 22 \\
    & Non-Standard Element & 119 \\
    & Other & 135 \\
    \midrule
    \multirow{2}{*}{TP} & Duplicate & 10 \\
    & Bug & 24 \\
    \bottomrule
  \end{tabular}
\end{wraptable}
Table~\ref{fig:piecharts-transplantation} shows the distribution of
False Positives (FPs) and True Positives (TPs). Considering false
positives, ``Undefined Behavior'' was the most predominant
source. Considering true positives, we found a reasonable number of
duplicate reports, but not high enough to justify attempting to
automate the detection of duplicates.



%% ``Url'' shows the URL
%% (de-anonymized upon request) for the bug report entry in the
%% corresponding bug-tracking system, column 

\input{table_transplantation}


Table~\ref{tab:test-transplantation-bugs} lists all bugs we found with
test transplantation. The first column shows the identifier we
assigned to the bug\Comment{, column ``Date'' shows the date the bug
  was reported in the format ``m/dd'',}, column ``Engine'' shows the
affected engine, column ``Status'' shows the status of the bug report
at the time of the writing. The status string appears in bold face for
status ``Confirmed'' or higher, \ie{}, ``Assigned'' and ``Fixed''.
Column ``Severity'' shows the severity of confirmed bugs, and,
finally, column ``Suite'' shows the name of the engine that originated
the test.  (We omitted the URLs of the issues to protect anonymity.)
Considering severity levels, we found that \jsc{}~\cite{jsc-severity}
and \smonkey{}~\cite{mozilla-severity} developers use five levels,
whereas \chakra{}~\cite{chakra-severity} and
\veight{}~\cite{v8-severity} developers use only three. As usual, the
smallest the severity value the highest the severity of the bug. We
use a dash (``-'') in place of the severity level for the cases where
the bug report is pending confirmation. Of the
\noBugsTransplantation{} bugs we reported in this experiment,
\noBugsTransplantationConfirmed\ were promoted from the status ``New''
to ``Confirmed''. Of these, \noBugsTransplantationSeverityTwo{} are severity-2 bugs.  Although we did
not find any critical bugs, most of the bugs are seemingly important
as per the categorization given by engineers. Analyzing the issue
tracker of \chakra, we found that severity-1 bugs are indeed
rare. Considering the number of bug reports confirmed by developers,
\chakra{} was the engine with the highest number--\bugsChakra{} (with \bugsChakraFixed{} bugs
fixed). Considering the remaining engines, \veight{} developers
confirmed the \noTransVeightBugsReported{} bugs we reported, fixing 
\noTransVeightBugsFixed{}. Curiously, Google
engineers confirmed the issued bug reports in a few hours. Overall, we
found that development teams of other engines, specially \jsc\, took
much longer to analyze bug reports as can be observed in the
\jsc\ stacked bar from Figure~\ref{fig:stacked-engine}. 
However, the bugs the \jsc{} team confirmed were quickly fixed.

%% Overall, we found these results promising given that precision (\ie{},
%% the ratio of true positives over all reports) needs to take into
%% account the cost of inspecting the failure~\cite{harman-ohearn-2018};
%% it took a week for the developer to inspect the entire set of
%% failures. The precision for \jsc\ was \Fix{X\%} (\ie, XX/270), which
%% seems a low rate, but it took approximately \Fix{x} days to find
%% \Fix{y} real bugs on this engine. Section~\ref{sec:bugs} shows samples
%% of false and true positives.

\begin{center}
  \fbox{
    \begin{minipage}{8cm}
      \textit{Summary:}~Test transplantation was effective at finding
      functional bugs. Although the cost of classifying failures was
      non-negligible, the approach revealed several non-trivial bugs
      in widely popular engines.
      \end{minipage}
    }
\end{center}


\subsection{Answering RQ3 (Differential Testing)}
\label{sec:cross-engine-diff-testing-results}

This section reports the results obtained with cross-engine differential
testing. More precisely, we report results obtained by fuzzing test
inputs, running those inputs on different engines, and checking the
outcomes with a differential oracle.

\vspace{0.5ex}
\subsubsection{Methodology}
The experimental methodology we used is as follows. As explained in
Section~\ref{sec:objects:fuzzers}, we used Radamsa~\cite{radamsa} and
QuickFuzz~\cite{quickfuzz} for fuzzing. To avoid experimental noise,
we only fuzz test files that pass in all engines--a total of
\totalTestFilesPassInAll\ tests satisfy this restriction. Those tests
appear under the column ``no-fail-in-all'' on
Table~\ref{tab:test-suites}.  We want to avoid the scenario where
fuzzing produces a fault-revealing input based on a test that was
already revealing failures on some engine. This decision facilitates
our inspection task; it helps us establish cause-effect relationship
between fuzzing and the observation of discrepancy.  We configured our
infrastructure (see Figure~\ref{fig:workflow}) to produce 20
well-formed fuzzed files per input file, \ie{}, the number of fuzzing
iterations can exceed the number above as we discard generated files
that are syntactically invalid.

%%  That
%% finding would be important to reduce inspection cost and guide future
%% developments of this approach. 

\vspace{0.5ex}
\lbrack{}Exploratory Phase.\rbrack{} For the first three months of the
study, our inspection process was exploratory.  In this phase, we
wanted to learn whether or not black-box fuzzers could reveal real
bugs and how effective was the \hi{}-\lo\ warning classification.  We
expected the number of warnings to increase dramatically compared to
the previous experiment and, if we realized that the ratio of bugs
from \lo\ warnings was rather low, we could focus our inspection
efforts on \hi\ warnings. To run this experiment, we trained eight
students in analyzing the warnings that our infrastructure
produces. The students were enrolled in a graduate-level testing
class. We listed warnings in a spreadsheet and requested the students
to update an ``owner'' column indicating who was working on it, but we
did not enforce a strict order on the warnings the students should
inspect. Recall from Section~\ref{sec:clusterization} that we
clustered \lo\ warnings in buckets. For that reason, we only listed
one \lo\ warning per representative class/bucket in the
spreadsheet. First, we explained, through examples, the possible
sources of false alarms they could find and then we asked the students
to use the following procedure when finding a suspicious
warning. Analyze the parts of the spec related to the problem and, if
still suspicious, look for potential duplicates on the bug tracker of
the affected engine using related keywords. If none was reported,
indicate in the spreadsheet that that warning is potentially
fault-revealing. We encouraged students to use lithium~\cite{lithium}
to minimize long test cases\Comment{; the tests from the Mozilla suite are
often longer than others}. Only after the co-authors reviewed the
diagnosis, a bug report was filed. Each student found at least one bug
using this methodology.

\vspace{0.5ex}
\lbrack{}Non-Exploratory Phase.\rbrack{} Results obtained in the
exploratory phase confirmed our expectations that most of the bugs
found during the initial period of investigation were related to
\hi\ warnings. For that reason, we changed our inspection
strategy. This time, only the co-authors inspected the bugs using a
similar discipline as before. However, the set of warnings inspected
and the order of inspection changed. We restricted our analysis to
\hi\ warnings and, aware that we would be unable to analyze each and
every warning reported, we grouped those warnings per engine,
analyzing each group in a round-robin fashion.  At each iteration, we
analyzed \warningsIteration{} warnings in each group. A warning
belongs to the group of a given engine if only that engine manifests
distinct behavior, \ie{}, it produces a distinct output compared to
others. We separated in a distinct group the warnings for which two
engines diverge. The rationale for this methodology was to give
attention to each engine more uniformly, enabling more fair comparison
across engines.

\vspace{0.5ex}
\subsubsection{Results} Table~\ref{tab:summary-lo}
shows statistics of \lo\ warnings obtained when fuzzing inputs with
different tools. Column ``\# buckets'' shows the number of buckets
associated with ``\lo'' warnings, column ``\# files'' shows the total
number of files involved in these warnings, and column ``avg. \# files
per bucket'' shows the average number of files per bucket. Note that,
although the number of buckets and files differ substantially across
fuzzers, the difference of files per bucket was not very high.\Mar{not
sure how relevant is this par.}

\begin{table}[t]
  \small
  \centering
  \caption{\label{tab:summary-lo}Summary of \lo\ warning reports.}
  \begin{tabular}{crrr}
    \toprule
    & \# buckets & \# files & avg. \# files per bucket \\
    \midrule
    \radamsa{} & 82 & 319 & 3.8 \\
    \quickfuzz{} & 78 & 218 & 2.7 \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:summary-hi} shows statistics of \hi\ warnings, which
are not clustered. The table breaks down \hi\ warning by the affected
engine, \ie, the engine manifesting distinct output among those
analyzed. Column ``+1'' shows the cases where more than one engine
disagree on the output. Note from the totals that the ordering of
engines is consistent with the one observed on
Table~\ref{tab:cross-testing}, with \chakra\ and \jsc\ in first and
second places, respectively, in number of warnings.


\begin{table}[t]
  \small
  \setlength{\tabcolsep}{4.5pt}
  \centering
  \caption{\label{tab:summary-hi}Number of \hi\ warning
    reports per engine.}
  \begin{tabular}{crrrrr}
    \toprule
    fuzzer\textbackslash{}engine & \jsc\ & \veight\ & \chakra & \smonkey & +1\\
    \midrule
    \radamsa{} & 137 & 47 & 323 & 21 & 483 \\
    \quickfuzz{} & 81 & 62 & 349 & 13 & 369 \\
    \midrule
    \textbf{total} & 218 & 109 & 672 & 34 & 852 \\
    \bottomrule
  \end{tabular}
\end{table}


Table~\ref{tab:false-positives} shows the distribution of false
positives per source. The sources of imprecision are as defined in
Section~\ref{sec:transplantation} with the addition of two new
sources, which we did not observe before. These new sources are
marked with a ``*'' in the table. The source ``Invalid Input''
indicates that the test input violated some part of the
specification. For example, the test indirectly invoked some function
with unexpected arguments; this happens because fuzzing is not
sensitive to function specifications. Consequently, it can replace
valid with invalid inputs. The source ``Error Message Mismatch''
corresponds to the cases where the fuzzer modifies the assertion
expression (\eg{}, some string expression or regular expression).


\begin{table}[t]
  \small
  \centering
  \caption{\label{tab:false-positives}Distribution of False (FP)
    and True Positives (TP).\Mar{why numbers in this table does match with
      that on table ``Number of hi warnings...''?}}
  \begin{tabular}{ccrr}
    \toprule
    & & \radamsa\ & \quickfuzz\ \\
    \midrule
    \multirow{4}{*}{FP} & Undefined Behavior & 26 & 9 \\
    & Timeout/OME & 28 & 13 \\
    & * Invalid Input & 39 & 48 \\    
    & * Error Message Mismatch & 41 & 12 \\
    \midrule
    \multirow{2}{*}{TP} & Duplicate & 34 & 28\\
    & Bug & 12 & 6\\
    \bottomrule     
  \end{tabular}
\end{table}


%% \Igor{devemos remover isso da 
%% etapa de fuzzing upcoming feature falha mesmo sem fuzzing...$\rightarrow$
%% The source ``Upcoming Feature''
%% indicates that the test fails because it uses a feature that is
%% available in the original engine, but it is still not standard, and
%% the target engine does not support that feature}. For example,
%% \Fix{provide concrete example}. \Fix{explain error message mismatch}


%% The table is
%% organized in two sections. The top section shows results obtained
%% during the exploratory phase of the study and the bottom section shows
%% the results obtained during the non-exploratory phase.


Table~\ref{tab:bugs} shows the list of bugs we reported. The table
shows the fuzzing tool used
(``Fuzzer''), the \js\ engine affected (``Engine''), the status of the
bug report (``Status'')\Comment{ as of Aug. 24, 2018, the Url of the bug report
(``Url'')}, the severity of the bug report (``Severity''), the priority
that we assigned to the warning that revealed the bug (``Priority''),
and the test suite from the original test input (``Suite''). So far,
\noDiffConfirmed{} of the bugs we reported were confirmed, \noDiffFixed{} of which
were fixed. Note that one bug report that we submitted was rejected on
the basis that the offending JS file manifested an incompatibility
across engine implementations that was considered to be acceptable. As
of now, we did not find any new bugs on \smonkey{}; the bugs we found
were duplicates and were not reported. Considering \veight{}, 
we reported \noDiffVeight{} bugs, 
all of them were confirmed, with \noDiffVeightFixed{} fixed.

\input{table_differential}

%% Note from the table that all bug reports still waiting for
%% confirmation are associated with the \jsc{} engine. A closer look at
%% the \jsc{} issue tracker showed that their triage process is slow
%% relative to \chakra's.\Comment{\Igor{ The bugs reported on \chakra{}'s
%%     bug tracker was defined as confirmed, however the bugs are
%%     included on a milestone for the next release following an internal
%%     priority.  }} 

%% Finally, it is worth noting that \Fix{11 of the 19}
%% JS files that manifested discrepancies were \emph{not} produced with
%% fuzzing (column ``Fuzz''). These are test files from suites of
%% different engines. This observation emphasizes the importance of
%% continuously collecting test suites from multiple sources; today, we
%% use test suites from seven different open source engines, including a
%% total of 30K test files.

\begin{center}
  \fbox{
    \begin{minipage}{8cm}
      \textit{Summary:}~Cross-engine differential testing was very
      effective at finding bugs in \js\ engines. 
      \end{minipage}
    }
\end{center}


\section{Discussion}
\label{sec:bugs}


\subsection{Example Bug Reports}

This section discusses a selection of bugs reports. The selection
criteria we used was: (i) to cover each engine that we found
bugs--\chakra, \jsc, and \veight, (ii) to cover a different
technique--test transplantation and differential testing (with
\radamsa\ and with \quickfuzz), (iii) to cover a case of rejected bug
report, and (iv) to use short test inputs (for space).

%% \CodeIn{\{var a =
%%   \{valueOf:~function()\{} \CodeIn{return ``\textbackslash{}x00''\}\}
%%   assert(+a === 0)\}}

\sloppy
\vspace{1ex}\noindent\textbf{Issue \#4, Table~\ref{tab:bugs}.} The
code snippet below shows a test that reveals a bug in \chakra.

\begin{figure}[h!]
  \centering
  \scriptsize
  \lstset{escapeinside={@}{@},
%    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c,
    numberstyle=\tiny,
    morekeywords={assertEq, var, yield, in, function, 
    typeof, return, throw, new, Error, if},
  }
  \begin{lstlisting}
 { var a = {valueOf: function(){ return "\x00"}}
   assert(+a === 0) }
  \end{lstlisting}
  \normalsize
  %% \caption{Warning captured as \hi{} priority.}
  %% \label{fig:hi-priority}
\end{figure}

The object property \CodeIn{valueOf} stores a function that returns a
primitive value identifying the target object~\cite{valueof}. The
original version of this code returns an empty string whereas the
version of the code modified by the \radamsa{} fuzzer~\cite{radamsa}
returns a string representation of a null character (\CodeIn{NUL})\Comment{ in
ascii}. The unary plus expression ``\CodeIn{+a}", used in the
assertion, is equivalent to the operation
\CodeIn{ToNumber(a.valueOf())} that converts a string to a number,
otherwise the operation returns NaN (Not a
Number)~\cite{unary-plus}. This test fails in all engines but
\chakra{}. For all three engines the string cannot be parsed as an
hexadecimal. As such, they produce a NaN as result and the test fails
as expected. \chakra{}, in contrast, incorrectly converts the string
to zero, making the test to pass. As Table~\ref{tab:bugs} shows, the
\chakra{} team fixed the issue soon after we reported the problem.

\vspace{1ex}\noindent\textbf{Issue \#18, Table~\ref{tab:bugs}.}  The
code {\footnotesize\ttfamily{eval(function b(a)\{break;\});}} shows
the test input we used to reveal a bug in \veight{}. This code snippet
was obtained by fuzzing a Test262 test with \quickfuzz. In its
original version, a string (omitted for space), passed as argument to
the \CodeIn{eval} function, encoded the actual test. The fuzzer
replaced the string argument with a function whose body is a
\CodeIn{break} statement outside a valid block statement. Section
B.3.3.3 from the spec~\cite{spec-b333} documents how eval should
handle code containing function declarations.  According to the
spec~\cite{break-statement}, the virtual machine should throw an early
error--in this case, a SyntaxError--if the break statement is not
nested in an iterable or a switch statement. All engines, but
\veight{}, behave as expected in this case.

%% \Igor{
%%   Este foi caso de bug confirmado que foi manifestado no engenho \veight{}.
%%   Este test case oriundo da suite Test262 foi fuzzado pelo \quickfuzz{}
%%   que por ser um fuzzer mutacional, usa uma gramatica \js{} que auxilia na geracao
%%   de entradas validas. In Figure~\ref{fig:bug-veight}, we observed a \CodeIn{eval} statement
%%   that try to evaluate a \CodeIn{function f()}, this test case specify the ecma spec
%%   B.3.3.3~\cite{spec-b333} that describe an evaluation of a block statement in eval code containing a 
%%   function declaration. The fuzzer removes the function declaration and 
%%   injects a function with a break statement outside a loop. According to specification~\cite{break-statement},
%%   the break statement should throw an early error (in this case a SyntaxError) 
%%   if the break statement is not nested in an iterational or a switch statement. All engines works
%%   as expected but \veight{} pass without failures.
%% }


\vspace{1ex}\noindent\textbf{Issue \#18,
  Table~\ref{tab:test-transplantation-bugs}.}  The code snippet below
shows the test input we used to reveal a bug in \jsc{}. This test
originates from \jerry{} test suite; the bug was found during the test
transplantation experiment.

\begin{figure}[h!]
  \centering
  \scriptsize
  \lstset{escapeinside={@}{@},
%    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c,
    numberstyle=\tiny,
    morekeywords={assertEq, var, yield, in, function, 
    typeof, return, throw, new, Error, if},
  }
  \begin{lstlisting}
 var obj = {}; var arr = [];
 try { arr.sort(obj); assert(false);}
 catch (e) { assert(e instanceof TypeError); }
  \end{lstlisting}
  \normalsize
  %% \caption{Warning captured as \hi{} priority.}
  %% \label{fig:hi-priority}
\end{figure}

According to the specification~\cite{ecmas262-array-sort}, the
parameter to the \CodeIn{Array.sort} function should be a comparable
object or an undefined value, otherwise it should throw a
\CodeIn{TypeError}. In this case, \jsc{} accepts a non-callable object
as argument to \CodeIn{sort} and the test fails in the subsequent
step. The other engines raise a \CodeIn{TypeError} as expected.

\vspace{1ex}\noindent\textbf{Issue \#2, Table~\ref{tab:bugs}.}  The
code snippet below shows an example input related to a bug report that
we issued to the \chakra\ development team, but they did not accept.

\begin{figure}[h!]
  \centering
  \scriptsize
  \lstset{escapeinside={@}{@},
%    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c,
    numberstyle=\tiny,
    morekeywords={assertEq, var, yield, in, function, 
    typeof, return, throw, new, Error, if},
  }
  \begin{lstlisting}
 function test() {
   return typeof String.prototype.repeat === "function"
     && "foo".repeat(657604378) === "foofoofoo"; }
  \end{lstlisting}
  \normalsize
  %% \caption{Warning captured as \hi{} priority.}
  %% \label{fig:hi-priority}
\end{figure}


This is a testcase original from the \jsc{} suite that the
\radamsa\ fuzzer modified. The original test used the integer literal
3 as argument to \CodeIn{repeat()}, but this test uses a long integer
instead. As result, the engine crashes. The team answered that this
was an incompatibility by design as the function was not expected to
receive such a long value.


\subsection{Threats To Validity}

As it is the case for most empirical evaluations, there are both
external and construct threats to validity associated to the results
we presented. In terms of external validity, our results might not
generalize to other test inputs and engines. To mitigate this threat,
we carefully selected inputs from various sources and cleaned them
according to a well-defined criteria. Considering the \js\ engines, we
used engines associated with the most popular browsers in the
market. In terms of construct validity, there might be errors in the
implementation of our infrastructure. To mitigate this threat, we
extensively inspected the results of our evaluation manually and, when
we found the need for implementing a different component we kept the
old one for cross-checking. For example, we used
eshost-cli~\cite{eshost-cli} to validate our fork-join implementation
of the ``Checker'' component from Figure~\ref{fig:workflow} that, for
efficiency, is tightly integrated to our infrastructure.

\Fix{elaborate}

\subsection{Lessons Learned}
\label{sec:lessons}

We learned several lessons in this study:

\begin{itemize}
  \item The diversity-aware techniques we used are great
    at finding bugs in \js\ engines;
  \item The cost of inspecting failures/warnings is high. Although the
    inspection activity was not uninterrupted, it is safe to say that
    each warning required a substantial amount of time to analyze for
    potential false alarms;
  \item We have found more bugs with test transplantation than we have
    with cross-engine differential testing. In addition, test
    transplantation demanded less effort per bug;
  \item Even simple black-box fuzzers can create surprisingly
    interesting inputs;
  \item Mozilla's \smonkey\ appears to be the most reliable
    \js\ engine we used, with Google's \veight\ after it;
  \item Reporting real bugs is a great way to train (and encourage)
    students in software testing.
\end{itemize}

\Fix{revisor(es) reclamaram de pouca discussao. isto poderia ser usado
  em outros contexto? (resp: JS possui varias engine e muitos
  tests...) ver tambem ``key findings'' na introducao (precisa ficar
  consistente)}

%%%%%%%%%%%%%%%%%%%%%%%
%%% comentario de igor
%%%%%%%%%%%%%%%%%%%%%%%
%% sim, me refiro a testes que possuem algum comentário do que se trata aquele teste. Por exemplo os testes262, possuem comentários descrevendo o que o teste tem que fazer
%% os testes da suite do mozilla são mais dificeis de debuggar... não dificeis de grosso modo, mas custava um pouco para identificar o real problema


\section{Related Work}
\label{sec:related-work}

\subsection{Diversity in Testing}
The idea of test set diversity dates back to the
eighties~\cite{white-cohen-tse1980, ostrand-balcer-1988}. The
assumption behind test set diversity is that faults often spread
contiguously in the input domain. Consequently, it should be
beneficial to partition the input domain and explore it more evenly as
to find bugs faster. Later in the nineties, Chen and
Yu~\cite{chen-yu-tse1996} analyzed numerical software and confirmed
the continuity assumption. They found that faults, most open, manifest
themselves in regular patterns across the input
domain~\cite{Chen:2010:ART:1663656.1663914,7515474}.  Such
observations led researchers to investigate generalizations of the
approach to non-numerical data types and strategies to explore the
input (or output) space to solve different problems in Software
Engineering (\eg{}, test input generation and test
selection)~\cite{mayer-ase2005,bueno-etal-ase2007,ciupa-etal-icse08,alshahwan-harman-icse2012,alshahwan-harman-issta2014,7515474}. This
paper explores diversity of sources of test cases and diversity of
engines to test and find bugs. We remain to investigate the diversity
of test cases themselves. In principle, that could help reduce the
number of alarms to inspect, for example.

\subsection{Differential Testing}
Several different applications of differential testing have been
proposed in recent years. Chen and
colleagues~\cite{Chen:2018:RDT:3180155.3180226} recently proposed a
technique to generate X.509 certificates based on Request For
Proposals (RFC) as specification with the goal of detecting bugs in
different SSL/TLS implementations. Those bugs can comprimise security
of servers which rely on these certificates to properly authenticate
the parties involved in a communication session. Lidbury and
colleagues~\cite{Lidbury:2015:MCF:2737924.2737986} and Donaldson and
colleagues~\cite{Donaldson:2017:ATG:3152284.3133917} have been
focusing on finding bugs in programs for graphic cards (\eg{},
OpenCL). These programs use the Single-Instruction Multiple-Data
(SIMD) programming abstraction and typically run on GPUs.  Perhaps the
application of differential testing that received most attention to
date was compiler testing. In 1972, Purdom~\cite{Purdom1972} proposed
the use of a generator of sentences from grammars to test correctness
of automatically generated parsers. After that, significant progress
has been made. Lammel and Shulte proposed Geno to cross-check XPath
implementations using grammar-based testing with controllable
combinatorial coverage~\cite{10.1007/11754008_2}. Yang and
colleagues~\cite{Yang:2011:FUB:1993498.1993532} proposed CSmith to
randomly create C programs from a grammar, for a subset of C, and then
check the output of these programs in different compilers (\eg{}, GCC
and LLVM). Le and colleagues~\cite{Le:2014:CVV:2594291.2594334}
proposed ``equivalence modulo inputs'', which creates variants of
program which should have equivalent behavior compared to the
original, but for which the compiler manifests
discrepancy. Differential testing has also been applied to test
refactoring engines~\cite{Daniel:2007:ATR:1287624.1287651}, to test
symbolic engine implementations~\cite{Kapus:2017:ATS:3155562.3155636},
and to test disassemblers and binary
lifters~\cite{Paleari:2010:NDD:1831708.1831741,Kim:2017:TIR:3155562.3155609}. All
in all, it has shown to be flexible and effective for a wide range of
applications. Surprisingly, not much work has been done on
differential testing of \js\ engines. Mozilla uses differential
testing to look for discrepanices across different configurations of
the same version of its \smonkey\ engine (using the ``compare\_jit''
flag of jsfunfuzz~\cite{jsfunfuzz}) whereas we focus on discrepancy
across engines. Patra and Pradel evaluated their language-agnostic
fuzzing strategy using differential testing. Their focuses on finding
differential bugs across multiple
browsers~\cite{patra2016learning}. As such they specialized their
fuzzer to HTML and JS (see Section~\ref{sec:testing-js-engines}). In
contrast to Patra and Pradel, we did not propose new techniques; our
contribution was empirical.

\subsection{Testing \js\ Programs}
Patra and colleagues~\cite{Patra:2018:CFU:3180155.3180184} proposed a
lightweight approach to detect conflicts in \js\ libraries that occur
when names introduced by different libraries collide. This problem was
found to be common as the design of \js\ allows for overlaps in
namespaces. A similar problem has been investigated by Nguyen and
colleagues~\cite{nguyen-etal-icse2014} and Eshkevari and
colleagues~\cite{eshkevari-etal-icpc2014} in the context of PHP
programs, which are popular in the context of Content Management
Systems as WordPress. The focus of this paper is on testing
\js\ engines as opposed to \js\ programs. Our goal is therefore
orthogonal to theirs.

\subsection{Testing \js{} Engines}
\label{sec:testing-js-engines}
The closest work to ours was done by Patra and
Pradel~\cite{patra2016learning}. Their work proposes a
language-agnostic fuzzer to find cross-browser HTML+JS
discrepancies.\Comment{ They applied the proposed fuzzer to find bugs in JS engines and
Web Assembly engines.} The sensible parts of the infrastructure they
built are the checks of input validity (as to reduce waste/cost) and
output correctness (as to reduce false positives). Patra and Pradel
work is complementary to ours--in principle, we could use their fuzzer
in our evaluation. The main difference of our work to theirs is in
goal--we aim at assessing reliability of \js\ engines and find bugs on
them using simple approaches whereas they aim at proposing a new
technique.

Fuzzing is an active area of investigation with development of new
techniques both in academia and industry. Several fuzzing tools exist
focused on \js. Section~\ref{sec:objects:fuzzers} briefly explain
different fuzzing strategies and tools. Existing techniques prioritize
automation with a focus on finding crashes; see the sanitizers used in
libFuzzer~\cite{libfuzzer-tutorial}, for instance. In general, it is
important for these tools that a warning reveals something potentially
alarming as a crash given that fuzzing is a time-consuming operation,
\ie{}, the ratio of bugs found per inputs generated is often very low.
Our approach contrasts with that aim as we focus on finding errors
manifested on the output, which rarely result in crashes and,
consequently, would go undetected by current fuzzing approaches. It is
should be noted, however, that such problems are not unimportant as
per the severity levels reported in
Tables~\ref{tab:test-transplantation-bugs} and~\ref{tab:bugs}.

\section{Conclusions}

JavaScript is a very popular language. For that reason, bugs in engine
implementations often affect lots of people and organizations.
Implementing correct engines is challenging because the specification
is intentionally incomplete and evolves frequently. Finding bugs is
challenging for similar reasons. We conducted a study to evaluate two
simple diversity-aware techniques--test transplantation and
cross-engine differential testing. The first technique runs test
suites of a given engine in another engine. The second technique
fuzzes existing inputs and then compares the output produced by
different engines with a differential oracle.

Overall, we reported \totalBugsReported\ bugs in this study. Of which,
\totalBugsConfirmed\ were confirmed by developers and
\totalBugsFixed\ were fixed. Although more work is necessary to
reduce cost of manual analysis, we found that both techniques we used
are effective at finding bugs in
\js\ engines. Section~\ref{sec:lessons} elaborates on the lessons
learned from this study. The result data, infrastructure, and
experimental scripts we prepared to run this study are available upon
request. Future work includes mining test inputs from alternative
sources (\eg{}, bug trackers) and predicting likelihood of false
positives as to guide human effort on the most promising warnings
first.

\balance
\bibliographystyle{ACM-Reference-Format}
\bibliography{references,../docs/google-research-awards-latam/tmp}

\end{document}

%%  LocalWords:  formedness Embeddings NLP Convolutional dataset
