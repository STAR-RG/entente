%\documentclass[conference]{IEEEtran}
\documentclass[10pt,conference,anonymous]{IEEEtran}
\IEEEoverridecommandlockouts

%% Marcelo added this
\makeatletter
\renewcommand\footnoterule{%
  \kern-3\p@
  \hrule\@width.4\columnwidth
  \kern2.6\p@}
  \makeatother

\input{packages}
\input{macros}

\begin{document}

%Should I Fuzz my Inputs or Improve my Tests? 
\title{Leveraging Diversity to Find Bugs\\ in JavaScript Engines}

%% \author{
%% \IEEEauthorblockN{Sabrina Souto}
%% \IEEEauthorblockA{State University of Para\'iba\\
%% Para\'iba, Brazil\\
%% sabrinadfs@gmail.com}
%% \and
%% \IEEEauthorblockN{Marcelo d'Amorim}
%% \IEEEauthorblockA{Federal University of Pernambuco\\
%%   Pernambuco, Brazil\\
%%   damorim@cin.ufpe.br}
%% \and
%% \IEEEauthorblockN{Rohit Gheyi}
%% \IEEEauthorblockA{Federal University of Campina Grande\\
%%   Para\'iba, Brazil\\
%%   rohit@dsc.ufcg.edu.br}
%% }

\maketitle

%% page numbering -M
\thispagestyle{plain}
\pagestyle{plain}

%% JavaScript (\js{}) is a popular programming language for the
%% web. Finding errors in JS runtime engines is an important problem.
\begin{abstract}
  \input{abstract}
\end{abstract}

\begin{IEEEkeywords}
JavaScript; Diversity; Software Testing
\end{IEEEkeywords}

\section{Introduction}

JavaScript (\js{}) is today one of the most popular programming
languages around~\cite{business-insider,stackify}, with penetration in
various segments including, web development, mobile, and, more
recently, the Internet of Things~(IoT)~\cite{simply-technologies}. The
interest of the community for the language encourages constant
improvements in its specification and implementations. It is natural
to expect that such improvements lead to sensible changes in engine
implementation. Even small changes can have high practical impact. For
example, in October 2014 a new attribute added to Array objects
resulted in the MS Outlook Calendar web app stop working under
Chrome~\cite{array-bug-chromium-issue4247,array-bug-discussion}.
Intuitively, developing reliable engines is difficult because
specifications are incomplete and evolve frequently.


Finding bugs in \js\ engines is an important problem given the range
of applications that could be affected with those
bugs. It\Comment{Finding bugs in \js\ engines} is also
challenging. For example, a test that passes in one engine could
legitimately fail in a different engine because of some undefined
behavior in the spec\footnote{These cases of undefined behavior can be
  localized by searching for the string ``implementation-dependent''
  in the ES6 spec document~\cite{ecmas262-spec}.}. As another example,
a test may fail because the engine does not fully implement some
feature. The publicly-available Kangax compatibility
table~\cite{kangax} relates \js\ features with the engines that
support them. To sum, it is often non-trivial to assess if a test
fails because of a bug or because of some other reason making test
automation more challenging.

%% when a bug is found in some engine a test case is created and added to
%% the regression suite. Those test inputs often check interesting
%% scenarios that could reveal bugs in different engine
%% implementations. 

%% WHAT WE DID
This paper reports on a study we ran to evaluate the importance of
diversity in finding bugs on JavaScript engines. We evaluated
effectiveness of two complementary diversity-aware testing
techniques. \emph{Test transplantation} leverages diversity of test
cases to find bugs. It evaluates the effects that running \js{} test
files from a given engine have in detecting bugs in other engines. The
intuition is that developers design test cases differently; hence
replaying the test in a different engine may reveal unanticipated
problems. \emph{Cross-engine differential testing} evaluates the
ability of leveraging diversity of engine implementations to find
bugs. More precisely, it fuzzes existing test
inputs~\cite{fuzz-testing-history} and then compares the output
produced by different engines using a differential oracle. One of
various causes of output discrepancy is a bug.

%% It automates test generation in scenarios where multiple
%% implementations of a system exist. DT leverages diversity across
%% system's implementations to detect anomalous behavior. 

\emph{Related Ideas.}  It is worth noting that Differential
Testing~\cite{Brumley-etal-ss07} (DT) has been applied in a variety of
contexts to find
bugs~\cite{Yang-etal-pldi11,Chen-etal-fse2015,Argyros-etla-ccs16,Chen-etal-pldi16,petsios-etal-sp2017,SivakornAPKJ17,Zhang:2017:ATD:3097368.3097448}.
It has shown to be specially practical in scenarios where the
observation of difference gives a strong signal of a real problem. For
example, Mozilla runs JS files\footnote{These files are created with
  the grammar-based fuzzer jsfunfuzz~\cite{jsfunfuzz}. Look for option
  ``compare\_jit'' from funfuzz.} against different configurations of
a given build of their \smonkey\ engine (\eg{}, trying to enable or
not eager JIT compilation). In that setting, test outcomes are
expected to be identical. Mozilla uses this approach since 2002 and
have been able to find over 270 bugs since
then~\cite{jsfunfuzz-at-mozilla}, including security
bugs. Cross-engine differential testing, in contrast, has not been
popularized to similar extent. One possible reason is that it is still
not practical to fully automate the technique. In contrast to other
applications of differential testing, a number of legitimate reasons
exist, other than a bug, for a test execution to manifest discrepancy
(see Table~\ref{tab:false-positives}). Recently, Patra and
Pradel~\cite{patra2016learning} explored the idea. They proposed a
language-agnostic fuzzer to find cross-browser HTML+JS
discrepancies. Their main contribution is analytical whereas our main
contribution is empirical--goal is to evaluate ability and cost of
simple diversity-aware approaches to find bugs in \js\ engines.

\emph{Results.}~\Fix{REVISE}We considered engines from four major players--Apple,
Google, Microsoft, and Mozilla. Our results indicate that both
techniques revealed several bugs, most of which confirmed by
developers. Test transplantation revealed \noBugsTransplantation{}
bugs (\noBugsTransplantationConfirmed{} confirmed) and differential
testing revealed \noBugsDifferentialTesting{} bugs
(\noBugsDifferentialTestingConfirmed{}). Furthermore, results indicate
that most of these bugs affected only two engines--Apple's \jsc{}
(\percJSC{}) and Microsoft's \chakra{} (\percChakra{}); we found only
one bug in Google \veight{} and none in Mozilla's \smonkey{}. Although
our experience indicates that more research needs be done to automate
parts of the inspection process, our results show that exploring
diversity is a valuable help to find bugs in JavaScript engines.


%% LIST OF CONTRIBUTIONS
This paper makes the following contributions.
\begin{itemize}
  \item We ran a comprehensive study to analyze the effectiveness of
    simple diversity-aware techniques to find bugs in highly-popular
    \js\ engines. 
    
  \item We reported more than \Fix{50} bugs on these engines over a
    period of 5 months.

  \item We offered insights on how to reduce cost of analyzing
    warnings reported by diversity-aware techniques and make the
    approach more practical.
    
  \item We made our testing infrastructure, results, and experimental
    scripts available to the public. (upon request for anonimity.)
\end{itemize}



\section{JavaScript}
\label{sec:es6-design}
\label{sec:imp-dep-behavior}

JavaScript engines are virtual machines that parse source code,
compile it in bytecodes, and run these bytecodes. These engines
implement some version of the ECMAScript (\es{}), which emerged with
the goal to standardize variants of the language, such as Netscape's
JavaScript and Microsoft's JScript\footnote{The name JavaScript still
  prevails today, certainly for historical reasons.}. The \es{}
specification is regulated by Ecma International~\cite{es6-website}
under the TC39~\cite{tc39-github} technical committee.  Every year, a
new version of the \es{} specification is produced with new features
and minor fixes. The canonical spec today is
ES6~\cite{ecmas262-spec-repo,ecmas262-spec}.

%% In the following, we briefly describe different sources of
%% discrepancies that can emerge across different engine implementations
%% as results of the active changes in the \es{} specification.
%% .........
%% A compatibility table relating features and supporting engines is
%% publicly available~\cite{kangax}.

Both the specification and implementation of JavaScript are
incomplete. Certain parts of the specification are undefined; it is
responsibility of engine engineers to decide how to implement these
functionalities. The JavaScript spec uses the label
``implementation-dependent'' to indicate these cases, where behavior
may differ from engine to engine. One reason this flexibility in the
spec exists is to enable compiler optimizations. For example, the
\mcodeid{for-in} loop does not clearly specify the iteration
order~\cite{so-forin-undefined,javascript-in-chrome} and different
engines capitalize on that~\cite{for-in-undefined}.  As another
example, the latest version of the \es{} specification states that if
the \mcodeid{Number.toPrecision()} function is called with more than
one argument, then the behavior is
implementation-dependent~\cite{es6-toPrecision}. Various other cases
like these exist in the specification. Such gap also exists at the
implementation level. Given the speed the specification changes and
the complexity of the language some features are not fully implemented
as can be observed by the Kangax compatibility table~\cite{kangax}.

\sloppy It is also worth noting that, as in other languages, some
elements in JS have non-deterministic behavior (\eg{},
\mcodeid{Math.random} and \mcodeid{Date}). A test that make decisions
based on these elements could, in principle, produce different
outcomes on different runs. Carefully-written test cases should not
manifest this kind of
flakiness~\cite{luo-etal-fse2014,palomba-zaidman-icsme2017}, but
fuzzed test files could are more likely, intuitively, to manifest
them.

As mentioned previously, all those aspects make testing \js\ engines
more challenging.


%\subsection{Randomness}


%\subsection{\Fix{Other}}

%% 1) The stack depth in JS is often close to the real (native) C stack
%% depth, nevertheless observable by JS through recursion. This causes
%% differential behavior even on the same engine when JITs are enabled or
%% disabled, as the stack frame sizes vary. LangFuzz in particular
%% produces a lot of recursions and hits these problems even more often
%% than jsfunfuzz.\Mar{ask C. Holler. it is unclear to me how the output
%%   could be different.}

\section{Objects of Study}
\label{sec:methodology}

This section discusses the objects we used in our study.

\subsection{Engines}
\label{sec:methodology:engines}~We selected 
JS engines according to the following criteria.

\begin{itemize}
\item Released latest version after Jan 1, 2018
\item Contains more than 1K stars on GitHub  
\item Uses a public issue tracker
\end{itemize}  

We look for highly-maintained (as per the first criterion) and popular
(as per the second criterion) projects. As we wanted to report bugs,
we also looked for project with public issue
trackers. Table~\ref{tab:engines} lists the engines we analyzed in
this work. It is worth noting that we used GoogleChromeLab's JSVU
tool~\cite{jsvu} to automatically install and configure versions of
different JS engines in our host environment. This is important as we
aim to use the most recent stable versions of each engine as to avoid
reporting old and already-fixed bugs to developers.

%% Table~\ref{tab:engines} shows the engines we selected in this
%% study. We made an exception to the second criterion with
%% XS~\cite{xs2018repo}. As the project is young, created in Oct 2017, we
%% thought there was insufficient time to obtain 1K stars for XS. We
%% still considered this project as it seems to be attracting interest
%% from the community\Fix{is this true?}

\begin{table}[t]
  \centering
  \caption{\label{tab:engines}Engines analyzed.}
  \begin{tabular}{cccr}
    \toprule
    Team & Name & URL & \# Stars \\
    \midrule
    Apple & \jsc{} (WebKit) & \cite{jsc2018repo} & \multicolumn{1}{c}{3300+} \\
    Google & \veight{} & \cite{v82018repo} & 9800+ \\
%    \jerry{} & \jerry{} & \cite{jerryscript2018repo} & 3100+ \\
    Microsoft & \chakra{} & \cite{chakra2018repo} & 7200+ \\
%    Moddable & XS & \cite{xs2018repo} & 140+ \\
    %% \midrule
    %% \multirow{2}{*}{Mozilla} & Rhino & \cite{rhino2018repo} & 1800+ \\
    Mozilla & \smonkey{} & \cite{spidermonkey2018repo} & \multicolumn{1}{c}{1100+} \\
   \bottomrule     
  \end{tabular}
\end{table}

\subsection{Seed JS files\label{sec:seeds}}~
We selected test files from several public projects from GitHub. We
looked for self-contained tests from JS engine projects, initially
using the GitHub REST API~\cite{github-rest-api}. We noticed that some
of the tests we found depend on external libraries, which not all
engines we use support. We decided to discard those. For example, many
tests we found required a Node.js runtime~\cite{node} for
execution. The main source of test cases is the official
Test262~\cite{tc39-github} conformance test suite for the ECMA262
specification~\cite{ecmas262-spec}. This test suite contains 87\% of
all the tests we used. We also considered test suites of three of the
four engines mentioned in Section~\ref{sec:methodology:engines} and
four other engines. Table~\ref{tab:test-suites} shows the breakdown of
tests we considered. Overall, we found a total of \totfiles{} JS
files. Column ``original'' shows the test cases for each
test suite. We did not consider tests from the \chakra{} repository because they depend
on non-portable objects and notice that the number of tests in
\veight\ is low. That happens because many tests in \veight{} are
inherited from Mozilla and \jsc{}; we discarded those to avoid
repetition and to give credit where it is due. Column
``compile-in-all'' shows the number of test cases for which all four
engines we analyzed do not throw compile-time error. We discarded
tests that manifest compile errors as they clearly indicate missing
features; these tests use non-portable names (\eg{}, \jsc{}'s
\CodeIn{drainMicrotasks()} and \smonkey{}'s \CodeIn{Error.lineNumber})
or use functions that, although being part of the spec, not all
engines currently support. Finally, column ``no-fail-in-all'' shows
the tests for which all engines
pass. Section~\ref{sec:transplantation} discusses these failures, some
of which are indicative of bugs. The tests under this column will be
used to create new, potentially fault-revealing, tests. For that, it
is important to assure that all of them pass.

It is also worth mentioning that some engines use a custom shell to
run tests, including a harness with specific assertions.  For these,
we needed to make small changes in the testing infrastructure to be
able to run the tests uniformly across all engines. More precisely, we
needed to mock non-portable harness functions, which are only
available to certain engines.

 %% The table shows the
 %%    number of test cases in each test suite considered: TC-39
 %%    conformance test suite, suites from engines we analyzed, and
 %%    suites from other engines.

\begin{table}[t]
  \centering
  \caption{\label{tab:test-suites}Number of test cases per
    suite. Column ``original'' shows the original size of the test
    suite, column ``compile-in-all'' shows the number of test cases
    for which engines do not throw compile error (\eg{}, undefined
    name), and column ``no-fail-all'' shows the number of test cases
    that pass in all four engines we analyzed.}
  \begin{tabular}{ccrrr}
    \toprule
    \multirow{2}{*}{Name}      &  \multirow{2}{*}{Source} &
    \multicolumn{3}{c}{\# JS files} \\
    \cline{3-5}
                               &         & original & compile-in-all &  no-fail-in-all \\
    \midrule
    TC-39 (Test262) & \cite{ecma262-conformance-suite} & 31,276 &
    29,846 & 18,943 \\
    \midrule
    \jsc{} & \cite{webkit} & 1,031 & 1,021 & 955\\
    \smonkey\ & \cite{mozilla} & 2,634 & 2,116 & 1,686\\
    \veight{} & \cite{v8} & 75 & 57 & 57\\
    \midrule
    Duktape & \cite{duktape} & 1,195 & 921 & 915\\ 
    \jerry{} & \cite{jerryscript} & 1,951 & 1,878 & 1,837\\
    JSI & \cite{jsi} & 99 & 63 & 63\\
    Tiny-js & \cite{tinyjs} & 49 & 37 & 37\\
    % \chakra{} & \cite{chakracore} & 2632 \\
    \midrule
     &  & \totalTestFiles{} & \totalTestFilesCompileInAll{} & \totalTestFilesPassInAll{}\\
   \bottomrule 
  \end{tabular}
\end{table}

% For example to run the \jerry{} tests it was necessary 
% use the unit-test package to run it, but with our changes we added the assertion
% does not have an assertion in the test file
% \Fix{add code to explain}

\subsection{Fuzzers}
\label{sec:objects:fuzzers}

%% In the following, we describe the list of fuzzers we analyzed. We
%% initially considered generational grammar-based fuzzers and mutational
%% fuzzers.

Fuzzers are typically categorized in two main groups--those that build
inputs anew (generational) and those that modify existing inputs
(mutational). We used two black-box mutational
fuzzers\Comment{Radamsa~\cite{radamsa} and QuickFuzz~\cite{quickfuzz}}
in this study. In the following, we provide rationale for this
selection.

Generational fuzzers are typically grammar-based. These fuzzers
generate a new file using the grammar of the language whose inputs
should be fuzzed. Intuitively, those fuzzers implement a traversal of
the production rules of the input grammar to create syntax trees,
which are then pretty-printed. Consequently, this approach produces
inputs that are syntactically valid by construction. We analyzed four
grammar-based fuzzers--Grammarinator~\cite{grammarinator},
jsfunfuzz~\cite{jsfunfuzz},
LangFuzz~\cite{Holler:2012:FCF:2362793.2362831}, and
Megadeth~\cite{grieco2016quickfuzz}.  Unfortunately, none of those
were effective out of the box. For example, we produced 100K inputs
with Grammarinator and only \Fix{X} of those were valid. With
Megadeth, we were able to produce \Fix{Y, Y$>$Y?}  valid inputs as it
contains some heuristics to circumvent violations of certain type
rules such as \Fix{variable used must be defined?}. Nonetheless,
running those inputs in our infrastructure we were unable to find
discrepancies. Inspecting those inputs, we realized that they reflected
very simple scenarios. To sum, a high percentage of inputs that
Grammarinator and Megadeth generated were semantically-invalid that we
needed to discard whereas the valid inputs manifested no
discrepancies. Considering jsfunfuzz~\cite{jsfunfuzz}, we noticed
that, in addition to the issues mentioned above, it produces inputs
that use functions that are only available in the \smonkey{}
engine. We would need either to mock those functions in other engines
or to discard those tests. Considering
LangFuzz~\cite{Holler:2012:FCF:2362793.2362831}, the tool is not
publicly available. One fundamental issue associated with generational
fuzzers in our context is that the tests they produce do not contain
assertions; to enable the integration of this kind of fuzzers in our
infrastructure, we would need to look for discrepancies across
compiler messages.  All in all, although grammar-based fuzzers have
been shown effective to find real
bugs~\cite{Holler:2012:FCF:2362793.2362831}, generating valid inputs,
albeit promising conceptually, still requires some engineering
effort. For this reason, we did not consider those fuzzers in this
study.

%% Our
%% infrastructure supports any grammar fuzzer with a few
%% adjusts. However, we try to integrate several grammar-based fuzzers,
%% for example
%% and \Fix{others fuzzers} to
%% generate new JavaScript files based on grammar, but after several runs
%% it was observed that this approach was ineffective due the amount of
%% invalid files and/or files without discrepancies.
%% For example, if we
%% ran Grammarinator to generate 1K JS files ten times with a random seed
%% generation, we obtained \Fix{XX\%} of valid files. Checking in our
%% environment almost \Fix{XX\%} are js files that shows undefined
%% variables and due the differential testing in our environment all
%% engines will raise a SyntaxError and this approach was not relevant to
%% our experiment.

%% We initially considered used representatives of popular fuzzing approaches. For
%% random-based fuzzing we used Radamsa~\cite{radamsa}; for
%% coverage-based fuzzing we used
%% \Fix{AFL~\cite{afl}/libfuzzer~\cite{libfuzzer}?}, and for
%% generative-based fuzzing we used
%% \Fix{grammarinator,jsfunfuzz?}. Details on how these fuzzers work can
%% be found elsewhere~\cite{fuzz-bart}.

Mutational fuzzers can be either white-box or black-box. White-box
mutational fuzzers are typically coverage-based. American Fuzz Loop
(AFL)~\cite{afl} and libFuzzer~\cite{libfuzzer} are examples of this
kind of fuzzers. These fuzzers run tests inputs against instrumented
versions of the program under testing with the typical goal of finding
universal errors like crashes and buffer overflows. The instrumentation
adds code to collect branch coverage and to monitor specific
properties\footnote{There are options in the clang toolchain to build
  programs with fuzzing instrumentation~\cite{libfuzzer}. clang
  provides several sanitizers for property
  checking~\cite{clang-documentation}.}. AFL use coverage to determine
inputs that uncover a new branch and hence should be fuzzed more
whereas libFuzzer uses evolutionary generation--it tries to minimize
the distances to still-uncovered branches of the program. AFL takes
the instrumented program binary (say, a JS engine) and one seed input
to that program (say, a JS program) and produces on output
fault-revealing inputs, if found. Considering our context of
application, we needed to instrument one runtime engine for fuzzing;
we chose v8. Unfortunately, we found that most of the inputs produced
by AFL violate the JS grammar. Furthermore, the fuzzing task can take
days for a single seed input and there is no simple way to guide the
exploration\footnote{Exchanged emails with the tool author.}. That
happens because the fuzzer aims to explore the entire decision tree
induced from the engine's main function, including the branches
associated with the higher layers of the compiler (\eg{}, lexer and
parser). It is worth mentioning that Google mitigates that problem
with libFuzzer by asking developers to create fuzz targets for
specific program
functions\cite{libFuzzer-tutorial-google,libFuzzer-chromium-google}. Although
that approach has shown very effective, it requires domain knowledge
to create the calling context to invoke the fuzz target. For those
reasons, coverage-based fuzzers were not considered in this study.

We used two black-box mutational fuzzers in this
study--\radamsa~\cite{radamsa} and \quickfuzz~\cite{quickfuzz}. These
fuzzers require no instrumentation and domain-knowledge. They mutate
\emph{existing} inputs randomly. The strength of the approach is
limited by the quality of the test suite and the supported mutation
operators, which are typically simple. We chose \radamsa\ and
\quickfuzz\ \Fix{...elaborate...} \Mar{I mention below that these
  fuzzers ``only make valid changes in literals of the
  language''. Please confirm and explain that still this is
  interesting!}

\section{Differential Testing Infrastructure}
\label{sec:design}

%\begin{wrapfigure}[10]{r}[0pt]{0.45\textwidth}
\begin{figure}[t!]
  \centering
%  \includegraphics[trim=20 350 200
%    0,clip,width=0.35\textwidth]{google-awards-workflow}
  \includegraphics[trim=0 350 0 0,clip,width=0.65\textwidth]{diff-testing-runtimes}
  \caption{\label{fig:workflow}Differential Testing infrastructure overview.}
\end{figure}

Figure~\ref{fig:workflow} illustrates the infrastructure for
differential testing, which is used in part of this study. It takes on
input a list of JS files and generates warnings on output. Boxes in
the figure denote encapsulation, arrowed lines denote data flow, and
the cycle icons denote repetition--the leftmost icon indicates that
each file in the input list will be analyzed in separate whereas the
rightmost icon shows that a single file will be fuzzed multiple times.
The bug-finding process works as follows. New inputs are obtained for
a given test input using some off-the-shelf input
fuzzer~\cite{fuzz-testing-history}.\Comment{ Several fuzzing methods
  have been proposed in the past, varying with respect to how new
  inputs are
  generated~\cite{afl,honggfuzz,grammarinator,jsfunfuzz,radamsa}.}
Section~\ref{sec:objects:fuzzers} describes the fuzzers we selected.
The oracle checks whether or not the output produced for the fuzzed
file is consistent across all engines. In case the test passes in all
engines or fails in all engines (\ie{}, the output is consistent), the
infrastructure discards the test. Otherwise, it considers the input as
potentially fault-revealing; hence interesting for human
inspection. The infrastructure uses the
open-source tool eshost-cli~\cite{eshost-cli}, also used at
Microsoft, for checking output discrepancy. 

%The checker can produce multiple warnings for a given input. 

Note that a number of reasons exist, other than a bug, to explain
discrepancy (see Table~\ref{tab:false-positives}) and there is no
clear way to distinguish false and true positives automatically. As
such, a human needs to inspect the warning to classify the issue. To
facilitate inspection, we prioritized warnings and clustered them in
groups. We describe these strategies in the following.

\subsection{Prioritization}

We classified warnings in two types,
reflecting their likelihood of manifesting a real bug. Warnings of the
kind ``\hi{}'' are associated with the cases where the test code
executed to completion without violating any internal checks, but it
violated assertions declared in the test itself or its harness. The
rationale is that the input is more likely to be valid as it it did
not raise exceptions during the execution of the application code,
but, it manifested discrepancy through the violation of an assertion
declared in the test. In contrast, warnings of type ``\lo{}'' are
more likely to be asssociated with invalid inputs. They reflect the
cases where the anomaly is observed during the execution of functions,
which are part of the application. We found that different engines
often check pre-conditions of those functions differently. It can
happen, for example, that one engine enforces a weaker pre-condition
on a function input compared to another engine and that is
acceptable.\Comment{ For instance, it is acceptable to pass values greater than
\Fix{x} to function \CodeIn{\Fix{WWW}} in \Fix{y} but not in
\Fix{z}.} In those cases, the infrastructure would observe a
discrepancy that is more likely to be associated with an invalid input
produced by the fuzzer, \ie{}, it is likely to be a bug in the test
code not the engine (library or runtime) code. However, as expected, our heuristic
is not precise---``\lo'' warnings can 
reveal bugs. Figure~\ref{fig:lo-truepositive} shows one of these
cases. In this example, the test instantiates an \CodeIn{ArrayBuffer}
object and stores an 8-bit integer at the 0 position. According to the
specification~\cite{ecmas262-getviewvalue}, a \CodeIn{RangeError}
exception should be thrown if a negative value is passed to the
function \CodeIn{ToIndex}, indirectly called by the test case. In this
case, however, the \chakra{} engine did not throw any exception, which
violates the spec. This case of undocumented precondition, reported
and handled by developers as a bug, was fixed by developers after our bug
report and is no longer present in the most recent release of
\chakra. Despite that case, most bugs we found emerged from
``\hi{}'' warnings.

%% \Igor{
%%   Apesar dos bugs encontrados serem em sua maioria classificados como HI group,
%%   casos reportados como LO tambem podem ser considerados como um true positive.
%%   Nossa heuristica de classificacao eh baseada no error stream dos engenhos,
%%   se um teste falha antes da linha em que ha uma assercao, entao a falha
%%   ocorreu em uma funcao nativa do JS. Na figura \ref{fig:lo-truepositive}, \'e 
%%   possivel observar um caso de bug obtido em um bucket classificado como LO.
%%   Neste exemplo, o teste instancia um ArrayBuffer e armazena um 8-bit integer na 
%%   posicao 0 do dataframe, quando tentamos obter um elemento em uma posicao
%%   maior e/ou menor do que a suportada, neste caso \CodeIn{getInt8}, 
%%   de acordo com a especificacao\cite{ecmas262-getviewvalue}, deve-se 
%%   lancar Offset is outside the bounds.

%%   Os casos reportados no grupo LO nao foram capturados atraves das assercoes.
%%   Por exemplo, se o fuzzer gera uma entrada que viola uma especificacao, o 
%%   correto seria usar um levantar um erro
%% }

\subsection{Clusterization}
\label{sec:clusterization}

Clusterization helps to group tests that fail for similar reasons. We
only clustered ``\lo'' warnings as ``\hi'' warnings produce messages
that arise from the test case, which are typically
distinct. Figure~\ref{fig:lo-truepositive} shows, at the bottom, the
error messages produced by a ``\lo'' warning. Any warning, including
this one, that has this same message signature will be included in the
same cluster, which we may also refer as a ``bucket''.

\begin{figure}[!h]
  \centering
  \begin{lstlisting}
var buffer = new ArrayBuffer(64);
var view = new DataView(buffer);
view.setInt8(0,0x80);
assert(view.getInt8(-1770523502845470856) === -0x80);

Engines Messages (1:V8, 2:JavaScriptCore, 3:SpiderMonkey):
1. RangeError: Offset is outside the bounds of the DataView
2. RangeError: byteOffset cannot be negative
3. RangeError: invalid or out-of-range index
  \end{lstlisting}
  \caption{\label{fig:lo-truepositive}True positive example of a ``\lo''
    warning.}
\end{figure}

\section{Results}
\label{sec:results}

The goal of our study is to assess reliability of \js\ engines by
leveraging implementation diversity, which is specially rich in this
domain. We report results in three parts. First, we report on the
stability of the engines we analyzed using the Test262 standard
conformance test suite for EcmaScript~\cite{ecma262-conformance-suite}
(Section~\ref{sec:stability}). The intuition is that the bugs we
discover throughout this study would have low relevance if the engines
are too fragile. Second, we report results obtained with test
transplantation (Section~\ref{sec:transplantation}). The rationale is
that the domain of possible inputs is too large; consequently, we
expect that test suites written for a given engine cover scenarios and
corner cases uncovered by another engine. To sum, we conjecture that
test cases written for one engine could reveal bugs in different
engines as opposed to only manifesting false alarms. Third, we
analyzed the impact of using cross-engine differential testing to find
bugs (Section~\ref{sec:cross-engine-diff-testing-results}). The
rationale is that even black-box fuzzers that typically make valid
changes only in literals of the language could expose important
bugs. All in all, we want to assess the impact of practical approaches
that leverage diversity to find bugs in \js\ engines.

\subsection{Stability of Engines}
\label{sec:stability}

We evaluated stability with Test262~\cite{ecma262-conformance-suite},
the official \js{} conformance test suite for the ECMA262
specification. We ran this suite once a day for seven consecutive
days. Table~\ref{tab:test262} shows the number of failures over this
period. It is important to note that failures are expected as it takes
time for engines to adjust to the spec. It is also important to
highlight that all engines we analyzed, but \chakra{}, use some
variant of the Test262 suite as part of their regression
process\Mar{with fewer tests?}. We used the official version
instead\Mar{, which updates tests regularly?}. To sum, we observed
that \Fix{...} \Mar{justify regressions using the Kangax report}

\begin{table}[h]
  \centering
  \caption{\label{tab:test262}Number of failures in Test262 over
    a 7-day period.}
  \begin{tabular}{crrrrrrr}
    \toprule
    engine\textbackslash{}day& 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
    \midrule
    \jsc{} & 2278 & - & - & - & - & - & - \\
    \veight{} & 1213 & - & - & - & - & - & - \\
    \chakra{} & 7041 & - & - & - & - & - & - \\
    \smonkey{} & 1761 & - & - & - & - & - & - \\
    \bottomrule 
  \end{tabular}
\end{table}



\subsection{Test Transplantation}
\label{sec:transplantation}

\Mar{remember to say that we use dataset Table II''compile-in-all''.}

This section reports the results of test transplantation, \ie{}, we
analyzed the failures observed when running a test suite on an engine
other than the engine where the test suite
originated.

Table~\ref{tab:cross-testing} shows the number of failures for each
pair of test suite and engine. The first column shows the test suites
and the first row shows the engines that run those tests. We use a
dash (``-'') to indicate that we did not consider the combinations
that associated a test suite with its parent engine; failures in those
cases would either indicate regressions or flaky tests as opposed to
an unknown bug for that engine. We did not run the test cases that
fail on its parent engine in any other engines; we wanted all tests to
pass on their parent engine as a failure in that case would be a
stronger indicative of a bug. In total, 64 test cases were discarded
for that reason--6 from \jsc{} and 57 from \smonkey. In total, 7,029
test files (\ie{}, 38369-31276-64) were considered in this experiment,
which involved 24,922 test runs (\ie, (7029*4)-1021-57-2116).  Of
these, 812 failures were observed (3.25\% of the runs) across
\textbf{543} distinct files (7.72\% of the files). We analyzed each of
these file in separate to identify if the failure was fault-revealing
or a false positive.

We observed from the table that, considering this dataset, the Mozilla
\smonkey\ was the engine that failed the least whereas the Microsoft
engine \chakra\ failed the most. The \smonkey\ test suite also
revealed more failures than any other, perhaps as expected,
considering that it is the suite with more tests (see
Table~\ref{tab:test-suites}).

%% This suggests that the investiment in
%% testing paid off in this case.

\begin{table}[h]
  \centering
  \caption{\label{tab:cross-testing}Number of failures observed exchanging
  test suites.}
  \begin{tabular}{crrrr}
    \toprule
    test suite\textbackslash{}engine & \jsc{} & \veight{} & \smonkey{} & \chakra{}\\
    \midrule
    % \Fix{Versions (03.08)} & 234555 & 7.0.158 & 62.0b14 & 1.10.1 \\
    \Comment{
      Lembrar dos testes que os testes da propria engine falham:
      V8 0 
      JSC 2 
      Spidermonkey 58
    }
    \jsc{} & - & 9 & 10 & 58   \\
    \veight{} & 29 & - & 0 & 0  \\
%    \chakra{} & - & - & - & -  \\
    \smonkey{} & 217 & 106 & - & 278 \\
    Duktape & 0 & 4 & 4 & 1   \\
    \jerry{} & 23 & 25 & 22 & 23   \\
    JSI & 0 & 0 & 0 & 0   \\ 
   Tiny-js & 0 & 0 & 0 & 0  \\
    \midrule
   \textbf{total} & 270 & 145 & 36 & 361 \\
    \bottomrule 
  \end{tabular}
\end{table}

The piecharts from Figure~\ref{fig:piecharts-transplantation} show the
false and true positives observed with test transplantation. The top
piechart shows false alarms, which means that all
``transplanted'' tests should have passed in these cases. The absolute
number of tests in each category is shown in parentheses to the
right-side of each slice label, naming the source of false
positive. The sources are as follows:

\begin{figure}[h]
  \centering
  \subfloat[\label{fig:falsepositives}False Positives]{%
    \includegraphics[trim=0 150 0 100,clip,width=0.4\textwidth]{R/piecharts/falsepositives}
  }
  \label{1a}\hfill
  \subfloat[\label{fig:truepositives}True Positives]{%
    \includegraphics[trim=0 150 0 150,clip,width=0.4\textwidth]{R/piecharts/truepositives}    
  }
  \label{1b}\\
  \caption{\label{fig:piecharts-transplantation}False and True positives.}
\end{figure}


\begin{itemize}
  \item \textbf{Undefined Behavior.} False positives of this kind are
    manifested when tests cover implementation-dependent behavior, as
    defined in the ECMA262 specification~\cite{ecmas262-spec}. For
    example, one of the tests from \jerry\ uses the function
    \CodeIn{Number.toPrecision([precision])}, which translates a
    number to a string, considering a given number of significant
    digits. The floating-point approximation of the real value is
    implementation-dependent, making the test to pass only in
    \chakra\ for that test. As another example, some tests from
    \Fix{Y} use the implementation-dependent description message\Mar{what is the
      name of the property?} encapsulated in an \CodeIn{Error} object
    in assertions. As such, those tests fail in all engines but \Fix{Y}.

  \item \textbf{Timeout/OME\footnote{ome is for out of memory
      error.}.} False positives of this kind manifest because the
    engine that runs the test does not optimize the code as the
    original engine and that has an effect in the test outcome.  The
    test may fail to finish at the time budget or runs out of
    memory. For example, a test case from \jsc{} defines a function
    with a tail-call recursion. The test fails in all engines but
    \jsc{}, which implements tail-call optimization.

    %% \Igor{
    %%   For example, a testcase from \jsc{} suite explores the tail call
    %%   to optimize the recursion, the test uses a recursive function that
    %%   should be executed multiple times (ie. \mcodeid{1e6}). In this case, 
    %%   only the \jsc{} supports tail-call optimization and the testcase 
    %%   passes without failures. Another case can be found in \jerry{} suite
    %%   that uses a repetition statement to concatenate long strings 
    %%   exponentially, causing an out of memory error. The engines used 
    %%   supports the flags to increase memory, however we prefer to use 
    %%   the default engine build.
    %% }

  \item \textbf{Not implemented.} False positives of this kind
    manifest when a test fails because it covers a function that is
    part of the official spec, but is not implemented in the target
    engine. For example, at the time of writing, \chakra{} does not
    implement by default various properties from the \mcodeid{Symbol}
    object. These properties are only available activating ES6
    experimental mode with the flag \CodeIn{-ES6Experimental}.

  \item \textbf{Unknown.} This category corresponds to the cases we
    were unable to confidently diagnose the failure.

  %% \item \textbf{Error message mismatch.} Test fails because it
  %%   compares an error message, which is not portable across
  %%   engines. For example, \Igor{
  %%     a test from the engine \jsc{} uses an assert function called
  %%     \CodeIn{shouldThrow} that expects an specific error message. In this test, 
  %%     all engines throws the Error as expected, however only the message 
  %%     on \jsc{} engine match with the string expected in the test case.
  %%   }
\end{itemize}  

We observed that the sources of false positives were relatively
uniformly distributed, with the categories ``Unknown'' and
``Undefined'' holding more representation with \Fix{X and Y} reports,
respectively. Considering the true positives, we found a reasonable
number of duplicate reports, but not enough high to justify attempting
to automate the detection of duplicates. Section~\ref{sec:bugs}
exemplifies cases of false and true positives.


\begin{table}[t!]
\setlength{\tabcolsep}{2pt}    
%  \vspace{-3ex}
%  \scriptsize
  \centering
  \caption{List of bug reports issued as result of test
    transplantation. Url columns anonymized.}
  \label{tab:test-transplantation-bugs}
  \begin{tabular}{cccccccc}
    \toprule Issue\#    & Date & Engine  & Status  & \multicolumn{1}{c}{Url}  & Severity & Suite \\
    \midrule    
    1  & 4/18  & JavascriptCore  & New  & \anonym{\href{https://bugs.webkit.org/show\_bug.cgi?id=184749}{\#184749}} & - & \jerry{}      \\
   2  & 4/23 & \chakra{}  & \textbf{Confirmed}  & \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5033}{\#5033}} & 2 & \smonkey{}      \\
   3  & 4/29 & \chakra{}  & \textbf{Confirmed}   &
    \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5065}{\#5065}} & 2 & \smonkey{} \\
   \midrule
    \multirow{2}{*}{4}  & \multirow{2}{*}{4/29} & \chakra{} & \textbf{Confirmed} &    \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5067}{\#5067}} & 2 & \multirow{2}{*}{\smonkey{}}\\
                       &  &
    JavascriptCore & New &    \anonym{\href{https://bugs.webkit.org/show\_bug.cgi?id=185130}{\#185130} } &  -  & \\
   \midrule
   5 & 5/02  & JavascriptCore & New  & \anonym{\href{https://bugs.webkit.org/show\_bug.cgi?id=185208}{\#185208}} & - & \smonkey{} \\
   6 & 5/17  & \chakra{} & \textbf{Confirmed} & \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5187}{\#5187}} & 2 & \jsc{}\\
   7 & 5/21  & \chakra{} & \textbf{Fixed} & \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5203}{\#5203}} & 2 & \smonkey{}\\
   8 & 6/28  & \chakra{} & \textbf{Confirmed}  & \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5388}{\#5388}} & 2 & \jsc{}\\
   9 & 7/10  & \chakra{} & \textbf{Confirmed} & \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5442}{\#5442}} & 2 & \jerry{}\\
   10 & 7/18  & \chakra{} & \textbf{Fixed} & \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5478}{\#5478}} & 2 & \smonkey{}\\
   11 & 7/18  & JavascriptCore & New & \anonym{\href{https://bugs.webkit.org/show_bug.cgi?id=187777}{\#187777}} & - & \jerry{}\\
   12 & 7/18  & \chakra{} & \textbf{Fixed} & \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5549}{\#5549}} & 2 & \jerry{}\\
   13 & 8/07  & \chakra{} & New & \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5576}{\#5576}} & - & \jerry{}\\
   14 & 8/07  & JavascriptCore & \textbf{Fixed} & \anonym{\href{https://bugs.webkit.org/show_bug.cgi?id=188378}{\#188378}} & 2 & \jerry{}\\
   15 & 8/07  & \chakra{} & New & \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5579}{\#5579}} & - & \jerry{}\\
   16 & 8/07  & JavascriptCore & \textbf{Fixed} & \anonym{\href{https://bugs.webkit.org/show_bug.cgi?id=188382}{\#188382}} & 2 & \jerry{}\\
   17 & 8/08  & V8 & \textbf{Fixed} & \anonym{\href{https://bugs.chromium.org/p/v8/issues/detail?id=8033}{\#8033}} & 3 & \jerry{}\\
   18 & 8/08  & JavascriptCore & New & \anonym{\href{https://bugs.webkit.org/show_bug.cgi?id=188407}{\#188407}} & - & \jerry{}\\
   \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:test-transplantation-bugs} lists all bugs we found
with test transplantation. The first column shows the identifier we
assigned to the bug, column ``Date'' shows the date the bug was
reported in the format ``m/dd'', column ``Engine'' shows the affected
engine, column ``Status'' indicates the status of the bug report at
the time of the writing (\eg{},``New'', ``Rejected'', ``Confirmed'',
''Fixed''), column ``Url'' shows the web address for the corresponding
post in the bug tracking system where the discussion thread with
engine developers can be found (de-anonymized upon request), column
``Severity'' shows the severity level for confirmed bugs, and,
finally, column ``Suite'' shows the name of the engine that originated
the test. Considering severity levels, it is important to note that
\jsc{} uses five different severity levels~\cite{jsc-severity} whereas
\chakra{}~\cite{chakra-severity} and \smonkey{}~\cite{mozilla-severity}
use only two. As usual, the smallest the severity value the highest
the severity of the bug. We use a dash (``-'') as the severity level
for bugs with pending confirmation.

Of the \Fix{17} bugs we reported with test transplantation, \Fix{9}
were analyzed and moved from the status ``New'' to ``Confirmed''
(\ie{}, no bug we reported was rejected). Of these, \Fix{8} of them
are severity-2 bugs.  \chakra{} was the engine with the highest number
of confirmed bugs, \Fix{8} in total, with \Fix{1} bug
fixed. Considering the remaining engines, \veight{} confirmed one
bug. Curiously, Google engineers confirmed that report in a few
hours. Although we did not find any showstopper critical bugs, most of
the bugs are seemingly important as per the categorization given by
engineers. Analyzing the issue tracker of \chakra, we found that
severity-1 bugs are rare.


\subsection{Cross-Engine Differential Testing}
\label{sec:cross-engine-diff-testing-results}

This section reports the results obtained with differential
testing. More precisely, we report results obtained by fuzzing test
inputs, running those inputs on different engines, and checking the
outcomes with a differential oracle (see Figure~\ref{fig:workflow}).

\subsubsection{Methodology}
The methodology we used to assess the effectiveness of cross-engine
differential testing is as follows. To avoid experimental noise, we
only fuzz test files that pass in all engines. The intuition is that
we wanted to avoid the scenario where fuzzing produces a
fault-revealing input based on a test that was already
fault-revealing. This decision ensures that we can establish
cause-effect relationship between fuzzing and the observation of
discrepancy. Column ``no-fail-in-all'' from
Table~\ref{tab:test-suites} shows that a total of
\totalTestFilesPassInAll\ tests satisfy this restriction.

\vspace{1ex}
\noindent\textbf{Exploratory Phase.} For the first four months of the
study, our inspection process was exploratory.  Our goal in this phase
was to learn the extent to which \lo\ warnings could reveal bugs. We
realized that if we found that the ratio of bugs from \lo\ warnings
was rather low, we could focus our inspection efforts only on
\hi\ warnings. That finding would be important to reduce inspection
cost and guide future developments of this approach. To run this
experiment, we trained eight students in analyzing the warnings that
our infrastructure produces. The students were enrolled in a
graduate-level testing class. We listed warnings in a spreadsheet and
requested students to update an ``owner'' column indicating who was
working on it, but we did not enforce an order on the warnings the
students should inspect. Recall from Section~\ref{sec:clusterization}
that we clustered \lo\ warnings in buckets. For that reason, we only
listed one \lo\ warning per representative class/bucket in the
spreadsheet. First, we explained the possible sources of false alarms
with examples and then we asked the students to use the following
procedure when finding a suspicious warning. Analyze the parts of the
spec related to the problem and, if still suspicious, look for
potential duplicates on the bug tracker of the affected engine using
related keywords. If none was reported, indicate in the spreadsheet
that that warning is potentially fault-revealing. We encouraged
students to use lithium~\cite{lithium} to minimize long test cases;
the tests from the Mozilla suite are often longer than others. Only
after the co-authors checked the potential bugs, a bug report was
filed.

\vspace{1ex}
\noindent\textbf{Non-Exploratory Phase.} Results obtained in the
exploratory phase confirmed our expectations that most of the bugs
found during the initial four months of investigation were related to
\hi\ warnings. For that reason, we changed our inspection
strategy. This time, only the co-authors inspected the bugs using a
similar discipline as before. However, the set of warnings inspected
and the order of inspection changed. We restricted our analysis to
\hi\ warnings and, aware that we would be unable to analyze each and
every warning reported, we grouped those warnings per engine,
analyzing each group in a round-robin fashion. The rationale was to
give attention to each engine more uniformly, enabling more fair
comparison across engines. We repeated the following routine until
Aug. 22, 2018. We sampled five warnings associated with the group of a
given engine, analyzed these warnings, and moved to the group of
warnings associated with the next engine.

\subsubsection{Results}
Table~\ref{tab:summary-of-results} summarizes our results. It shows,
for each fuzzer and kind of bucket (\ie{}, hi or lo), the total number
of files involved, the number of buckets reported by our
infrastructure, the number of those that we classified as
bug-revealing (positive), and the number of those that we filed a bug
report. The difference between the last two columns corresponds to the
number of duplicates, \ie{}, the cases of bugs already reported in the
corresponding issue tracker. As expected, the hit ratio for the ``hi''
group is higher, indicating that chances of filing a bug report for a
``hi'' warning is higher.

\begin{table}[h]
  \centering
  \caption{\label{tab:summary-of-results}Summary of results.}
  \begin{tabular}{ccrrrr}
    \toprule
    \multirow{2}{*}{mode} & \multirow{2}{*}{type} & \multirow{2}{*}{\# files} &  \multicolumn{3}{c}{buckets} \\
    \cline{4-6}    
    & & & \multicolumn{1}{c}{\#} & \multicolumn{1}{c}{\# pos.} &
    \multicolumn{1}{c}{\# pos. filed} \\
    \midrule
    %% \multirow{2}{*}{nofuzz} & lo & \nofuzzFilesLO{} &  \nofuzzLOTotal{} & \Fix{.} & 11 (34) \\
    %% & hi & \nofuzzFilesHI{} & \nofuzzHITotal{} & \Fix{.} & \Fix{108 (-)} \\
    \multirow{2}{*}{\radamsa} & lo & \Fix{.} & \Fix{.} & \Fix{.} & \Fix{.} \\
    & hi & \Fix{.} & \Fix{.} & \Fix{.} & \Fix{.} \\
    \multirow{2}{*}{\quickfuzz} & lo & \Fix{.} & \Fix{.} & \Fix{.} & \Fix{.} \\
                             & hi & \Fix{.} & \Fix{.} & \Fix{.} & \Fix{.} \\    
    \bottomrule     
  \end{tabular}
\end{table}

Table~\ref{tab:false-positives} shows the distribution of false
positives per source. A false positive is a bucket that did not lend
itself in a bug report. The sources of imprecision are as defined in
Section~\ref{sec:transplantation} with the addition of two new
sources, which we did not observe before. The source ``Invalid Input''
indicates that the test violated some part of the specification. For
example, the test indirectly invoked some function with unexpected
arguments\Fix{provide concrete example}. The source ``Upcoming
Feature'' indicates that the test fails because it uses a feature that
is available in the original engine, but it is still not standard, and
the target engine does not support that feature. For example,
\Fix{provide concrete example} 

\begin{table}[t]
  \centering
  \caption{\label{tab:false-positives}Distribution of false
    positives buckets per source.}
  \begin{tabular}{crr}
    \toprule
    & \radamsa\ & \quickfuzz\ \\
    Undefined & - & - \\
    Timeout/OME & - & - \\
    Not Implemented & - & - \\
    Upcoming Feature & - & - \\
    Error Message Mismatch & - & - \\    
    Invalid Input & - & - \\    
    \bottomrule     
  \end{tabular}
\end{table}

% \Igor{
%   Para realizar nossos experimentos, tivemos que decidir qual as 
%   melhores abordagens para aumentar a efetividade dos fuzzers.
%   Os fuzzers mutacionais se mostraram mais eficientes em relacao
%   aos fuzzers geracionais, entao focamos neste ponto.
%   Utilizamos os fuzzers radamsa, QuickFuzz e \Fix{others} nas suas configuracoes
%   default para realizar a mutacao em arquivos existentes.
%   O fuzzer radamsa eh um fuzzer mutacional agnostico de linguagem e possui algumas
%   deficiencias para a nossa infraestrutura como a falta de uma gramatica para guiar a suas mutacoes, 
%   pois muitas das vezes a unica mutacao ocorre em uma palavra dentro de um comentario por exemplo.
%   O fuzzer QuickFuzz tem uma gramatica JS integrada e sua mutacao ocorre de forma mais robusta que o 
%   radamsa, gerando arquivos mais relevantes.
%   Nossa infraestrutura usa a library Esprima\footnote{cite esprima url} para garantir que o arquivo fuzzado
%   é um arquivo de teste relevante, seguindo estes requisitos:
%   \begin{itemize}
%     \item it contains only unicode text
%     \item it is parseable (i.e., it is structurally well-formed)
%     \item it does not contain engine-specific functions  
%     \item it does not empty
%   \end{itemize}

% }

\section{Bugs Found}
\label{sec:bugs}

%% Although there are many features yet to implement in our
%% infrastructure,

\begin{table*}[h!]
  \vspace{-3ex}
%  \scriptsize
  \centering
  \caption{List of bug reports issued as result of cross-engine
    differential testing.}
  \label{tab:bugs}
  \begin{tabular}{ccccccccc}
    \toprule
    Issue\#    & Date & Fuzz & Engine  & Status  &
    \multicolumn{1}{c}{Url}  & Severity & Priority & Suite \\
    \midrule
    \multicolumn{9}{c}{\emph{Exploratory Phase}} \\
    \midrule    
    1  & 4/12 & radamsa & \chakra{}   & \textbf{Fixed}  &
    \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/4978}{\#4978}}
    & 2 & LO & \jsc{} \\ 
    2  & 4/12 & radamsa & \chakra{}   & Rejected  &
    \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/4979}{\#4979}}
    & - & HI & \jsc{} \\
    3  & 4/14 & radamsa & JavascriptCore  & New &
    \anonym{\href{https://bugs.webkit.org/show\_bug.cgi?id=184629}{\#184629}
    } & -  & HI & \jsc{}    \\
    4  & 4/25 & radamsa & \chakra{}  & \textbf{Fixed}     &
    \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5038}{\#5038}}
    & 2 & HI & \jerry{}   \\
    5  & 4/29 & radamsa & JavascriptCore  & \textbf{Fixed}  &
    \anonym{\href{https://bugs.webkit.org/show\_bug.cgi?id=185127}{\#185127}
    } & 2  & HI  & \jerry{}\\
    \midrule
    \multirow{2}{*}{6} & \multirow{2}{*}{4/30}  &
    \multirow{2}{*}{radamsa} & \chakra{} & \textbf{Confirmed} &
    \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5076}{\#5076}}
    & 2 & \multirow{2}{*}{HI} & \multirow{2}{*}{TinyJS}\\    
                        &                        &        &
    JavascriptCore & New &
    \anonym{\href{https://bugs.webkit.org/show\_bug.cgi?id=185156}{\#185156}}
    & - &  & \\
    \midrule
    7 & 5/02 & radamsa & JavascriptCore  & \textbf{Fixed} &
    \anonym{\href{https://bugs.webkit.org/show\_bug.cgi?id=185197}{\#185197}}
    & 2 & LO & \smonkey{} \\
    8 & 5/10 & radamsa & \chakra{} & \textbf{Confirmed} &
    \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5128}{\#5128}}
    & 3 & HI & \jerry{} \\
    9 & 5/17 & radamsa & \chakra{} & \textbf{Fixed} &
    \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5182}{\#5182}}
    & 2 & HI & \veight{}\\
    10 & 5/24 & radamsa & JavascriptCore & \textbf{Fixed}  &
    \anonym{\href{https://bugs.webkit.org/show\_bug.cgi?id=185943}{\#185943}}
    & 2 & HI & \jsc{}\\
    11 & 6/26 & radamsa & JavascriptCore & \textbf{Fixed}  &
    \anonym{\href{https://bugs.webkit.org/show_bug.cgi?id=187042}{\#187042}}
    & 2 & HI & \jerry{}\\
    12 & 7/10 & quickfuzz & \jsc{} & \textbf{Fixed}  &
    \anonym{\href{https://bugs.webkit.org/show_bug.cgi?id=187520}{\#187520}}
    & 2 & HI & \jerry{}\\
    13 & 7/10 & quickfuzz & \chakra{} & \textbf{Confirmed}  &
    \anonym{\href{https://github.com/Microsoft/\chakra{}Core/issues/5443}{\#5443}}
    & 2 & HI & \jerry{}\\
    \midrule        
    \multicolumn{9}{c}{\emph{Non-Exploratory Phase}} \\
    \midrule    
   \bottomrule
  \end{tabular}
\end{table*}


This section shows results obtained with our
infrastructure. Table~\ref{tab:bugs} shows the list of bugs we
reported on issue trackers of different engines in the period of 42
days. So far, ten of the bugs we reported
were confirmed, two of which were fixed. One bug report we
submitted was rejected on the basis that the offending JS file
manifested an expected incompatibility across engine
implementations.
Note from the table that all bug
reports still waiting for confirmation are associated with the
\jsc{} engine (JSC). A closer look at the JSC issue tracker
showed that the triage process is very slow for that engine. 
\Igor{
The bugs reported on \chakra{}'s bug tracker was defined as confirmed, 
however the bugs are included on a milestone for the next release
following an internal priority.
}
As of now, we did not find any new bug on \smonkey{} and V8; 
the bugs we found were duplicates and were not reported. Finally, it is
worth noting that \Fix{11 of the 19} JS files that manifested
discrepancies were \emph{not} produced with fuzzing (column
``Fuzz''). These are test files from suites of different engines. This
observation emphasizes the importance of continuously collecting test suites from
multiple sources; today, we use test suites from seven different open
source engines, including a total of 30K test files.

\Mar{justify why we discuss these bugs} \Mar{discuss other bugs}

\vspace{1ex}\noindent\textbf{Bug \# 6.} The JS code \CodeIn{var a = \{valueOf:~function()\{return
  ``\textbackslash{}x00''\}\} assert(+a === 0)\}} 
manifested a bug in the \js{} engine \chakra{}.  The object
property \CodeIn{valueOf} stores a function that returns a primitive
value identifying the target object~\cite{valueof}. The original
version of this code returns an empty string whereas the version of
the code modified by the Radamsa fuzzer~\cite{radamsa} returns a string
representation of a null character (called \CodeIn{NUL} in ascii). The
unary plus expression ``\CodeIn{+a}", appearing in the assertion, is
equivalent to the abstract operation \CodeIn{ToNumber(a.valueOf())}
that converts a string to a number, otherwise the operation returns
NaN (Not a Number)\cite{unary-plus}. For this case, \chakra{} evaluates
the unary plus to NaN as expected, as the null character cannot be
converted. As result, the test fails as expected. \chakra{}, in contrast,
incorrectly converts the string to zero, making the test to pass. All
other engines fail on this test. As Table~\ref{tab:bugs} shows, the
\chakra{} team fixed the issue soon after we reported the problem.

\subsection{False Positives}

An example of a warning from the HI group is defined in Figure~\ref{fig:hi-priority}. 
This is a testcase from WebKit.es6 suite that it was mutate by radamsa fuzzer, the 
initial seed has in line 3 the code \CodeIn{"foo".repeat(3)} but the fuzzer changed the 
integer 3 to a big integer number. We reported this case in \chakra{} due a core dumps that occurs
during the runtime, however this case was rejected due \CodeIn{incompatibility by design} that
this is an intentional behavior of engine that crash the process if it runs out of memory.

\begin{figure}[h!]
  \centering
  \scriptsize
  \lstset{escapeinside={@}{@},
    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c,
    numberstyle=\tiny,
    morekeywords={assertEq, var, yield, in, function, 
    typeof, return, throw, new, Error, if},
  }
  \begin{lstlisting}
function test() {
  return typeof String.prototype.repeat === "function"
    && "foo".repeat(657604378) === "foofoofoo";
}
if (!test())
  throw new Error("Test failed");
  \end{lstlisting}
  \normalsize
  \caption{Warning captured as HI priority.}
  \label{fig:hi-priority}
  \end{figure}

% \subsection{LO bugs}
% 
% \Igor{For example, a bug caught by our environment and reported as LO priority was reported 
% on \chakra{} engine, the JS code can be found in Figure~\ref{fig:lo-priority}. In this case,
% this is a seed from Mozilla suite (\CodeIn{mozilla/non262/statements/for-in-with-assignments.js}),
% the warning was not caught by the \CodeIn{assertEq} function that compares if two arguments are equals, 
% the bug appears inside the generator function\footnote{Generators \url{https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*}}
% at line 2. According to ES6 specification\footnote{ES6 YieldExpression \url{https://www.ecma-international.org/ecma-262/8.0/index.html\#prod-YieldExpression}},
% it is allowed the use of \CodeIn{yield in} in a \CodeIn{for} loop. In our infrastructure,
% only \chakra{} engine fails with an error output \CodeIn{SyntaxError: Syntax error}, due
% the output does not shows nothing related with assertions, we considered this one as a warning from LO group.
% Until now, the issue was confirmed and waiting for merge/closed.
% \Fix{checar ate o prazo de submissao. Essa issue esta confirmada e com commits, falta apenas o merge.}
% }
% \begin{figure}[h!]
%   \centering
%   \scriptsize
%   \lstset{escapeinside={@}{@},
%     numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
%     basicstyle=\ttfamily\scriptsize, boxpos=c,
%     numberstyle=\tiny,
%     morekeywords={assertEq, var, yield, in, function},
%   }
%   \begin{lstlisting}
% function* g1() {
%   for (var x = yield in {}) ;
% }
% var it = g1();
% assertEq(it.next().done, false);
% assertEq(it.next().done, true);
%   \end{lstlisting}
%   \normalsize
%   \caption{Bug caught by our environment as LO priority.}
%   \label{fig:lo-priority}
%   \end{figure}

\section{Related Work}

\subsection{Testing \js{} Engines}
\label{sec:testing-js-engines}
The closest work to ours was done by Patra and
Pradel~\cite{patra2016learning}, where they proposed a
language-agnostic fuzzer to find cross-browser HTML+JS
discrepancies. Their work aims at building and evaluating an
infrastructure for differential testing of runtime engines, such as JS
engines nd WebAssembly engines. The sensible parts of the
infrastructure are the checks of input validity (as to reduce
waste/cost) and output correctness (as to reduce false
positives). Patra and Pradel work differs from ours in that their main
contribution is analytical whereas our main contribution is
empirical--we aim at assessing reliability of \js\ engines and find
bugs on them using simple approaches, such as test transplantation.

Fuzzing is an active area of investigation with development of new
techniques both in academia and industry. Several fuzzing tools exist
focused on \js. Section~\ref{sec:objects:fuzzers} briefly explain
different fuzzing strategies and tools. Existing techniques prioritize
automation with a focus on finding crashes; see the sanitizers used in
libFuzzer~\cite{libfuzzer-tutorial}, for instance. In general, it is
important for these tools that a warning reveals something potentially
alarming as a crash given that fuzzing is a time-consuming operation,
\ie{}, the ratio of bugs found per inputs generated is often very low.
Our approach contrasts with that aim as we focus on finding errors
manifested on the output, which rarely result in crashes and,
consequently, would go undetected by current fuzzing approaches. It is
should be noted, however, that such problems are not unimportant as
per the severity levels reported in
Tables~\ref{tab:test-transplantation-bugs} and~\ref{tab:bugs}.

\subsection{Differential Testing}
Several different applications of differential testing have been
proposed in recent years. Chen and
colleagues~\cite{Chen:2018:RDT:3180155.3180226} recently proposed a
technique to generate X.509 certificates based on Request For
Proposals (RFC) as specification with the goal of detecting bugs in
different SSL/TLS implementations. Those bugs can comprimise security
of servers which rely on these certificates to properly authenticate
the parties involved in a communication session. Lidbury and
colleagues~\cite{Lidbury:2015:MCF:2737924.2737986} and Donaldson and
colleagues~\cite{Donaldson:2017:ATG:3152284.3133917} have been
focusing on finding bugs in programs for graphic cards (\eg{},
OpenCL). These programs use the Single-Instruction Multiple-Data
(SIMD) programming abstraction and typically run on GPUs.  Perhaps the
application of differential testing that received most attention to
date was compiler testing. In 1972, Purdom~\cite{Purdom1972} proposed
the use of a generator of sentences from grammars to test correctness
of automatically generated parsers. After that, significant progress
has been made. Lammel and Shulte proposed Geno to cross-check XPath
implementations using grammar-based testing with controllable
combinatorial coverage~\cite{10.1007/11754008_2}. Yang and
colleagues~\cite{Yang:2011:FUB:1993498.1993532} proposed CSmith to
randomly create C programs from a grammar, for a subset of C, and then
check the output of these programs in different compilers (\eg{}, GCC
and LLVM). Le and colleagues~\cite{Le:2014:CVV:2594291.2594334}
proposed ``equivalence modulo inputs'', which creates variants of
program which should have equivalent behavior compared to the
original, but for which the compiler manifests
discrepancy. Differential testing has also been applied to test
refactoring engines~\cite{Daniel:2007:ATR:1287624.1287651}, to test
symbolic engine implementations~\cite{Kapus:2017:ATS:3155562.3155636},
and to test disassemblers and binary
lifters~\cite{Paleari:2010:NDD:1831708.1831741,Kim:2017:TIR:3155562.3155609}. All
in all, it has shown to be flexible and effective for a wide range of
applications. Surprisingly, not much work has been done on
differential testing of \js\ engines. Mozilla uses differential
testing to look for discrepanices across different configurations of
the same version of its \smonkey\ engine (using the ``compare\_jit''
flag of jsfunfuzz~\cite{jsfunfuzz}) whereas we focus on discrepancy
across engines. Patra and Pradel evaluated their language-agnostic
fuzzing strategy using differential testing. Their focuses on finding
differential bugs across multiple
browsers~\cite{patra2016learning}. As such they specialized their
fuzzer to HTML and JS (see Section~\ref{sec:testing-js-engines}). In
contrast to Patra and Pradel, we did not propose new techniques; our
contribution was empirical.

\subsection{Testing \js\ Programs}
Patra and colleagues~\cite{Patra:2018:CFU:3180155.3180184} proposed a
lightweight approach to detect conflicts in \js\ libraries that occur
when names introduced by different libraries collide. This problem was
found to be common as the design of \js\ allows for overlaps in
namespaces. A similar problem has been investigated by Nguyen and
colleagues~\cite{nguyen-etal-icse2014} and Eshkevari and
colleagues~\cite{eshkevari-etal-icpc2014} in the context of PHP
programs, which are popular in the context of Content Management
Systems as WordPress. The focus of this paper is on testing
\js\ engines as opposed to \js\ programs. Our goal is therefore
orthogonal to theirs.

\section{Conclusions}

\Fix{...}

%\section*{Acknowledgment}

%\bibliographystyle{IEEEtran}
\balance
\bibliographystyle{plain}
\bibliography{references,../docs/google-research-awards-latam/tmp}

\end{document}

%%  LocalWords:  bytecodes JScript Ecma ome IoT Kangax DT JS fuzzer
%%  LocalWords:  jsfunfuzz jit funfuzz \smonkey{} toPrecision JSVU
%%  LocalWords:  GoogleChromeLab's cccr js runtime Fuzzers Radamsa de
%%  LocalWords:  QuickFuzz fuzzers Grammarinator LangFuzz Megadeth
%%  LocalWords:  AFL libFuzzer toolchain sanitizers arrowed eshost
%%  LocalWords:  cli pre EcmaScript dataset piecharts piechart \chakra{}
%%  LocalWords:  JavascriptCore Url radamsa TinyJS \jerry{} JSC
%%  LocalWords:  quickfuzz \jsc{} \chakra{}'s valueOf NUL ascii
%%  LocalWords:  unary ToNumber NaN testcase WebKit typeof foofoofoo
%%  LocalWords:  SSL TLS ICSE
