%\documentclass[conference]{IEEEtran}
\documentclass[10pt,conference,anonymous]{IEEEtran}
\IEEEoverridecommandlockouts

%% Marcelo added this
\makeatletter
\renewcommand\footnoterule{%
  \kern-3\p@
  \hrule\@width.4\columnwidth
  \kern2.6\p@}
  \makeatother

\input{packages}
\input{macros}

\begin{document}

%Should I Fuzz my Inputs or Improve my Tests? 
\title{Assessing Reliability of JavaScript Engines by Leveraging Diversity}

%% \author{
%% \IEEEauthorblockN{Sabrina Souto}
%% \IEEEauthorblockA{State University of Para\'iba\\
%% Para\'iba, Brazil\\
%% sabrinadfs@gmail.com}
%% \and
%% \IEEEauthorblockN{Marcelo d'Amorim}
%% \IEEEauthorblockA{Federal University of Pernambuco\\
%%   Pernambuco, Brazil\\
%%   damorim@cin.ufpe.br}
%% \and
%% \IEEEauthorblockN{Rohit Gheyi}
%% \IEEEauthorblockA{Federal University of Campina Grande\\
%%   Para\'iba, Brazil\\
%%   rohit@dsc.ufcg.edu.br}
%% }

\maketitle

%% page numbering -M
\thispagestyle{plain}
\pagestyle{plain}

%% JavaScript (\js{}) is a popular programming language for the
%% web. Finding errors in JS runtime engines is an important problem.
\begin{abstract}
This paper assesses the impact of Differential Testing (DT) to find
functional bugs in JS engines. This is an important problem given the
importance of JS today. DT has shown successful in finding bugs in
compilers and runtimes, but has not been thoroughly explored in this
important domain. Our study \Fix{...}
\end{abstract}

\begin{IEEEkeywords}
...
\end{IEEEkeywords}

\section{Introduction}

JavaScript (\js{}) is today one of the most popular programming
languages for the web~\cite{business-insider,stackify}. The interest
of the community for the language encourages constant improvements in
its specification and implementations. It is natural to expect that
such improvements entails sensible changes in runtime engines. Even
small changes can have a large impact in practice. For example, in
October 2014 a new attribute added to Array objects caused a bug that
resulted in MS Outlook web not working under
Chrome~\cite{array-bug-chromium-issue4247,array-bug-discussion}. To
sum, finding bugs in JS engines is an important problem specially
because of the speed the specification evolves and the availability of
various public engines today~\cite{kangax}.


%% WHAT WE DID
This paper reports the results of a study we conducted to assess
reliability of existing JS engines by leveraging implementation
diversity in two orthogonal ways. The first method considered is
\emph{test transplantation}. This approach evaluates the effects that
running \js{} test files from a given engine have in detecting bugs in
other engines. The intuition is that when bugs are found in some
engine implementation a test case is created and added to the
regression suite. Those test inputs often check interesting corner
cases that could reveal bugs in different engine implementations. The
second method considered is \emph{cross-engine differential
  testing}. This method fuzzes existing inputs, using existing
techniques, and then compares the output produced by different engines
using a differential oracle. One possible cause of output discrepancy
is a bug. It is worth noting that Differential
Testing~\cite{Brumley-etal-ss07} (DT) has been applied in a variety of
contexts to find bugs in
software~\cite{Yang-etal-pldi11,Chen-etal-fse2015,Argyros-etla-ccs16,Chen-etal-pldi16,petsios-etal-sp2017,SivakornAPKJ17,Zhang:2017:ATD:3097368.3097448}. It
automates test generation in scenarios where multiple implementations
of a system exist. DT leverages the diversity across system's
implementations to detect anomalous behavior. For example, Mozilla
runs JS files--created with the grammar-based fuzzer
jsfunfuzz~\cite{jsfunfuzz}--against different configurations of their
SpiderMonkey engine~\cite{jsfunfuzz-mozilla-bug} to find discrepancies
across different configurations of the same build (\eg{}, enabling or
not eager JIT compilation). They use this technique since 2002 and
have been able to find over 270 bugs since
then~\cite{jsfunfuzz-at-mozilla}, including security bugs.  In
contrast, cross-engine differential testing--the technique used in
this work--checks correctness by comparing the output produced by
\emph{different engines} on a fuzzed
input~\cite{patra2016learning}. This approach is not popularized,
certainly because it is more challenging to fully automate--a number
of legitimate reasons exist, other than a bug, for a test execution to
manifest discrepancy (see Table~\ref{tab:false-positives}).

%% WHAT WE FOUND
To sum, we found that \Fix{...}

%% LIST OF CONTRIBUTIONS
This paper makes the following contributions \Fix{...}

\section{JavaScript}
\label{sec:es6-design}

JavaScript engines are virtual machines that parse source code and
compile it in bytecodes. These engines implement some version of
ECMAScript\footnote{The name JavaScript still prevails today,
  certainly for historical reasons.} (\es{}), which emerged with the
goal to standardize variants of the language, such as Netscape's
JavaScript and Microsoft's JScript. The \es{} specification is
regulated by Ecma International~\cite{es6-website} under the
TC39~\cite{tc39-github} technical committee.  Every year, a new
version of the \es{} specification is produced with new features and
minor fixes. The canonical spec today is
ES6~\cite{ecmas262-spec-repo,ecmas262-spec}. Features of these
versions and supporting engines are publicly available~\cite{kangax}.
In the following, we briefly describe different sources of
discrepancies that can emerge across different engine implementations
as results of the active changes in the \es{} specification.

%% \Mar{what is that exactly? can we cite? is that only spec or actually
%%   code as most of java.* libraries.}  \Igor{seems like an API. They
%%   define the language behavior; grammar; values to static variables;
%%   the methods for each "class"(Object)...  These irregularities
%%   appears on engine implementation, maybe an wrong if statement and so
%%   on.} \Mar{Igor, avoid general discussion like $\leftarrow$this. look for
%%   explanation in some forum/tutorial and explain **when** you
%%   undertand the JS ecosystem design and can explain with confidence.}


\subsection{Undefined (Implementation-Dependent) Behavior}
\label{sec:imp-dep-behavior}


The \es{} spec uses the label ``implementation-dependent'' to indicate
cases where each engine developer can decide how to implement some
aspect of a functionality. In these cases, execution may differ from
engine to engine. One of the reasons implementation-dependent behavior
exists is to enable compiler optimizations. For example, the
\mcodeid{for-in} loop does not clearly specify the iteration
order~\cite{so-forin-undefined,javascript-in-chrome} and different
engines capitalize on that~\cite{for-in-undefined}. As another
example, \Fix{...}

\Mar{explain what is an **acceptable** incompatibilities. cite example
bug reports.}
\Igor{For example, the latest version of ES specs~\cite{es6-spec} describes that
the function \CodeIn{Number.toPrecision(precision)}, in general,
if precision is defined the method returns a String 
containing the Number value represented either 
in decimal exponential notation or in decimal 
fixed notation~\cite{es6-toPrecision}. The Number object uses a 
property that the specification describes as approximately so when we use 
toPrecision with a bigger precision, the values of each engine may vary, this
could be considered as a case of incompatibility by engine
design.}\Mar{I don't understand the example very well and don't understand the
  relationship with HI/LO.}

\subsection{Randomness}

\sloppy Some elements in JS have non-deterministic behavior (\eg{},
Math.random and Date). A test that make decisions based on these
elements could, in principle, produce different outcomes on different
runs. Carefully-written test cases should not manifest this kind of
flakiness~\cite{luo-etal-fse2014,palomba-zaidman-icsme2017}, but
fuzzed test files are more likely to manifest them.

\subsection{\Fix{Other}}

1) The stack depth in JS is often close to the real (native) C stack
depth, nevertheless observable by JS through recursion. This causes
differential behavior even on the same engine when JITs are enabled or
disabled, as the stack frame sizes vary. LangFuzz in particular
produces a lot of recursions and hits these problems even more often
than jsfunfuzz.\Mar{ask C. Holler. it is unclear to me how the output
  could be different.}

\subsection{Bugs}

It is worth to recall that there is no formal specification for \es{};
the specification is written in natural language. As such, differences
in behavior from one engine to another, other than those mentioned
above (Sections~\ref{sec:imp-dep-behavior}
and~\ref{sec:undef-behavior}), can occur. But these discrepancies are
not acceptable.


\section{Differential Testing Infrastructure}
\label{sec:design}


%\begin{wrapfigure}[10]{r}[0pt]{0.45\textwidth}
\begin{figure}[t]
  \centering
%  \includegraphics[trim=20 350 200
%    0,clip,width=0.35\textwidth]{google-awards-workflow}
  \includegraphics[trim=0 350 0 0,clip,width=0.65\textwidth]{diff-testing-runtimes}
  \caption{\label{fig:workflow}Infrastructure.}
\end{figure}

Figure~\ref{fig:workflow} illustrates the infrastructure we used for
differential testing.  The bug-finding process takes on input JS files
from regression test suites of various JS engines and generates
warnings on output. Boxes in the figure denote encapsulation; arrowed
lines indicate data flow. The cycle icons denote repetition--the
leftmost icon indicates that each file in the input list will be
analyzed in separate whereas the rightmost icon shows that a single
file will be fuzzed multiple times. The bug-finding process works as
follows.

New inputs are obtained for a given test input using some
off-the-shelf input fuzzer~\cite{fuzz-testing-history}.\Comment{
  Several fuzzing methods have been proposed in the past, varying with
  respect to how new inputs are
  generated~\cite{afl,honggfuzz,grammarinator,jsfunfuzz,radamsa}.}
Section~\ref{sec:objects:fuzzers} describes the fuzzers we selected.
The oracle checks whether or not the output produced for the fuzzed
file is consistent across all engines. In case the test passes in all
engines or fails in all engines (\ie{}, the output is consistent), the
infrastructure discards the test. Otherwise, it considers the input as
potentially fault-revealing; hence interesting for human
inspection. Note that a number of reasons exist, other than a bug, to
explain discrepancy (see Table~\ref{tab:false-positives}). We used the
open-source eshost-cli project~\cite{eshost-cli} for checking output
discrepancy. Microsoft uses that tool in the Chakra JS engine.

%The checker can produce multiple warnings for a given input. 

The infrastructure outputs a list of warnings for human inspection.
To reduce the number of false alarms, we classified warnings in two
kinds, reflecting their likelihood to manifest a real bug. Warnings of
the kind ``\hi{}'' are associated with the cases where the anomaly has
been detected in the execution of the test code or its harness. The
rationale is that the test in this group executed without violating
any internal checks of the API. In constrast, warnings in the
``\lo{}'' group reflect the cases where the anomaly was observed
during the execution of functions not related to the test itself. We
found that different engines often check pre-conditions of those
functions differently. It can happen, for example, that one engine
enforces a weaker pre-condition compared to another and that is
acceptable.  In those cases, our infrastructure would observe a
discrepancy that is more likely to be associated with an invalid input
produced by the fuzzer, \ie{}, a bug in the test not the
code. Although we did find real bugs from ``\lo{}'' warnings, the
proportion was much lower compared to the ``\hi{}'' warnings. Overall,
only \Fix{12\%} of the reals bugs we found originated from ``\lo{}''
warnings.

\Mar{explain what is a bucket (we refer to it later).} 

\section{Objects of Study}
\label{sec:methodology}

This section discusses the objects we used in our study.

\subsection{Engines}
\label{sec:methodology:engines}~We selected 
JS engines according to the following criteria.

\begin{itemize}
\item Released latest version after Jan 1, 2018
\item Contains more than 1K starts, if on GitHub  
\item Uses a public issue tracker
\end{itemize}  

We look for highly-maintained (as per the first criterion) and popular
(as per the second criterion) projects. As we wanted to report bugs,
we also looked for project with public issue
trackers. Table~\ref{tab:engines} lists the engines we analyzed in
this work. It is worth noting that we used GoogleChromeLab's JSVU
tool~\cite{jsvu} to automatically install and configure versions of
different JS engines in our host environment. This is important as we
aim to use the most recent stable versions of each engine as to avoid
reporting old and already-fixed bugs to developers.

%% Table~\ref{tab:engines} shows the engines we selected in this
%% study. We made an exception to the second criterion with
%% XS~\cite{xs2018repo}. As the project is young, created in Oct 2017, we
%% thought there was insufficient time to obtain 1K stars for XS. We
%% still considered this project as it seems to be attracting interest
%% from the community\Fix{is this true?}

\begin{table}[t]
  \centering
  \caption{\label{tab:engines}Engines Analyzed.}
  \begin{tabular}{cccr}
    \toprule
    Team & Name & URL & \# Stars \\
    \midrule
    Apple & JavaScriptCore (WebKit) & \cite{jsc2018repo} & \multicolumn{1}{c}{3300+} \\
    Google & v8 & \cite{v82018repo} & 9800+ \\
%    JerryScript & JerryScript & \cite{jerryscript2018repo} & 3100+ \\
    Microsoft & Chakra & \cite{chakra2018repo} & 7200+ \\
%    Moddable & XS & \cite{xs2018repo} & 140+ \\
    %% \midrule
    %% \multirow{2}{*}{Mozilla} & Rhino & \cite{rhino2018repo} & 1800+ \\
    Mozilla & SpiderMonkey & \cite{spidermonkey2018repo} & \multicolumn{1}{c}{1100+} \\
   \bottomrule     
  \end{tabular}
\end{table}

\subsection{Seed JS files\label{sec:seeds}}~
We selected test files from several public projects from GitHub. We
looked for self-contained tests from JS engine projects, initially
using the GitHub REST API~\cite{github-rest-api}. We noticed that some
of the tests we found depend on external libraries, which not all
engines we use (see Section~\ref{sec:methodology:engines}) support. We
decided to discard those. For example, many tests we found required a
Node.js runtime~\cite{node} for execution. The main source of test
cases is the offical TC-39~\cite{tc39-github} conformance test suite
for the ECMA262 specification~\cite{ecmas262-spec}. This test suite
contains 87\% of all the tests we used. Furthermore, we
considered test suites of three of the four engines mentioned in
Section~\ref{sec:methodology:engines} and four other engines.
Table~\ref{tab:test-suites} shows the JS sources we considered for
each of the selected projects. Overall, we found a total of
\totfiles{} JS files. It is worth noting that some test suites use
distinct frameworks for testing and require external libraries to run
the tests.  For these projects, we needed to make small
changes in the testing infrastructure to be able to run the tests
uniformly across all engines. More precisely, we needed to mock
some functions, related to the test harness, which are only available to
certain engines.

\begin{table}[t]
  \centering
  \caption{\label{tab:test-suites}Test Suites. The sections of the
    table show, respectively, the TC-39 conformance test suite, suites
    from engines we analyzed, and suites from other engines.}
  \begin{tabular}{ccr}
    \toprule
    Name & Source & \# JS files \\
    \midrule
    TC-39 (Test262) & \cite{ecma262-conformance-suite} & 31,276 \\
    \midrule
    Mozilla & \cite{mozilla} & 2,633 \\
    V8 & \cite{v8} & 74 \\
    WebKit & \cite{webkit} & 1,031 \\
    \midrule    
    Duktape & \cite{duktape} & 1,195 \\ 
    JerryScript & \cite{jerryscript} & 1,950 \\
    JSI & \cite{jsi} & 98 \\
    Tiny-js & \cite{tinyjs} & 48 \\    
    % Chakra & \cite{chakracore} & 2632 \\
    \midrule
     &  & 38,305 \\
   \bottomrule     
  \end{tabular}
\end{table}

% For example to run the JerryScript tests it was necessary 
% use the unit-test package to run it, but with our changes we added the assertion
% does not have an assertion in the test file
% \Fix{add code to explain}

\subsection{Fuzzers}
\label{sec:objects:fuzzers}

%% In the following, we describe the list of fuzzers we analyzed. We
%% initially considered generational grammar-based fuzzers and mutational
%% fuzzers.

Fuzzers are typically categorized in two main groups--those that build
inputs anew (generational) and those that modify existing inputs
(mutational). We used two black-box mutational
fuzzers\Comment{Radamsa~\cite{radamsa} and QuickFuzz~\cite{quickfuzz}}
in this study. In the following, we provide rationale for this
selection.

Generational fuzzers are typically grammar-based. These fuzzers
generate a new file using the grammar of the language whose inputs
should be fuzzed. Intuitively, those fuzzers implement a traversal of
the production rules of the input grammar to create syntax trees,
which are then pretty-printed. Consequently, this approach produces
inputs that are syntactically valid by construction. We analyzed four
grammar-based fuzzers--Grammarinator~\cite{grammarinator},
jsfunfuzz~\cite{jsfunfuzz},
LangFuzz~\cite{Holler:2012:FCF:2362793.2362831}, and
Megadeth~\cite{grieco2016quickfuzz}.  Unfortunately, none of those
were effective out of the box. For example, we produced 100K inputs
with Grammarinator and only \Fix{X} of those were valid. With
Megadeth, we were able to produce \Fix{Y, Y$>$Y?}  valid inputs as it
contains some heuristics to circumvent violations of certain type
rules such as \Fix{variable used must be defined?}. Nonetheless,
running those inputs in our infrastructure we were unable to find
discrepancies. Inpecting those inputs, we realized that they reflected
very simple scenarios. To sum, a high percentage of inputs that
Grammarinator and Megadeth generated were semantically-invalid that we
needed to discard whereas the valid inputs manifested no
discrepancies. Considering jsfunfuzz~\cite{jsfunfuzz}, we noticed
that, in addition to the issues mentioned above, it produces inputs
that use functions that are only available in the SpiderMonkey
engine. We would need either to mock those functions in other engines
or to discard those tests. Considering
LangFuzz~\cite{Holler:2012:FCF:2362793.2362831}, the tool is not
publicly available. One fundamental issue associated with generational
fuzzers in our context is that the tests they produce do not contain
assertions; to enable the integration of this kind of fuzzers in our
infrastructure, we would need to look for discrepancies across
compiler messages.  All in all, although grammar-based fuzzers have
been shown effective to find real
bugs~\cite{Holler:2012:FCF:2362793.2362831}, generating valid inputs,
albeit promising conceptually, still requires some engineering
effort. For this reason, we did not consider those fuzzers in this
study.

%% Our
%% infrastructure supports any grammar fuzzer with a few
%% adjusts. However, we try to integrate several grammar-based fuzzers,
%% for example
%% and \Fix{others fuzzers} to
%% generate new JavaScript files based on grammar, but after several runs
%% it was observed that this approach was ineffective due the amount of
%% invalid files and/or files without discrepancies.
%% For example, if we
%% ran Grammarinator to generate 1K JS files ten times with a random seed
%% generation, we obtained \Fix{XX\%} of valid files. Checking in our
%% environment almost \Fix{XX\%} are js files that shows undefined
%% variables and due the differential testing in our environment all
%% engines will raise a SyntaxError and this approach was not relevant to
%% our experiment.

%% We initially considered used representatives of popular fuzzing approaches. For
%% random-based fuzzing we used Radamsa~\cite{radamsa}; for
%% coverage-based fuzzing we used
%% \Fix{AFL~\cite{afl}/libfuzzer~\cite{libfuzzer}?}, and for
%% generative-based fuzzing we used
%% \Fix{grammarinator,jsfunfuzz?}. Details on how these fuzzers work can
%% be found elsewhere~\cite{fuzz-bart}.

Mutational fuzzers can be either white-box or black-box. White-box
mutational fuzzers are typically coverage-based. American Fuzz Loop
(AFL)~\cite{afl} and libFuzzer~\cite{libfuzzer} are examples of this
kind of fuzzers. These fuzzers run tests inputs against instrumented
versions of the program under testing with the typical goal of finding
univeral errors like crashes and buffer overflows. The instrumentation
adds code to collect branch coverage and to monitor specific
properties\footnote{There are options in the clang toolchain to build
  programs with fuzzing instrumentation~\cite{libfuzzer}. clang
  provides several sanitizers for property
  checking~\cite{clang-documentation}.}. AFL use coverage to determine
inputs that uncover a new branch and hence should be fuzzed more
whereas libFuzzer uses evolutionary generation--it tries to minimize
the distances to still-uncovered branches of the program. AFL takes
the instrumented program binary (say, a JS engine) and one seed input
to that program (say, a JS program) and produces on output
fault-revealing inputs, if found. Considering our context of
application, we needed to instrument one runtime engine for fuzzing;
we chose v8. Unfortunately, we found that most of the inputs produced
by AFL violate the JS grammar. Furthermore, the fuzzing task can take
days for a single seed input and there is no simple way to guide the
exploration\footnote{Exchanged emails with the tool author.}. That
happens because the fuzzer aims to explore the entire decision tree
induced from the engine's main function, including the branches
associated with the higher layers of the compiler (\eg{}, lexer and
parser). It is worth mentioning that Google mitigates that problem
with libFuzzer by asking developers to create fuzz targets for
specific program
functions\cite{libFuzzer-tutorial-google,libFuzzer-chromium-google}. Although
that approach has shown very effective, it requires domain knowledge
to create the calling context to invoke the fuzz target. For those
reasons, coverage-based fuzzers were not considered in this study.

We used two black-box mutational fuzzers in this
study--\radamsa~\cite{radamsa} and \quickfuzz~\cite{quickfuzz}. These
fuzzers require no instrumentation and domain-knowledge. They mutate
\emph{existing} inputs randomly. The strenght of the approach is
limited by the supported mutation operators, which are typically
simple, and the quality of the test suite. We chose \radamsa\ and
\quickfuzz\ \Fix{...elaborate...}


\section{Results}
\label{sec:results}

Recall that the goal of our study is to assess reliability of
\js\ engines by leveraging the rich diversity of implementations in
this domain. We report results in three parts. First, we report on the
stability of the engines we analyzed using the EcmaScript standard
conformance test suite Test262~\cite{ecma262-conformance-suite}. The
intuition is that the bugs we discover would have low relevance if the
engines are unstable (\ie{}, too fragile). Second, we report results
obtained with test transplantation. The hypothesis is that test cases
written for one engine may reveal bugs in different engines. The
rationale is that the domain of possible inputs is just too large;
consequently, test suites of a given engine will cover scenarios and
corner cases uncovered by another engine.  Third, we analyzed the
impact of using cross-engine differential testing to find bugs in
\js{} engines. \Fix{hypothesis/rationale}

\subsection{Stability of Engines}

We evaluated stability with Test262~\cite{ecma262-conformance-suite},
the official \js{} conformance test suite for the ECMA262
specification. We ran this suite once per day for seven consecutive
days. Table~\ref{tab:test262} shows the number of failures over this
period. It is important to note that failures are expected as it takes
time for engines to adjust to the spec. It is also important to
highlight that some engines, namely \smonkey{}, \Fix{...}, have
variants of Test262 as part of their regression test suite\Mar{with
  fewer tests?}. We used the official version instead. To sum, we
observed that \Fix{...} \Mar{check connection with kangax report}

\begin{table}[h]
  \centering
  \caption{\label{tab:test262}Results of Test262.}
  \begin{tabular}{crrrrrrr}
    \toprule
           & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
    \jsc{} & - & - & - & - & - & - & - \\
    \veight{} & - & - & - & - & - & - & - \\
    \chakra{} & - & - & - & - & - & - & - \\
    \smonkey{} & - & - & - & - & - & - & - \\
    \bottomrule 
  \end{tabular}
\end{table}

\subsection{Test Transplantation}

We expanded these results by looking for tests from different
sources. \Fix{elaborate} Table~\ref{tab:cross-testing} shows
results. To sum, we observed that \Fix{...}

\begin{table}[h]
  \centering
  \caption{\label{tab:cross-testing}Crossed execution of test suites..}
  \begin{tabular}{crrrr}
    \toprule
    test suite/engine & \jsc{} & \veight{} & \chakra{} & \smonkey{} \\
    Mozilla & - & 333 & - & -  \\
    v8 & - & - & - & -  \\
    WebKit & - & 11 & - & -  \\
    Duktape & - & 80 & - & -  \\
    JerryScript & - & 92 & - & -  \\
    JSI & - & 14 & - & -  \\
    Tiny-js & - & 11 & - & -  \\
    \bottomrule 
  \end{tabular}
\end{table}  

%% Our infrastructure reported a total of \nofuzzAll{} buckets, including
%% \lo{} and \hi{}, that resulted in \nofuzzBugs{} bug reports. Of the
%% total of \nofuzzAll{} buckets, we found that \nofuzzDuplicates\ were
%% duplicates and \nofuzzFalsePositives\ were false positives. Most of
%% the cases of duplicates were related to the Test262 conformance test
%% suite, which all \js{} should strive to pass but we found that it is
%% not uncommon for the engines to fail on some of these. For example,
%% \veight{} \nofuzzFalsePositives


\subsection{Cross-Engine Differential Testing}

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=0.40\textwidth]{R/histograms/histograms.pdf}  
  \caption{\label{fig:distribution}Distribution of lo and hi warnings
   in buckets.}
\end{figure}

In the following, we describe the methodology we used to asses the
impact of cross-engine differential testing in finding bugs on \js{}
engines. Recall that our tool outputs a set of buckets, that each
bucket includes a set of warnings, that each warning is related to one
file, and that a bucket is of ``lo'' or ``hi'' importance, according
to the heuristic we defined. Figure~\ref{fig:distribution} shows the
distribution of number of warnings per bucket. We use the the label
``nofuzz'' to refer to the experiment that analyzed the impact of test
mining as we do not use fuzzing in that experiment and use the names
of the fuzzing tools to refer to the results obtained with those
tools. We inspected each and every bucket for bugs, but observe that
the number of files per bucket is very high in some cases (\eg{},
nofuzz [hi]). As such, it would be impractical to manually inspect all
the files in the bucket. For that reason, we only inspected one file
per bucket. For the cases we found the warning suspicious, we analyzed
the parts of the documentation related to the problem and, if still
suspicious, we looked on the bug tracker of the affected engine for
potential duplicates. If none was reported, we filed a bug report
including the test input that manifested the potential problem and
explaining the reasons we were suspicious. In some cases, we used the
Mozila lithium tool~\cite{lithium} to minimize the test case. For the
cases of false positives, we identified the source of false positive
and moved to the next bucket. Table~\ref{tab:summary-of-results}
summarizes our results. It shows, for each execution mode and category
of bucket (\ie{}, hi or lo), the total number of files involved, the
number of buckets reported by our infrastructure, the number of those
that we classified as bug-revealing, and the number of those that we
filed a bug report. The difference between the last two columns
indicate the number of duplicates, \ie{}, the cases of
already-reported bugs. As expected, the hit ratio for the ``hi'' group
is higher.

\begin{table}[t]
  \centering
  \caption{\label{tab:summary-of-results}Summary of results.}
  \begin{tabular}{ccrrrr}
    \toprule
    \multirow{2}{*}{mode} & \multirow{2}{*}{type} & \multirow{2}{*}{\# files} &  \multicolumn{3}{c}{buckets} \\
    \cline{4-6}    
    & & & \multicolumn{1}{c}{\#} & \multicolumn{1}{c}{\# pos.} &
    \multicolumn{1}{c}{\# pos. filed} \\
    \midrule
    %% \multirow{2}{*}{nofuzz} & lo & \nofuzzFilesLO{} &  \nofuzzLOTotal{} & \Fix{.} & 11 (34) \\
    %% & hi & \nofuzzFilesHI{} & \nofuzzHITotal{} & \Fix{.} & \Fix{108 (-)} \\
    \multirow{2}{*}{\radamsa} & lo & \Fix{.} & \Fix{.} & \Fix{.} & \Fix{.} \\
    & hi & \Fix{.} & \Fix{.} & \Fix{.} & \Fix{.} \\
    \multirow{2}{*}{\quickfuzz} & lo & \Fix{.} & \Fix{.} & \Fix{.} & \Fix{.} \\
                             & hi & \Fix{.} & \Fix{.} & \Fix{.} & \Fix{.} \\    
    \bottomrule     
  \end{tabular}
\end{table}

Table~\ref{tab:false-positives} shows the distribution of false
positives per source. A false positive is a bucket that did not reveal
a bug. We identified five sources of imprecision. The source ``not yet
implemented'' indicates that the engine does not implement some part
of the specification. For example, at the time of writing, Chakra does
not implement various properties from the \CodeIn{Symbol} object. The
source ``invalid input'' indicates that the test violated some part of
the specification. For example, the test indirectly invoked some
function with unexpected arguments\Fix{provide concrete example}. In
all cases that we analyzed, that happened because fuzzing produced an
invalid test input. Note that no false positives of this kind appears
under the column nofuzz. The source ``timeout/ome'' indicates that the
test times out (we used a 3s time budget) or runs out of memory. That
happens because engines implement different set of optimizations. For
example, only \jsc{} implements tail-call optimization to avoid tail
recursion. Consequently, for a certain input, it finishes execution
quickly whereas other engines time out. The source
``implementation-dependent'' is as defined in
Section~\ref{sec:imp-dep-behavior}.\Fix{provide concrete example} The
source ``future feature'' corresponds to the case where some feature
will be part of the spec in the future but implementing it is
facultative.\Fix{provide concrete example}

\begin{table}[t]
  \centering
  \caption{\label{tab:false-positives}Distribution of false
    positives buckets per source.}
  \begin{tabular}{crr}
    \toprule
    & \radamsa\ & \quickfuzz\ \\
    %% not yet implemented & 8 & - & - \\
    %% invalid input & 0 & - & - \\
    %% timeout/ome & 13 & - & - \\
    %% implementation-dependent & 3 & - & - \\
    %% future feature & 3 & - & - \\
    not yet implemented & - & - \\
    invalid input & - & - \\
    timeout/ome & - & - \\
    implementation-dependent & - & - \\
    future feature & - & - \\
    \bottomrule     
  \end{tabular}
\end{table}


\Igor{
  Neste experimento, obtemos \nofuzzAll{} buckets distintos, a proporcao \'e de 
  \nofuzzHITotal{} HI e apenas \nofuzzLOTotal{} LO. Como descrito anteriormente,
  n\'os analisamos e discutimos se estas discrepancias reportadas pela ferramenta sao 
  casos de bugs reais ou casos de falso positivos.
  Nesta analise, nos observamos que cerca de \Fix{63} buckets sao duplicados, ou seja,
  estes warnings foram reportados anteriormente nos issues trackers dos engenhos 
  ou podem ser encontrados casos similares em buckets distintos. Um exemplo claro dos 
  buckets duplicados podem ser encontrados atraves dos testes da suite Test262,
  onde esta suite cobre as especificacoes da linguagem em arquivos distintos e que
  podem conter mensagens de erro diferentes para um mesmo warning.

  Nos separamos os buckets considerados falso positivos de acordo com 

   
  % Ao total, foram obtidos \nofuzzAll{} warnings, sendo estes \nofuzzHighTotal{}
  % pertencentes aos buckets com prioridade HI e \nofuzzLowTotal{} da prioridade LO. 
  % Durante 3 meses, n\'os analisamos com rigor cada log gerado, sempre checando as 
  % especificacoes do EcmaScript, a fim de encontrar bugs reais nos engenhos
  % javascript que violem condições da especificacao ou apresentem discrepancias
  % entre as saidas de cada engenhos.
  % Nosso estudo revelou que dos \nofuzzHighTotal{} buckets do grupo HI, 
  % \nofuzzHighReport{} foram reportados enquanto que no grupo LO, apenas 
  % \nofuzzLowReport{} possuiam bugs reais (ratio HI $>$ LO).
}
  % A diversidade de suites de testes influenciou na busca de bugs nao descobertos
  % pelo proprio projeto do engenhos. Por exemplo, cerca de \nofuzzTC{} buckets 
  % foram encontrados usando a suite do TC-39, alem de \nofuzzMozilla{} warnings 
  % foram obtidos do repositorio da Mozilla, \nofuzzJerry{} na suite do JerryScript, 
  % \nofuzzWebKit{} sao do WebKit e \nofuzzGoogle{} para o V8 e o Duktape, 
  % os arquivos dos repositorios do TinyJS e JSI nao mostraram discrepancias entre as
  % engines.

  % A importancia tanto de utilizar fuzzing em arquivos de teste do proprio projeto,
  % quanto em projetos distintos de mesma ordem, pois neste caso engines JS 
  % devem interpretar arquivos, entao quanto mais entradas, melhores os resultados.

  % Nosso estudo revelou que dos \tableBugsNum{} bugs reportados pelo nosso time, 
  % apenas \Fix{10} nao foram fuzzados \Fix{43.4\%},
  % Apesar do engenhos do V8 e SpiderMonkey
  % serem os mais robustos em relacao aos demais, tambem 
  % foram encontrados warnings nestes engenhos, porem


%% \Fix{nao entendi muito sobre essa subsection... 
%% seria explicacao do porque utilizar o fuzzing baseado nos nosso resultados?}
%% \Igor{
%%   Os engines JS mais populares foram arduamente testados ao longo dos últimos anos, 
%%   o sistema robusto nao significa que esta livre de bugs, ao contrario,
%%   estamos testando-o com simples entradas a partir de testes de regressao 
%%   de bugs previamente encontrados, porem estas entradas poderiam servir como seeds 
%%   para novos testcases. Partindo disso, realizar fuzzer em arquivos 
%%   existentes em projetos open-sources eh uma maneira de
%%   aumentar ainda mais a eficiencia destes engines, por exemplo o Google incentiva o 
%%   uso de fuzzers no seu ecossistema\cite{oss-fuzz,honggfuzz}.



\subsection{Fuzzing}

This section reports the results obtained by fuzzing inputs with
existing tools. It is important to mention that, to avoid experimental
noise, we did not consider the \nofuzzTotalFiles{} files (\Fix{2.8\%} of the
total) reported in the previous experiment, \ie{}, we only fuzzed test
files whose output is consistent across all engines analyzed.

% \Igor{
%   Para realizar nossos experimentos, tivemos que decidir qual as 
%   melhores abordagens para aumentar a efetividade dos fuzzers.
%   Os fuzzers mutacionais se mostraram mais eficientes em relacao
%   aos fuzzers geracionais, entao focamos neste ponto.
%   Utilizamos os fuzzers radamsa, QuickFuzz e \Fix{others} nas suas configuracoes
%   default para realizar a mutacao em arquivos existentes.
%   O fuzzer radamsa eh um fuzzer mutacional agnostico de linguagem e possui algumas
%   deficiencias para a nossa infraestrutura como a falta de uma gramatica para guiar a suas mutacoes, 
%   pois muitas das vezes a unica mutacao ocorre em uma palavra dentro de um comentario por exemplo.
%   O fuzzer QuickFuzz tem uma gramatica JS integrada e sua mutacao ocorre de forma mais robusta que o 
%   radamsa, gerando arquivos mais relevantes.
%   Nossa infraestrutura usa a library Esprima\footnote{cite esprima url} para garantir que o arquivo fuzzado
%   é um arquivo de teste relevante, seguindo estes requisitos:
%   \begin{itemize}
%     \item it contains only unicode text
%     \item it is parseable (i.e., it is structurally well-formed)
%     \item it does not contain engine-specific functions  
%     \item it does not empty
%   \end{itemize}

% }

\section{Bugs Found}
\label{sec:bugs}

%% Although there are many features yet to implement in our
%% infrastructure,

\begin{table*}[h!]
  \vspace{-3ex}
%  \scriptsize
  \centering
  \caption{List of bug reports issued by our team since April,
    2018. The analysis of warnings was not uninterrupted.}
  \label{tab:bugs}
  \begin{tabular}{cccccccc}
    \toprule
    Issue\#    & Date & Fuzz & Engine  & Status  & \multicolumn{1}{c}{Url}  & Priority & Seed \\
    \midrule    
    1  & 4/12 & radamsa & Chakra   & \textbf{Fixed}  & \href{https://github.com/Microsoft/ChakraCore/issues/4978}{\#4978} & LO & WebKit \\ 
    2  & 4/12 & radamsa & Chakra   & Rejected  & \href{https://github.com/Microsoft/ChakraCore/issues/4979}{\#4979} & HI & WebKit \\
    3  & 4/14 & radamsa & JavascriptCore  & New & \href{https://bugs.webkit.org/show\_bug.cgi?id=184629}{\#184629}  & HI & WebKit    \\
    4  & 4/18 & \crossmark & JavascriptCore  & New  & \href{https://bugs.webkit.org/show\_bug.cgi?id=184749}{\#184749} & HI & JerryScript      \\
    5  & 4/23 & \crossmark & Chakra  & \textbf{Confirmed}  & \href{https://github.com/Microsoft/ChakraCore/issues/5033}{\#5033} & HI & Mozilla      \\
    6  & 4/25 & radamsa & Chakra  & \textbf{Fixed}     & \href{https://github.com/Microsoft/ChakraCore/issues/5038}{\#5038} & HI & JerryScript   \\
    7  & 4/29 & \crossmark & Chakra  & \textbf{Confirmed}   &
    \href{https://github.com/Microsoft/ChakraCore/issues/5065}{\#5065} & HI & Mozilla
    \\
    \midrule
    \multirow{2}{*}{8}  & \multirow{2}{*}{4/29} &  \multirow{2}{*}{\crossmark} & Chakra & \textbf{Confirmed} &    \href{https://github.com/Microsoft/ChakraCore/issues/5067}{\#5067} & \multirow{2}{*}{HI} & \multirow{2}{*}{Mozilla}\\
                        &  &                       &
    JavascriptCore & New &    \href{https://bugs.webkit.org/show\_bug.cgi?id=185130}{\#185130}  &   & \\
    \midrule    
    9  & 4/29 & radamsa & JavascriptCore  & New  &    \href{https://bugs.webkit.org/show\_bug.cgi?id=185127}{\#185127}  & HI  & JerryScript\\
    \midrule    
    \multirow{2}{*}{10} & \multirow{2}{*}{4/30}  & \multirow{2}{*}{radamsa} & Chakra & \textbf{Confirmed} &    \href{https://github.com/Microsoft/ChakraCore/issues/5076}{\#5076} & \multirow{2}{*}{HI} & \multirow{2}{*}{TinyJS}\\    
                        &                        &        &
    JavascriptCore & New &
    \href{https://bugs.webkit.org/show\_bug.cgi?id=185156}{\#185156} &  & 
    \\
    \midrule    
    11 & 5/02 & radamsa & JavascriptCore  & \textbf{Fixed} & \href{https://bugs.webkit.org/show\_bug.cgi?id=185197}{\#185197} & LO & Mozilla \\
    12 & 5/02 & \crossmark & JavascriptCore & New  & \href{https://bugs.webkit.org/show\_bug.cgi?id=185208}{\#185208} & HI & Mozilla \\
    13 & 5/10 & radamsa & Chakra & \textbf{Confirmed} & \href{https://github.com/Microsoft/ChakraCore/issues/5128}{\#5128} & HI & JerryScript \\
    14 & 5/17 & radamsa & Chakra & \textbf{Fixed} & \href{https://github.com/Microsoft/ChakraCore/issues/5182}{\#5182} & HI & V8\\
    15 & 5/17 & \crossmark & Chakra & \textbf{Confirmed} & \href{https://github.com/Microsoft/ChakraCore/issues/5187}{\#5187} & HI & WebKit\\
    16 & 5/21 & \crossmark & Chakra & \textbf{Confirmed} & \href{https://github.com/Microsoft/ChakraCore/issues/5203}{\#5203} & LO & Mozilla\\
    17 & 5/24 & radamsa & JavascriptCore & \textbf{Fixed}  & \href{https://bugs.webkit.org/show\_bug.cgi?id=185943}{\#185943} & HI & Webkit\\
    18 & 6/26 & radamsa & JavascriptCore & \textbf{Fixed}  & \href{https://bugs.webkit.org/show_bug.cgi?id=187042}{\#187042} & HI & JerryScript\\
    19 & 6/28 & \crossmark & Chakra & \textbf{Confirmed}  & \href{https://github.com/Microsoft/ChakraCore/issues/5388}{\#5388} & HI & WebKit\\
    20 & 7/10 & quickfuzz & JavaScriptCore & \textbf{Fixed}  & \href{https://bugs.webkit.org/show_bug.cgi?id=187520}{\#187520} & HI & JerryScript\\
    22 & 7/10 & \crossmark & Chakra & \textbf{Confirmed} & \href{https://github.com/Microsoft/ChakraCore/issues/5442}{\#5442} & HI & JerryScript\\
    21 & 7/10 & quickfuzz & Chakra & \textbf{Confirmed}  & \href{https://github.com/Microsoft/ChakraCore/issues/5443}{\#5443} & HI & JerryScript\\
    23 & 7/18 & \crossmark & Chakra & \textbf{Fixed} & \href{https://github.com/Microsoft/ChakraCore/issues/5478}{\#5478} & HI & Mozilla\\
    24 & 7/18 & \crossmark & JavascriptCore & New & \href{https://bugs.webkit.org/show_bug.cgi?id=187777}{\#187777} & HI & JerryScript\\
    25 & 7/27 & \crossmark & Chakra & New & \href{https://github.com/Microsoft/ChakraCore/issues/5548}{\#5548} & HI & Test262\\
    26 & 7/27 & \crossmark & Chakra & \textbf{Confirmed} & \href{https://github.com/Microsoft/ChakraCore/issues/5549}{\#5549} & HI & Test262\\
   \bottomrule
  \end{tabular}
\end{table*}


This section shows results obtained with our
infrastructure. Table~\ref{tab:bugs} shows the list of bugs we
reported on issue trackers of different engines in the period of 42
days. So far, ten of the bugs we reported
were confirmed, two of which were fixed. One bug report we
submitted was rejected on the basis that the offending JS file
manifested an expected incompatibility across engine
implementations.
Note from the table that all bug
reports still waiting for confirmation are associated with the
JavaScriptCore engine (JSC). A closer look at the JSC issue tracker
showed that the triage process is very slow for that engine. 
\Igor{
The bugs reported on Chakra's bug tracker was defined as confirmed, 
however the bugs are included on a milestone for the next release
following an internal priority.
}
As of now, we did not find any new bug on SpiderMonkey and V8; 
the bugs we found were duplicates and were not reported. Finally, it is
worth noting that \Fix{11 of the 19} JS files that manifested
discrepancies were \emph{not} produced with fuzzing (column
``Fuzz''). These are test files from suites of different engines. This
observation emphasizes the importance of continuously collecting test suites from
multiple sources; today, we use test suites from seven different open
source engines, including a total of 30K test files.

\Mar{justify why we discuss these bugs} \Mar{discuss other bugs}

\vspace{1ex}\noindent\textbf{Bug \# 6.} The JS code \CodeIn{var a = \{valueOf:~function()\{return
  ``\textbackslash{}x00''\}\} assert(+a === 0)\}} 
manifested a bug in the \js{} engine Chakra.  The object
property \CodeIn{valueOf} stores a function that returns a primitive
value identifying the target object~\cite{valueof}. The original
version of this code returns an empty string whereas the version of
the code modified by the Radamsa fuzzer~\cite{radamsa} returns a string
representation of a null character (called \CodeIn{NUL} in ascii). The
unary plus expression ``\CodeIn{+a}", appearing in the assertion, is
equivalent to the abstract operation \CodeIn{ToNumber(a.valueOf())}
that converts a string to a number, otherwise the operation returns
NaN (Not a Number)\cite{unary-plus}. For this case, Chakra evaluates
the unary plus to NaN as expected, as the null character cannot be
converted. As result, the test fails as expected. Chakra, in contrast,
incorrectly converts the string to zero, making the test to pass. All
other engines fail on this test. As Table~\ref{tab:bugs} shows, the
Chakra team fixed the issue soon after we reported the problem.

\subsection{False Positives}

An example of a warning from the HI group is defined in Figure~\ref{fig:hi-priority}. 
This is a testcase from WebKit.es6 suite that it was mutate by radamsa fuzzer, the 
initial seed has in line 3 the code \CodeIn{"foo".repeat(3)} but the fuzzer changed the 
integer 3 to a big integer number. We reported this case in chakra due a core dumps that occurs
during the runtime, however this case was rejected due \CodeIn{incompatibility by design} that
this is an intentional behavior of engine that crash the process if it runs out of memory.

\begin{figure}[h!]
  \centering
  \scriptsize
  \lstset{escapeinside={@}{@},
    numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
    basicstyle=\ttfamily\scriptsize, boxpos=c,
    numberstyle=\tiny,
    morekeywords={assertEq, var, yield, in, function, 
    typeof, return, throw, new, Error, if},
  }
  \begin{lstlisting}
function test() {
  return typeof String.prototype.repeat === "function"
    && "foo".repeat(657604378) === "foofoofoo";
}
if (!test())
  throw new Error("Test failed");
  \end{lstlisting}
  \normalsize
  \caption{Warning captured as HI priority.}
  \label{fig:hi-priority}
  \end{figure}

% \subsection{LO bugs}
% 
% \Igor{For example, a bug caught by our environment and reported as LO priority was reported 
% on Chakra engine, the JS code can be found in Figure~\ref{fig:lo-priority}. In this case,
% this is a seed from Mozilla suite (\CodeIn{mozilla/non262/statements/for-in-with-assignments.js}),
% the warning was not caught by the \CodeIn{assertEq} function that compares if two arguments are equals, 
% the bug appears inside the generator function\footnote{Generators \url{https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*}}
% at line 2. According to ES6 specification\footnote{ES6 YieldExpression \url{https://www.ecma-international.org/ecma-262/8.0/index.html\#prod-YieldExpression}},
% it is allowed the use of \CodeIn{yield in} in a \CodeIn{for} loop. In our infrastructure,
% only Chakra engine fails with an error output \CodeIn{SyntaxError: Syntax error}, due
% the output does not shows nothing related with assertions, we considered this one as a warning from LO group.
% Until now, the issue was confirmed and waiting for merge/closed.
% \Fix{checar ate o prazo de submissao. Essa issue esta confirmada e com commits, falta apenas o merge.}
% }
% \begin{figure}[h!]
%   \centering
%   \scriptsize
%   \lstset{escapeinside={@}{@},
%     numbers=left,xleftmargin=1em,frame=single,framexleftmargin=0.5em,
%     basicstyle=\ttfamily\scriptsize, boxpos=c,
%     numberstyle=\tiny,
%     morekeywords={assertEq, var, yield, in, function},
%   }
%   \begin{lstlisting}
% function* g1() {
%   for (var x = yield in {}) ;
% }
% var it = g1();
% assertEq(it.next().done, false);
% assertEq(it.next().done, true);
%   \end{lstlisting}
%   \normalsize
%   \caption{Bug caught by our environment as LO priority.}
%   \label{fig:lo-priority}
%   \end{figure}

\section{Discussion}

It is important to highlight that our treatment on both dimensions of
analysis--test mining and fuzzing--could be improved by selecting more
test cases or using more advanced fuzzing tools. The main rationale is
that if these techinques perform well with the objects we used
(Section~\ref{sec:methodology}) then the approach would perform even
better with more elaborate objects.

\Fix{...}


\section{Related Work}

\Fix{The
  closest work to ours was done by Patra and Pradel~\cite{patra2016learning},
  where they evaluated their proposed language-agnostic fuzzer to find
  cross-browser HTML+JS discrepancies. This project aims at building
  and evaluating an infrastructure for differential testing of runtime
  engines, such as the JS engine or WebAssembly's. The sensible parts
  of the infrastructure are the checks of input validity (as to reduce
  waste/cost) and output correctness (as to reduce false positives).}

RFC-Directed Differential Testing of Certificate Validation in SSL/TLS Implementations...[ICSE18]

%\section*{Acknowledgment}

%\bibliographystyle{IEEEtran}
\balance
\bibliographystyle{plain}
\bibliography{references,../docs/google-research-awards-latam/tmp}

\end{document}

%%  LocalWords:  bytecodes JScript Ecma ome
