%\documentclass[conference]{IEEEtran}
\documentclass[10pt,conference,anonymous]{IEEEtran}
\IEEEoverridecommandlockouts

%% Marcelo added this
\makeatletter
\renewcommand\footnoterule{%
  \kern-3\p@
  \hrule\@width.4\columnwidth
  \kern2.6\p@}
  \makeatother

\input{packages}
\input{macros}

\begin{document}

\title{Should I Fuzz my Inputs or Improve my Test Suite? Lessons from
  Differential Testing Runtime Engines.}

%% \author{
%% \IEEEauthorblockN{Sabrina Souto}
%% \IEEEauthorblockA{State University of Para\'iba\\
%% Para\'iba, Brazil\\
%% sabrinadfs@gmail.com}
%% \and
%% \IEEEauthorblockN{Marcelo d'Amorim}
%% \IEEEauthorblockA{Federal University of Pernambuco\\
%%   Pernambuco, Brazil\\
%%   damorim@cin.ufpe.br}
%% \and
%% \IEEEauthorblockN{Rohit Gheyi}
%% \IEEEauthorblockA{Federal University of Campina Grande\\
%%   Para\'iba, Brazil\\
%%   rohit@dsc.ufcg.edu.br}
%% }

\maketitle

\begin{abstract}
Differential testing has shown to be a successful approach to finding
bugs in compilers and runtime engines. This paper assesses the impact
that the fuzzing strategy and the test suites have in bug-finging
ability. We focused on JavaScript and WebAssembly runtime
engines. \Fix{...}
\end{abstract}

\begin{IEEEkeywords}
...
\end{IEEEkeywords}

\section{Introduction}

JavaScript (\js{}) is one of the most popular programming languages
for the web today~\cite{business-insider,stackify}. The interest of
the community in the JS ecossystem incentivizes improvements both in
the language and their runtime engine implementations. It is natural
to expect that such improvements will entail sensible changes in
engine implementations~\cite{kangax} that could lead to
errors. Finding bugs on JS engines is therefore and important
problem. Automated techniques, such as Fuzz Testing~\cite{fuzz-bart},
have shown successful at finding universal errors, such as memory
errors~\cite{libfuzzer-tutorial}. However, they don't address the
problem of finding functional errors that could arise when an engine
does not adhere to the specification.

This paper reports on a study to find those kinds of bugs in
JavaScript engines.\Mar{revise...a study answer questions? the
  statement is more propositive.} The goal of the study is to improve
quality of existing \js{} engines by improving the bug-finding
process. We used differential testing~\cite{Brumley-etal-ss07}, a
technique that has been applied in a variety of
contexts~\cite{Yang-etal-pldi11,Chen-etal-fse2015,Argyros-etla-ccs16,Chen-etal-pldi16,petsios-etal-sp2017,SivakornAPKJ17}
to address the oracle problem. Differential testing leverages the
diversity across system's implementations to detect anomalous
behavior. \Mar{this needs work$\rightarrow$} \Fix{but it has not been
  thoroughly explored to find functional bugs in JS engines. The
  closest work was done by Patra and Pradel~\cite{patra2016learning},
  where they evaluated their proposed language-agnostic fuzzer to find
  cross-browser HTML+JS discrepancies. This project aims at building
  and evaluating an infrastructure for differential testing of runtime
  engines, such as the JS engine or WebAssembly's. The sensible parts
  of the infrastructure are the checks of input validity (as to reduce
  waste/cost) and output correctness (as to reduce false positives).}

\section{Infrastructure}
\label{sec:design}


%\begin{wrapfigure}[10]{r}[0pt]{0.45\textwidth}
\begin{figure}[t]
  \centering
%  \includegraphics[trim=20 350 200
%    0,clip,width=0.35\textwidth]{google-awards-workflow}
  \includegraphics[trim=0 250 0 0,clip,width=0.5\textwidth]{google-awards-workflow}  
  \caption{\label{fig:workflow}Infrastructure.}
\end{figure}

Figure~\ref{fig:workflow} illustrates the workflow of the
infrastructure we used in this study. Boxes denote encapsulation;
arrowed lines indicate control flow (dashed) and data flow
(filled). The cyclic icons denote repetition--the leftmost cyclic icon
indicates that each file in the input list will be analyzed in
separate and the rightmost icon indicates that a single file will be
processed multiple times in fuzzing mode. The bug-finding process
takes on input JS files from regression test suites of various JS
engines. 

\Igor{
\subsection{Populating seed files.}~This section describes 
how we found valid javascript files based on 
existing sources on the open-source repositories.
Originally, we mined JS engines repositories that we 
support (ie. V8, SpiderMonkey, Chakra and JavascriptCore) and 
we found a total of \Fix{XX} files of regression suites and 
benchmarks testing involving javascript files.
Latelly, we need to increase our input seeds through existing
open-sources projects that involves javascript engines.
We mined repositories from GitHub based on these requirements:
\begin{itemize}
  \item The project must be an open-source project and at least 1K stars from community.
  \item The repository must have JavaScript tests included.
  \item The tests should be plug-and-play and/or easy integration.
\end{itemize}

We wrote a query to mine these repositories on GitHub
\CodeIn{\url{https://api.github.com/search/repositories?q=javascript-engine&sort=stars&order=desc}}\Fix{wip}
and we follow the requirements ignoring invalid projects.
\Fix{discuss wip}

Table~\ref{tab:test-suites} shows the JS sources we mined
from online repositories of various open-source engines. Overall, we
found a total of \Fix{30K} JS files.
}
\Igor{
\begin{itemize}
  \item discuss about changes in seeds for run in our infraestructure (wip)
  \item discuss about generating new files via fuzzers grammar-based (wip)
  \item discuss about mutation fuzzers (wip)
\end{itemize}
}

\begin{table}[h]
  \centering
  \caption{\label{tab:test-suites}Test Suites\Fix{use path in the repo}}
  \begin{tabular}{ccc}
    \toprule
    TestSuite & Source & \# JS files \\
    \midrule
    \Fix{duktape} & - & 997 \\
    \Fix{jerryjs} & - & 1.803 \\
    \Fix{jsi} & - & 99 \\
    \Fix{tiny} & - & 44 \\
    \Fix{mozilla} & - & - \\
    v8.test.benchmarks.data & - & 64 \\
    webkit.jstests.es6 & - & 605 \\
    webkit.jstests.microbenchmarks & - & 426 \\
   \bottomrule     
  \end{tabular}
\end{table}


The bug-finding process works as follows.  Considering the case
fuzzing is not selected, as illustrated in the inner box at the bottom
of the figure, the oracle checks whether or not the output produced
for that file is consistent across all engine implementations. In case
the test passes in all engines or fails in all engines (\ie{}, the
output is consistent), the infrastructure discards the corresponding
input (mark ....). Otherwise, it considers the input as potentially
fault-revealing; hence interesting for human inspection. Considering
the case where fuzzing~\cite{fuzz-testing-history} is selected, new
inputs are obtained from a given input using some off-the-shelf
fuzzer\footnote{Several fuzzing methods have been proposed in the
  past, varying with respect to how new inputs are generated (\eg{},
  coverage-based~\cite{afl,honggfuzz},
  grammar-based~\cite{grammarinator,jsfunfuzz}, and
  random-based~\cite{radamsa}).}. The workflow is similar to that of
no fuzzing. In this case, however, multiple warnings can be produced
for a given seed input.

The infrastructure outputs a list of warnings for human inspection.
To reduce the number of false alarms, we clustered warnings in two
groups, reflecting their likelihood to manifest a real bug. The HI
group includes those inputs where execution manifests anomaly in the
test code (or its close neighborhood) as opposed to some internal JS
function. The rationale is that the test in this group executed
without violating any internal checks of the API.
The LO group, in contrast, includes those cases where the anomaly
was observed executing some JS function. We found that engine
implementations often check pre-conditions of API functions
differently, leading to premature failures which manifest themselves as
discrepancy in our infrastructure. Although we did find real bugs from
warnings in the LO group, the proportion was much lower compared to the HI
group--only 12\% of the reals bugs we found originated from the LO
category. \Mar{show real examples of HI and LO and provide the
  rationale for such classification.} \Mar{can we detect dups mining
  issue trackers?}

\begin{table*}[h!]
  \vspace{-3ex}
%  \scriptsize
  \centering
  \caption{List of bug reports issued by our team from April 12 to May
    24, 2018.}
  \label{tab:bugs}
  \begin{tabular}{cccccc}
    \toprule
    Issue\#    & Date & Fuzz & Engine  & Status  & \multicolumn{1}{c}{Url}  \\
    \midrule    
    1  & 4/12 & radamsa & Chakra   & \textbf{Fixed}  & \href{https://github.com/Microsoft/ChakraCore/issues/4978}{\#4978}      \\ 
    2  & 4/12 & radamsa & Chakra   & Rejected  & \href{https://github.com/Microsoft/ChakraCore/issues/4979}{\#4979}      \\
    3  & 4/14 & radamsa & JavascriptCore  & New & \href{https://bugs.webkit.org/show\_bug.cgi?id=184629}{\#184629}        \\
    4  & 4/18 & \crossmark & JavascriptCore  & New  & \href{https://bugs.webkit.org/show\_bug.cgi?id=184749}{\#184749}        \\
    5  & 4/23 & \crossmark & Chakra  & \textbf{Confirmed}  & \href{https://github.com/Microsoft/ChakraCore/issues/5033}{\#5033}       \\
    6  & 4/25 & radamsa & Chakra  & \textbf{Fixed}     & \href{https://github.com/Microsoft/ChakraCore/issues/5038}{\#5038}      \\
    7  & 4/29 & \crossmark & Chakra  & \textbf{Confirmed}   &
    \href{https://github.com/Microsoft/ChakraCore/issues/5065}{\#5065}
    \\
    \midrule
    \multirow{2}{*}{8}  & \multirow{2}{*}{4/29} &  \multirow{2}{*}{\crossmark} & Chakra & \textbf{Confirmed} &    \href{https://github.com/Microsoft/ChakraCore/issues/5067}{\#5067} \\
                        &  &                       &
    JavascriptCore & New &    \href{https://bugs.webkit.org/show\_bug.cgi?id=185130}{\#185130}    \\
    \midrule    
    9  & 4/29 & radamsa & JavascriptCore  & New  &    \href{https://bugs.webkit.org/show\_bug.cgi?id=185127}{\#185127}    \\
    \midrule    
    \multirow{2}{*}{10} & \multirow{2}{*}{4/30}  & \multirow{2}{*}{radamsa} & Chakra & \textbf{Confirmed} &    \href{https://github.com/Microsoft/ChakraCore/issues/5076}{\#5076} \\    
                        &                        &        &
    JavascriptCore & New &
    \href{https://bugs.webkit.org/show\_bug.cgi?id=185156}{\#185156}
    \\
    \midrule    
    11 & 5/02 & radamsa & JavascriptCore  & New & \href{https://bugs.webkit.org/show\_bug.cgi?id=185197}{\#185197}\\
    12 & 5/02 & \crossmark & JavascriptCore & New  & \href{https://bugs.webkit.org/show\_bug.cgi?id=185208}{\#185208}\\
    13 & 5/10 & radamsa & Chakra & \textbf{Confirmed} & \href{https://github.com/Microsoft/ChakraCore/issues/5128}{\#5128} \\
    14 & 5/17 & radamsa & Chakra & \textbf{Fixed} & \href{https://github.com/Microsoft/ChakraCore/issues/5182}{\#5182} \\
    15 & 5/17 & \crossmark & Chakra & \textbf{Confirmed} & \href{https://github.com/Microsoft/ChakraCore/issues/5187}{\#5187} \\
    16 & 5/21 & \crossmark & Chakra & \textbf{Confirmed} & \href{https://github.com/Microsoft/ChakraCore/issues/5203}{\#5203} \\
    17 & 5/24 & radamsa & JavascriptCore & New  & \href{https://bugs.webkit.org/show\_bug.cgi?id=185943}{\#185943}\\
    18 & 6/26 & radamsa & JavascriptCore & New  & \href{https://bugs.webkit.org/show_bug.cgi?id=187042}{\#187042}\\
   \bottomrule     
  \end{tabular}
\end{table*}


\section{Methodology}
\label{sec:methodology}

The goal of this study is to assess the impact of fuzzing in finding
bugs with differential testing.

\subsection{Constants.}~We used the following engines in
this study: Microsoft's Chakra~\cite{chakra2018repo}, Google's v8~\cite{v82018repo},
Mozilla's SpiderMonkey~\cite{spidermonkey2018repo}, and Apple's JavaScriptCore
(WebKit)~\cite{jsc2018repo}. We selected these engines based on
popularity~\Fix{cite}. Considering fuzzing tools, we used
representatives of the most popular approaches to fuzzing. For
random-based fuzzing we used Radamsa~\cite{radamsa}; for
coverage-based fuzzing we used
\Fix{AFL~\cite{afl}/libfuzzer~\cite{libfuzzer}?}, and for
generative-based fuzzing we used
\Fix{grammarinator,jsfunfuzz?}. Details on how these fuzzers work can
be found elsewhere~\cite{fuzz-bart}.

\subsection{Dependent and Independent variables.}

The independent variable of our experiment is the fuzzing strategy as
implemented by various fuzzing tools. Our dependent variables are (i)
the ratio of bugs found with fuzzing versus no fuzzing and (ii) and
the ratio of bugs found with each fuzzing strategy. We consider a
search successful only if it lends in a bug confirmed by the
developers of the affected engine.  The rationale for considering the
ratio of bugs found with fuzzing versus no fuzzing is to observe how
often fuzzing really helps in this scenario. In the limit, all bugs
could be found without extra inputs, \ie{}, simply executing the seed
inputs on each engine and comparing their outcomes. Recall that we
mined test suites from a variety of sources. The rationale for
considering the fuzzing strategy is to understand to which extent each
strategy--as implemented by representative tools--helps in this
context.

\Igor{
\subsection{Grammar-based Fuzzers}
Fuzzers grammar-based can generate a new file through a language grammar (ie. BNF).
Our infrastructure supports any grammar fuzzer with a few adjusts. However, 
we try to integrate several grammar-based fuzzers, for example 
Grammarinator~\footnote{https://github.com/renatahodovan/grammarinator}, QuickFuzz~\cite{grieco2016quickfuzz}
and \Fix{others fuzzers} to generate new JavaScript files based on grammar, but 
after several runs it was observed that this approach was ineffective
due the amount of invalid files and/or files without discrepancies. For example, if we ran 
Grammarinator to generate 1K JS files ten times with a random seed generation, we obtained
\Fix{XX\%} of valid files. Checking in our environment almost \Fix{XX\%}
are js files that shows undefined variables and due the differential testing in our environment
all engines will raise a SyntaxError and this approach was not relevant to our experiment.
}

\section{Bugs Found}
\label{sec:results}

%% Although there are many features yet to implement in our
%% infrastructure, 

This section shows results obtained with our
infrastructure. Table~\ref{tab:bugs} shows the list of bugs we
reported on issue trackers of different engines in the period of 42
days. So far, ten of the bugs we reported
were confirmed, two of which were fixed. One bug report we
submitted was rejected on the basis that the offending JS file
manifested an expected incompatibility across engine
implementations.
Note from the table that all bug
reports still waiting for confirmation are associated with the
JavaScriptCore engine (JSC). A closer look at the JSC issue tracker
showed that the triage process is very slow for that engine. As of
now, we did not find any new bug on SpiderMonkey and V8; the bugs we
found were duplicates and were not reported. Finally, it is
worth noting that 7 of the 17 JS files that manifested
discrepancies were \emph{not} produced with fuzzing (column
``Fuzz''). These are test files from suites of different engines. This
observation emphasizes the importance of continuously collecting test suites from
multiple sources; today, we use test suites from seven different open
source engines, including a total of 30K test files.

\Mar{justify why we discuss these bugs} \Mar{discuss other bugs}

\vspace{1ex}\noindent\textbf{Bug \# 6.} The JS code \CodeIn{var a = \{valueOf:~function()\{return
  ``\textbackslash{}x00''\}\} assert(+a === 0)\}} 
manifested a bug in the \js{} engine Chakra.  The object
property \CodeIn{valueOf} stores a function that returns a primitive
value identifying the target object~\cite{valueof}. The original
version of this code returns an empty string whereas the version of
the code modified by the Radamsa fuzzer~\cite{radamsa} returns a string
representation of a null character (called \CodeIn{NUL} in ascii). The
unary plus expression ``\CodeIn{+a}", appearing in the assertion, is
equivalent to the abstract operation \CodeIn{ToNumber(a.valueOf())}
that converts a string to a number, otherwise the operation returns
NaN (Not a Number)\cite{unary-plus}. For this case, Chakra evaluates
the unary plus to NaN as expected, as the null character cannot be
converted. As result, the test fails as expected. Chakra, in contrast,
incorrectly converts the string to zero, making the test to pass. All
other engines fail on this test. As Table~\ref{tab:bugs} shows, the
Chakra team fixed the issue soon after we reported the problem.

\section{Results}

\subsection{Fuzzing versus No Fuzzing}

\subsection{Fuzzing Strategy}

\section{Related Work}

RFC-Directed Differential Testing of Certificate Validation in SSL/TLS Implementations...[ICSE18]

%\section*{Acknowledgment}

%\bibliographystyle{IEEEtran}
\bibliographystyle{plain}
\bibliography{references,../docs/google-research-awards-latam/tmp}

\end{document}
